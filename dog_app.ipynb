{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Intelligence Nanodegree\n",
    "\n",
    "## Convolutional Neural Networks\n",
    "\n",
    "## Project: Write an Algorithm for a Dog Identification App \n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, some template code has already been provided for you, and you will need to implement additional functionality to successfully complete this project. You will not need to modify the included code beyond what is requested. Sections that begin with **'(IMPLEMENTATION)'** in the header indicate that the following block of code will require additional functionality which you must provide. Instructions will be provided for each section, and the specifics of the implementation are marked in the code block with a 'TODO' statement. Please be sure to read the instructions carefully! \n",
    "\n",
    "> **Note**: Once you have completed all of the code implementations, you need to finalize your work by exporting the iPython Notebook as an HTML document. Before exporting the notebook to html, all of the code cells need to have been run so that reviewers can see the final implementation and output. You can then export the notebook by using the menu above and navigating to  \\n\",\n",
    "    \"**File -> Download as -> HTML (.html)**. Include the finished document along with this notebook as your submission.\n",
    "\n",
    "In addition to implementing code, there will be questions that you must answer which relate to the project and your implementation. Each section where you will answer a question is preceded by a **'Question X'** header. Carefully read each question and provide thorough answers in the following text boxes that begin with **'Answer:'**. Your project submission will be evaluated based on your answers to each of the questions and the implementation you provide.\n",
    "\n",
    ">**Note:** Code and Markdown cells can be executed using the **Shift + Enter** keyboard shortcut.  Markdown cells can be edited by double-clicking the cell to enter edit mode.\n",
    "\n",
    "The rubric contains _optional_ \"Stand Out Suggestions\" for enhancing the project beyond the minimum requirements. If you decide to pursue the \"Stand Out Suggestions\", you should include the code in this IPython notebook.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "### Why We're Here \n",
    "\n",
    "In this notebook, you will make the first steps towards developing an algorithm that could be used as part of a mobile or web app.  At the end of this project, your code will accept any user-supplied image as input.  If a dog is detected in the image, it will provide an estimate of the dog's breed.  If a human is detected, it will provide an estimate of the dog breed that is most resembling.  The image below displays potential sample output of your finished project (... but we expect that each student's algorithm will behave differently!). \n",
    "\n",
    "![Sample Dog Output](images/sample_dog_output.png)\n",
    "\n",
    "In this real-world setting, you will need to piece together a series of models to perform different tasks; for instance, the algorithm that detects humans in an image will be different from the CNN that infers dog breed.  There are many points of possible failure, and no perfect algorithm exists.  Your imperfect solution will nonetheless create a fun user experience!\n",
    "\n",
    "### The Road Ahead\n",
    "\n",
    "We break the notebook into separate steps.  Feel free to use the links below to navigate the notebook.\n",
    "\n",
    "* [Step 0](#step0): Import Datasets\n",
    "* [Step 1](#step1): Detect Humans\n",
    "* [Step 2](#step2): Detect Dogs\n",
    "* [Step 3](#step3): Create a CNN to Classify Dog Breeds (from Scratch)\n",
    "* [Step 4](#step4): Use a CNN to Classify Dog Breeds (using Transfer Learning)\n",
    "* [Step 5](#step5): Create a CNN to Classify Dog Breeds (using Transfer Learning)\n",
    "* [Step 6](#step6): Write your Algorithm\n",
    "* [Step 7](#step7): Test Your Algorithm\n",
    "\n",
    "---\n",
    "<a id='step0'></a>\n",
    "## Step 0: Import Datasets\n",
    "\n",
    "### Import Dog Dataset\n",
    "\n",
    "In the code cell below, we import a dataset of dog images.  We populate a few variables through the use of the `load_files` function from the scikit-learn library:\n",
    "- `train_files`, `valid_files`, `test_files` - numpy arrays containing file paths to images\n",
    "- `train_targets`, `valid_targets`, `test_targets` - numpy arrays containing onehot-encoded classification labels \n",
    "- `dog_names` - list of string-valued dog breed names for translating labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'slice'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-824df84fad93>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# load train, test, and validation datasets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mtrain_files\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_targets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'dogImages/train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[0mvalid_files\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_targets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'dogImages/valid'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mtest_files\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_targets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'dogImages/test'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-824df84fad93>\u001b[0m in \u001b[0;36mload_dataset\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_files\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mdog_files\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'filenames'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mdog_targets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'target'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m133\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'slice'"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files       \n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "# define function to load train, test, and validation datasets\n",
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    print(data['filenames'][:10])\n",
    "    dog_files = np.array(data['filenames'])\n",
    "    dog_targets = np_utils.to_categorical(np.array(data['target']), 133)\n",
    "    return dog_files, dog_targets\n",
    "\n",
    "# load train, test, and validation datasets\n",
    "train_files, train_targets = load_dataset('dogImages/train')\n",
    "valid_files, valid_targets = load_dataset('dogImages/valid')\n",
    "test_files, test_targets = load_dataset('dogImages/test')\n",
    "\n",
    "# load list of dog names\n",
    "dog_names = [item[20:-1] for item in sorted(glob(\"dogImages/train/*/\"))]\n",
    "\n",
    "# print statistics about the dataset\n",
    "print('There are %d total dog categories.' % len(dog_names))\n",
    "print('There are %s total dog images.\\n' % len(np.hstack([train_files, valid_files, test_files])))\n",
    "print('There are %d training dog images.' % len(train_files))\n",
    "print('There are %d validation dog images.' % len(valid_files))\n",
    "print('There are %d test dog images.'% len(test_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dogImages/train\\057.Dalmatian\\Dalmatian_04054.jpg\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.]\n",
      "['Affenpinscher', 'Afghan_hound', 'Airedale_terrier', 'Akita', 'Alaskan_malamute', 'American_eskimo_dog', 'American_foxhound', 'American_staffordshire_terrier', 'American_water_spaniel', 'Anatolian_shepherd_dog']\n"
     ]
    }
   ],
   "source": [
    "print(train_files[1])\n",
    "\n",
    "print(train_targets[1])\n",
    "\n",
    "print(dog_names[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Human Dataset\n",
    "\n",
    "In the code cell below, we import a dataset of human images, where the file paths are stored in the numpy array `human_files`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 13233 total human images.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(8675309)\n",
    "\n",
    "# load filenames in shuffled human dataset\n",
    "human_files = np.array(glob(\"lfw/*/*\"))\n",
    "random.shuffle(human_files)\n",
    "\n",
    "# print statistics about the dataset\n",
    "print('There are %d total human images.' % len(human_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step1'></a>\n",
    "## Step 1: Detect Humans\n",
    "\n",
    "We use OpenCV's implementation of [Haar feature-based cascade classifiers](http://docs.opencv.org/trunk/d7/d8b/tutorial_py_face_detection.html) to detect human faces in images.  OpenCV provides many pre-trained face detectors, stored as XML files on [github](https://github.com/opencv/opencv/tree/master/data/haarcascades).  We have downloaded one of these detectors and stored it in the `haarcascades` directory.\n",
    "\n",
    "In the next code cell, we demonstrate how to use this detector to find human faces in a sample image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of faces detected: 1\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQsAAAD8CAYAAABgtYFHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x22e84e1bef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2                \n",
    "import matplotlib.pyplot as plt                        \n",
    "%matplotlib inline                               \n",
    "\n",
    "# extract pre-trained face detector\n",
    "face_cascade = cv2.CascadeClassifier('haarcascades/haarcascade_frontalface_alt.xml')\n",
    "\n",
    "# load color (BGR) image\n",
    "img = cv2.imread(human_files[3])\n",
    "# convert BGR image to grayscale\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# find faces in image\n",
    "faces = face_cascade.detectMultiScale(gray)\n",
    "\n",
    "# print number of faces detected in the image\n",
    "print('Number of faces detected:', len(faces))\n",
    "\n",
    "# get bounding box for each detected face\n",
    "for (x,y,w,h) in faces:\n",
    "    # add bounding box to color image\n",
    "    cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "    \n",
    "# convert BGR image to RGB for plotting\n",
    "cv_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# display the image, along with bounding box\n",
    "plt.imshow(cv_rgb)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using any of the face detectors, it is standard procedure to convert the images to grayscale.  The `detectMultiScale` function executes the classifier stored in `face_cascade` and takes the grayscale image as a parameter.  \n",
    "\n",
    "In the above code, `faces` is a numpy array of detected faces, where each row corresponds to a detected face.  Each detected face is a 1D array with four entries that specifies the bounding box of the detected face.  The first two entries in the array (extracted in the above code as `x` and `y`) specify the horizontal and vertical positions of the top left corner of the bounding box.  The last two entries in the array (extracted here as `w` and `h`) specify the width and height of the box.\n",
    "\n",
    "### Write a Human Face Detector\n",
    "\n",
    "We can use this procedure to write a function that returns `True` if a human face is detected in an image and `False` otherwise.  This function, aptly named `face_detector`, takes a string-valued file path to an image as input and appears in the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns \"True\" if face is detected in image stored at img_path\n",
    "def face_detector(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray)\n",
    "    return len(faces) > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Assess the Human Face Detector\n",
    "\n",
    "__Question 1:__ Use the code cell below to test the performance of the `face_detector` function.  \n",
    "- What percentage of the first 100 images in `human_files` have a detected human face?  \n",
    "- What percentage of the first 100 images in `dog_files` have a detected human face? \n",
    "\n",
    "Ideally, we would like 100% of human images with a detected face and 0% of dog images with a detected face.  You will see that our algorithm falls short of this goal, but still gives acceptable performance.  We extract the file paths for the first 100 images from each of the datasets and store them in the numpy arrays `human_files_short` and `dog_files_short`.\n",
    "\n",
    "__Answer:__ \n",
    "\n",
    "98% human faces detected in the human_files set\n",
    "\n",
    "11% human faces detected in the dog_files set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "human_files_short = human_files[:100]\n",
    "dog_files_short = train_files[:100]\n",
    "# Do NOT modify the code above this line.\n",
    "\n",
    "## TODO: Test the performance of the face_detector algorithm \n",
    "## on the images in human_files_short and dog_files_short.\n",
    "num_of_humans_in_humans_set = 0\n",
    "num_of_humans_in_dogs_set = 0\n",
    "for img in human_files_short:\n",
    "    if face_detector(img):\n",
    "        num_of_humans_in_humans_set = num_of_humans_in_humans_set + 1\n",
    "        \n",
    "for dogimg in dog_files_short:\n",
    "    if face_detector(dogimg):\n",
    "        num_of_humans_in_dogs_set = num_of_humans_in_dogs_set + 1\n",
    "\n",
    "print(num_of_humans_in_humans_set)\n",
    "print(num_of_humans_in_dogs_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 2:__ This algorithmic choice necessitates that we communicate to the user that we accept human images only when they provide a clear view of a face (otherwise, we risk having unneccessarily frustrated users!). In your opinion, is this a reasonable expectation to pose on the user? If not, can you think of a way to detect humans in images that does not necessitate an image with a clearly presented face?\n",
    "\n",
    "__Answer:__\n",
    "\n",
    "No, It is not so reasonable because we can still see faces in not clear image. I learned how haar cascade works for face detection and I believe it would be better we added some patterns how to half turned face for example and a half dark face. \n",
    "\n",
    "We suggest the face detector from OpenCV as a potential way to detect human images in your algorithm, but you are free to explore other approaches, especially approaches that make use of deep learning :).  Please use the code cell below to design and test your own face detection algorithm.  If you decide to pursue this _optional_ task, report performance on each of the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## (Optional) TODO: Report the performance of another  \n",
    "## face detection algorithm on the LFW dataset\n",
    "### Feel free to use as many code cells as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step2'></a>\n",
    "## Step 2: Detect Dogs\n",
    "\n",
    "In this section, we use a pre-trained [ResNet-50](http://ethereon.github.io/netscope/#/gist/db945b393d40bfa26006) model to detect dogs in images.  Our first line of code downloads the ResNet-50 model, along with weights that have been trained on [ImageNet](http://www.image-net.org/), a very large, very popular dataset used for image classification and other vision tasks.  ImageNet contains over 10 million URLs, each linking to an image containing an object from one of [1000 categories](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a).  Given an image, this pre-trained ResNet-50 model returns a prediction (derived from the available categories in ImageNet) for the object that is contained in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels.h5\n",
      "102825984/102853048 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "from keras.applications.resnet50 import ResNet50\n",
    "\n",
    "# define ResNet50 model\n",
    "ResNet50_model = ResNet50(weights='imagenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process the Data\n",
    "\n",
    "When using TensorFlow as backend, Keras CNNs require a 4D array (which we'll also refer to as a 4D tensor) as input, with shape\n",
    "\n",
    "$$\n",
    "(\\text{nb_samples}, \\text{rows}, \\text{columns}, \\text{channels}),\n",
    "$$\n",
    "\n",
    "where `nb_samples` corresponds to the total number of images (or samples), and `rows`, `columns`, and `channels` correspond to the number of rows, columns, and channels for each image, respectively.  \n",
    "\n",
    "The `path_to_tensor` function below takes a string-valued file path to a color image as input and returns a 4D tensor suitable for supplying to a Keras CNN.  The function first loads the image and resizes it to a square image that is $224 \\times 224$ pixels.  Next, the image is converted to an array, which is then resized to a 4D tensor.  In this case, since we are working with color images, each image has three channels.  Likewise, since we are processing a single image (or sample), the returned tensor will always have shape\n",
    "\n",
    "$$\n",
    "(1, 224, 224, 3).\n",
    "$$\n",
    "\n",
    "The `paths_to_tensor` function takes a numpy array of string-valued image paths as input and returns a 4D tensor with shape \n",
    "\n",
    "$$\n",
    "(\\text{nb_samples}, 224, 224, 3).\n",
    "$$\n",
    "\n",
    "Here, `nb_samples` is the number of samples, or number of images, in the supplied array of image paths.  It is best to think of `nb_samples` as the number of 3D tensors (where each 3D tensor corresponds to a different image) in your dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image                  \n",
    "    from tqdm import tqdm\n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions with ResNet-50\n",
    "\n",
    "Getting the 4D tensor ready for ResNet-50, and for any other pre-trained model in Keras, requires some additional processing.  First, the RGB image is converted to BGR by reordering the channels.  All pre-trained models have the additional normalization step that the mean pixel (expressed in RGB as $[103.939, 116.779, 123.68]$ and calculated from all pixels in all images in ImageNet) must be subtracted from every pixel in each image.  This is implemented in the imported function `preprocess_input`.  If you're curious, you can check the code for `preprocess_input` [here](https://github.com/fchollet/keras/blob/master/keras/applications/imagenet_utils.py).\n",
    "\n",
    "Now that we have a way to format our image for supplying to ResNet-50, we are now ready to use the model to extract the predictions.  This is accomplished with the `predict` method, which returns an array whose $i$-th entry is the model's predicted probability that the image belongs to the $i$-th ImageNet category.  This is implemented in the `ResNet50_predict_labels` function below.\n",
    "\n",
    "By taking the argmax of the predicted probability vector, we obtain an integer corresponding to the model's predicted object class, which we can identify with an object category through the use of this [dictionary](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "\n",
    "def ResNet50_predict_labels(img_path):\n",
    "    # returns prediction vector for image located at img_path\n",
    "    img = preprocess_input(path_to_tensor(img_path))\n",
    "    return np.argmax(ResNet50_model.predict(img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a Dog Detector\n",
    "\n",
    "While looking at the [dictionary](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a), you will notice that the categories corresponding to dogs appear in an uninterrupted sequence and correspond to dictionary keys 151-268, inclusive, to include all categories from `'Chihuahua'` to `'Mexican hairless'`.  Thus, in order to check to see if an image is predicted to contain a dog by the pre-trained ResNet-50 model, we need only check if the `ResNet50_predict_labels` function above returns a value between 151 and 268 (inclusive).\n",
    "\n",
    "We use these ideas to complete the `dog_detector` function below, which returns `True` if a dog is detected in an image (and `False` if not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### returns \"True\" if a dog is detected in the image stored at img_path\n",
    "def dog_detector(img_path):\n",
    "    prediction = ResNet50_predict_labels(img_path)\n",
    "    return ((prediction <= 268) & (prediction >= 151)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Assess the Dog Detector\n",
    "\n",
    "__Question 3:__ Use the code cell below to test the performance of your `dog_detector` function.  \n",
    "- What percentage of the images in `human_files_short` have a detected dog?  \n",
    "- What percentage of the images in `dog_files_short` have a detected dog?\n",
    "\n",
    "__Answer:__ \n",
    "\n",
    "2% dogs in human files\n",
    "\n",
    "100% dogs in dogs files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "### TODO: Test the performance of the dog_detector function\n",
    "### on the images in human_files_short and dog_files_short.\n",
    "dogs_in_human_files = 0\n",
    "dogs_in_dogs_files = 0\n",
    "for human in human_files_short:\n",
    "    if dog_detector(human):\n",
    "        dogs_in_human_files = dogs_in_human_files + 1\n",
    "\n",
    "for dog in dog_files_short:\n",
    "    if dog_detector(dog):\n",
    "        dogs_in_dogs_files = dogs_in_dogs_files + 1\n",
    "        \n",
    "print(dogs_in_human_files)\n",
    "print(dogs_in_dogs_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step3'></a>\n",
    "## Step 3: Create a CNN to Classify Dog Breeds (from Scratch)\n",
    "\n",
    "Now that we have functions for detecting humans and dogs in images, we need a way to predict breed from images.  In this step, you will create a CNN that classifies dog breeds.  You must create your CNN _from scratch_ (so, you can't use transfer learning _yet_!), and you must attain a test accuracy of at least 1%.  In Step 5 of this notebook, you will have the opportunity to use transfer learning to create a CNN that attains greatly improved accuracy.\n",
    "\n",
    "Be careful with adding too many trainable layers!  More parameters means longer training, which means you are more likely to need a GPU to accelerate the training process.  Thankfully, Keras provides a handy estimate of the time that each epoch is likely to take; you can extrapolate this estimate to figure out how long it will take for your algorithm to train. \n",
    "\n",
    "We mention that the task of assigning breed to dogs from images is considered exceptionally challenging.  To see why, consider that *even a human* would have great difficulty in distinguishing between a Brittany and a Welsh Springer Spaniel.  \n",
    "\n",
    "Brittany | Welsh Springer Spaniel\n",
    "- | - \n",
    "<img src=\"images/Brittany_02625.jpg\" width=\"100\"> | <img src=\"images/Welsh_springer_spaniel_08203.jpg\" width=\"200\">\n",
    "\n",
    "It is not difficult to find other dog breed pairs with minimal inter-class variation (for instance, Curly-Coated Retrievers and American Water Spaniels).  \n",
    "\n",
    "Curly-Coated Retriever | American Water Spaniel\n",
    "- | -\n",
    "<img src=\"images/Curly-coated_retriever_03896.jpg\" width=\"200\"> | <img src=\"images/American_water_spaniel_00648.jpg\" width=\"200\">\n",
    "\n",
    "\n",
    "Likewise, recall that labradors come in yellow, chocolate, and black.  Your vision-based algorithm will have to conquer this high intra-class variation to determine how to classify all of these different shades as the same breed.  \n",
    "\n",
    "Yellow Labrador | Chocolate Labrador | Black Labrador\n",
    "- | -\n",
    "<img src=\"images/Labrador_retriever_06457.jpg\" width=\"150\"> | <img src=\"images/Labrador_retriever_06455.jpg\" width=\"240\"> | <img src=\"images/Labrador_retriever_06449.jpg\" width=\"220\">\n",
    "\n",
    "We also mention that random chance presents an exceptionally low bar: setting aside the fact that the classes are slightly imabalanced, a random guess will provide a correct answer roughly 1 in 133 times, which corresponds to an accuracy of less than 1%.  \n",
    "\n",
    "Remember that the practice is far ahead of the theory in deep learning.  Experiment with many different architectures, and trust your intuition.  And, of course, have fun! \n",
    "\n",
    "### Pre-process the Data\n",
    "\n",
    "We rescale the images by dividing every pixel in every image by 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 6680/6680 [00:54<00:00, 122.44it/s]\n",
      "100%|| 835/835 [00:06<00:00, 136.88it/s]\n",
      "100%|| 836/836 [00:06<00:00, 137.78it/s]\n"
     ]
    }
   ],
   "source": [
    "from PIL import ImageFile                            \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True                 \n",
    "\n",
    "# pre-process the data for Keras\n",
    "train_tensors = paths_to_tensor(train_files).astype('float32')/255\n",
    "valid_tensors = paths_to_tensor(valid_files).astype('float32')/255\n",
    "test_tensors = paths_to_tensor(test_files).astype('float32')/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Model Architecture\n",
    "\n",
    "Create a CNN to classify dog breed.  At the end of your code cell block, summarize the layers of your model by executing the line:\n",
    "    \n",
    "        model.summary()\n",
    "\n",
    "We have imported some Python modules to get you started, but feel free to import as many modules as you need.  If you end up getting stuck, here's a hint that specifies a model that trains relatively fast on CPU and attains >1% test accuracy in 5 epochs:\n",
    "\n",
    "![Sample CNN](images/sample_cnn.png)\n",
    "           \n",
    "__Question 4:__ Outline the steps you took to get to your final CNN architecture and your reasoning at each step.  If you chose to use the hinted architecture above, describe why you think that CNN architecture should work well for the image classification task.\n",
    "\n",
    "__Answer:__ \n",
    " - The first convolutional layer is usually used to extract color features and also define the images shape.\n",
    " - The MaxPooling presents in the second line of the model to take the max of each region created by the first layer and output a matrix where each element is the max of the previous regions of the original input.\n",
    " - The second layer is used to detect objects such as circles, stripes.. etc.\n",
    " - Again a MaxPooling for scaling down the image.\n",
    " - Finally, the last conv layer for detecting patterns inside the image.\n",
    " - One more MaxPooling for scaling down\n",
    " - GlobalAveragePooling is present here for previnting overfitting.\n",
    " - The last thing is the `softmax` activation function to turn values into probabilities.\n",
    " \n",
    " I think this architecture is good for image classification task because it helps reduce the number of parameters which is performance critical issue for network training time. And the 3 layers architecture follows a good procedure toward extracting features within images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 224, 224, 16)      208       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 112, 112, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 112, 112, 32)      2080      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 56, 56, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 56, 56, 64)        8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 133)               8645      \n",
      "=================================================================\n",
      "Total params: 19,189.0\n",
      "Trainable params: 19,189.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "### TODO: Define your architecture.\n",
    "model.add(Conv2D(filters=16, kernel_size=2, padding='same', activation='relu', input_shape=(224, 224, 3)))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Conv2D(filters=64, kernel_size=2, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(Dense(133, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Train the Model\n",
    "\n",
    "Train your model in the code cell below.  Use model checkpointing to save the model that attains the best validation loss.\n",
    "\n",
    "You are welcome to [augment the training data](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html), but this is not a requirement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/10\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 4.8840 - acc: 0.0093Epoch 00000: val_loss improved from inf to 4.86776, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "6680/6680 [==============================] - 23s - loss: 4.8841 - acc: 0.0093 - val_loss: 4.8678 - val_acc: 0.0084\n",
      "Epoch 2/10\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 4.8614 - acc: 0.0107Epoch 00001: val_loss improved from 4.86776 to 4.84597, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "6680/6680 [==============================] - 22s - loss: 4.8612 - acc: 0.0106 - val_loss: 4.8460 - val_acc: 0.0168\n",
      "Epoch 3/10\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 4.8225 - acc: 0.0150Epoch 00002: val_loss improved from 4.84597 to 4.81413, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "6680/6680 [==============================] - 22s - loss: 4.8228 - acc: 0.0151 - val_loss: 4.8141 - val_acc: 0.0228\n",
      "Epoch 4/10\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 4.7880 - acc: 0.0201Epoch 00003: val_loss improved from 4.81413 to 4.79173, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "6680/6680 [==============================] - 22s - loss: 4.7876 - acc: 0.0201 - val_loss: 4.7917 - val_acc: 0.0144\n",
      "Epoch 5/10\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 4.7570 - acc: 0.0182Epoch 00004: val_loss improved from 4.79173 to 4.76593, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "6680/6680 [==============================] - 23s - loss: 4.7574 - acc: 0.0181 - val_loss: 4.7659 - val_acc: 0.0156\n",
      "Epoch 6/10\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 4.7282 - acc: 0.0236Epoch 00005: val_loss improved from 4.76593 to 4.74853, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "6680/6680 [==============================] - 22s - loss: 4.7286 - acc: 0.0235 - val_loss: 4.7485 - val_acc: 0.0168\n",
      "Epoch 7/10\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 4.7013 - acc: 0.0284Epoch 00006: val_loss improved from 4.74853 to 4.74561, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "6680/6680 [==============================] - 22s - loss: 4.7014 - acc: 0.0283 - val_loss: 4.7456 - val_acc: 0.0240\n",
      "Epoch 8/10\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 4.6752 - acc: 0.0291- EEpoch 00007: val_loss improved from 4.74561 to 4.71587, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "6680/6680 [==============================] - 22s - loss: 4.6753 - acc: 0.0292 - val_loss: 4.7159 - val_acc: 0.0192\n",
      "Epoch 9/10\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 4.6459 - acc: 0.0338Epoch 00008: val_loss improved from 4.71587 to 4.71353, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "6680/6680 [==============================] - 22s - loss: 4.6450 - acc: 0.0338 - val_loss: 4.7135 - val_acc: 0.0180\n",
      "Epoch 10/10\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 4.6190 - acc: 0.0369Epoch 00009: val_loss improved from 4.71353 to 4.70215, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "6680/6680 [==============================] - 22s - loss: 4.6185 - acc: 0.0370 - val_loss: 4.7022 - val_acc: 0.0263\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f86a002f198>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint  \n",
    "\n",
    "### TODO: specify the number of epochs that you would like to use to train the model.\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "### Do NOT modify the code below this line.\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.from_scratch.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "model.fit(train_tensors, train_targets, \n",
    "          validation_data=(valid_tensors, valid_targets),\n",
    "          epochs=epochs, batch_size=20, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Model with the Best Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('saved_models/weights.best.from_scratch.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model\n",
    "\n",
    "Try out your model on the test dataset of dog images.  Ensure that your test accuracy is greater than 1%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 4.4258%\n"
     ]
    }
   ],
   "source": [
    "# get index of predicted dog breed for each image in test set\n",
    "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step4'></a>\n",
    "## Step 4: Use a CNN to Classify Dog Breeds\n",
    "\n",
    "To reduce training time without sacrificing accuracy, we show you how to train a CNN using transfer learning.  In the following step, you will get a chance to use transfer learning to train your own CNN.\n",
    "\n",
    "### Obtain Bottleneck Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottleneck_features = np.load('bottleneck_features/DogVGG16Data.npz')\n",
    "train_VGG16 = bottleneck_features['train']\n",
    "valid_VGG16 = bottleneck_features['valid']\n",
    "test_VGG16 = bottleneck_features['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "\n",
    "The model uses the the pre-trained VGG-16 model as a fixed feature extractor, where the last convolutional output of VGG-16 is fed as input to our model.  We only add a global average pooling layer and a fully connected layer, where the latter contains one node for each dog category and is equipped with a softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "global_average_pooling2d_1 ( (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 133)               68229     \n",
      "=================================================================\n",
      "Total params: 68,229\n",
      "Trainable params: 68,229\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "\n",
    "VGG16_model = Sequential()\n",
    "VGG16_model.add(GlobalAveragePooling2D(input_shape=train_VGG16.shape[1:]))\n",
    "VGG16_model.add(Dense(133, activation='softmax'))\n",
    "\n",
    "VGG16_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG16_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/100\n",
      "6580/6680 [============================>.] - ETA: 284s - loss: 14.4864 - acc: 0.0000e+ - ETA: 59s - loss: 14.7559 - acc: 0.0200     - ETA: 28s - loss: 14.6152 - acc: 0.013 - ETA: 18s - loss: 14.6177 - acc: 0.017 - ETA: 14s - loss: 14.6468 - acc: 0.019 - ETA: 11s - loss: 14.5172 - acc: 0.024 - ETA: 10s - loss: 14.3795 - acc: 0.031 - ETA: 8s - loss: 14.2926 - acc: 0.031 - ETA: 7s - loss: 14.1904 - acc: 0.03 - ETA: 7s - loss: 14.1072 - acc: 0.03 - ETA: 6s - loss: 14.0028 - acc: 0.04 - ETA: 5s - loss: 13.9593 - acc: 0.04 - ETA: 5s - loss: 13.7790 - acc: 0.04 - ETA: 5s - loss: 13.7538 - acc: 0.04 - ETA: 4s - loss: 13.6858 - acc: 0.05 - ETA: 4s - loss: 13.6438 - acc: 0.05 - ETA: 4s - loss: 13.5378 - acc: 0.06 - ETA: 4s - loss: 13.4639 - acc: 0.06 - ETA: 3s - loss: 13.4115 - acc: 0.06 - ETA: 3s - loss: 13.3521 - acc: 0.07 - ETA: 3s - loss: 13.3206 - acc: 0.07 - ETA: 3s - loss: 13.2746 - acc: 0.07 - ETA: 3s - loss: 13.2038 - acc: 0.07 - ETA: 3s - loss: 13.1946 - acc: 0.07 - ETA: 2s - loss: 13.1126 - acc: 0.08 - ETA: 2s - loss: 13.0471 - acc: 0.08 - ETA: 2s - loss: 13.0281 - acc: 0.08 - ETA: 2s - loss: 12.9734 - acc: 0.08 - ETA: 2s - loss: 12.9446 - acc: 0.09 - ETA: 2s - loss: 12.8918 - acc: 0.09 - ETA: 2s - loss: 12.8798 - acc: 0.09 - ETA: 2s - loss: 12.8754 - acc: 0.09 - ETA: 1s - loss: 12.8387 - acc: 0.09 - ETA: 1s - loss: 12.8217 - acc: 0.09 - ETA: 1s - loss: 12.7961 - acc: 0.09 - ETA: 1s - loss: 12.7579 - acc: 0.09 - ETA: 1s - loss: 12.7174 - acc: 0.10 - ETA: 1s - loss: 12.6606 - acc: 0.10 - ETA: 1s - loss: 12.6033 - acc: 0.10 - ETA: 1s - loss: 12.5776 - acc: 0.10 - ETA: 1s - loss: 12.5482 - acc: 0.10 - ETA: 1s - loss: 12.4803 - acc: 0.11 - ETA: 1s - loss: 12.4513 - acc: 0.11 - ETA: 1s - loss: 12.4443 - acc: 0.11 - ETA: 0s - loss: 12.3957 - acc: 0.11 - ETA: 0s - loss: 12.3763 - acc: 0.11 - ETA: 0s - loss: 12.3459 - acc: 0.11 - ETA: 0s - loss: 12.3220 - acc: 0.12 - ETA: 0s - loss: 12.2852 - acc: 0.12 - ETA: 0s - loss: 12.2644 - acc: 0.12 - ETA: 0s - loss: 12.2420 - acc: 0.12 - ETA: 0s - loss: 12.2017 - acc: 0.12 - ETA: 0s - loss: 12.1678 - acc: 0.13 - ETA: 0s - loss: 12.1545 - acc: 0.13 - ETA: 0s - loss: 12.1451 - acc: 0.13 - ETA: 0s - loss: 12.1329 - acc: 0.13 - ETA: 0s - loss: 12.1083 - acc: 0.1333Epoch 00000: val_loss improved from inf to 10.53732, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 12.1033 - acc: 0.1341 - val_loss: 10.5373 - val_acc: 0.2216\n",
      "Epoch 2/100\n",
      "6640/6680 [============================>.] - ETA: 2s - loss: 11.7374 - acc: 0.20 - ETA: 2s - loss: 10.3931 - acc: 0.26 - ETA: 2s - loss: 9.7518 - acc: 0.2808 - ETA: 2s - loss: 9.8767 - acc: 0.276 - ETA: 2s - loss: 10.0918 - acc: 0.27 - ETA: 2s - loss: 10.1770 - acc: 0.26 - ETA: 2s - loss: 10.2740 - acc: 0.26 - ETA: 2s - loss: 10.1908 - acc: 0.27 - ETA: 2s - loss: 10.1953 - acc: 0.27 - ETA: 2s - loss: 10.2190 - acc: 0.27 - ETA: 2s - loss: 10.2418 - acc: 0.26 - ETA: 2s - loss: 10.2613 - acc: 0.26 - ETA: 2s - loss: 10.2877 - acc: 0.26 - ETA: 2s - loss: 10.3056 - acc: 0.26 - ETA: 2s - loss: 10.3033 - acc: 0.26 - ETA: 2s - loss: 10.2288 - acc: 0.27 - ETA: 2s - loss: 10.1810 - acc: 0.27 - ETA: 2s - loss: 10.1910 - acc: 0.27 - ETA: 2s - loss: 10.2267 - acc: 0.27 - ETA: 2s - loss: 10.2478 - acc: 0.27 - ETA: 1s - loss: 10.2483 - acc: 0.27 - ETA: 1s - loss: 10.1694 - acc: 0.27 - ETA: 1s - loss: 10.1273 - acc: 0.28 - ETA: 1s - loss: 10.1362 - acc: 0.28 - ETA: 1s - loss: 10.0963 - acc: 0.28 - ETA: 1s - loss: 10.1394 - acc: 0.28 - ETA: 1s - loss: 10.1512 - acc: 0.28 - ETA: 1s - loss: 10.1736 - acc: 0.28 - ETA: 1s - loss: 10.1720 - acc: 0.28 - ETA: 1s - loss: 10.1950 - acc: 0.27 - ETA: 1s - loss: 10.1675 - acc: 0.28 - ETA: 1s - loss: 10.1022 - acc: 0.28 - ETA: 1s - loss: 10.0633 - acc: 0.28 - ETA: 1s - loss: 10.0695 - acc: 0.28 - ETA: 1s - loss: 10.0567 - acc: 0.28 - ETA: 1s - loss: 10.0820 - acc: 0.28 - ETA: 1s - loss: 10.0567 - acc: 0.28 - ETA: 1s - loss: 10.0516 - acc: 0.28 - ETA: 1s - loss: 10.0510 - acc: 0.28 - ETA: 0s - loss: 10.0491 - acc: 0.28 - ETA: 0s - loss: 10.0401 - acc: 0.28 - ETA: 0s - loss: 10.0609 - acc: 0.28 - ETA: 0s - loss: 10.0726 - acc: 0.28 - ETA: 0s - loss: 10.0857 - acc: 0.28 - ETA: 0s - loss: 10.0630 - acc: 0.28 - ETA: 0s - loss: 10.0773 - acc: 0.28 - ETA: 0s - loss: 10.0858 - acc: 0.28 - ETA: 0s - loss: 10.0721 - acc: 0.28 - ETA: 0s - loss: 10.0661 - acc: 0.29 - ETA: 0s - loss: 10.0995 - acc: 0.28 - ETA: 0s - loss: 10.1059 - acc: 0.28 - ETA: 0s - loss: 10.0778 - acc: 0.28 - ETA: 0s - loss: 10.0912 - acc: 0.28 - ETA: 0s - loss: 10.0806 - acc: 0.29 - ETA: 0s - loss: 10.0826 - acc: 0.29 - ETA: 0s - loss: 10.0680 - acc: 0.29 - ETA: 0s - loss: 10.0843 - acc: 0.29 - ETA: 0s - loss: 10.0873 - acc: 0.2914Epoch 00001: val_loss improved from 10.53732 to 10.11589, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 10.0848 - acc: 0.2919 - val_loss: 10.1159 - val_acc: 0.2814\n",
      "Epoch 3/100\n",
      "6580/6680 [============================>.] - ETA: 3s - loss: 10.1934 - acc: 0.30 - ETA: 2s - loss: 10.0100 - acc: 0.32 - ETA: 2s - loss: 10.9027 - acc: 0.27 - ETA: 2s - loss: 10.8704 - acc: 0.26 - ETA: 2s - loss: 10.7391 - acc: 0.27 - ETA: 2s - loss: 10.5966 - acc: 0.27 - ETA: 2s - loss: 10.3999 - acc: 0.29 - ETA: 2s - loss: 10.1772 - acc: 0.31 - ETA: 2s - loss: 9.9721 - acc: 0.3214 - ETA: 2s - loss: 9.7947 - acc: 0.335 - ETA: 2s - loss: 9.7849 - acc: 0.336 - ETA: 2s - loss: 9.7666 - acc: 0.338 - ETA: 2s - loss: 9.7991 - acc: 0.336 - ETA: 2s - loss: 9.7962 - acc: 0.338 - ETA: 2s - loss: 9.7741 - acc: 0.339 - ETA: 2s - loss: 9.7509 - acc: 0.342 - ETA: 2s - loss: 9.7520 - acc: 0.343 - ETA: 2s - loss: 9.7290 - acc: 0.344 - ETA: 2s - loss: 9.6746 - acc: 0.346 - ETA: 2s - loss: 9.6782 - acc: 0.347 - ETA: 1s - loss: 9.6759 - acc: 0.345 - ETA: 1s - loss: 9.6854 - acc: 0.344 - ETA: 1s - loss: 9.6842 - acc: 0.343 - ETA: 1s - loss: 9.6586 - acc: 0.345 - ETA: 1s - loss: 9.7115 - acc: 0.342 - ETA: 1s - loss: 9.7350 - acc: 0.340 - ETA: 1s - loss: 9.7497 - acc: 0.340 - ETA: 1s - loss: 9.7349 - acc: 0.341 - ETA: 1s - loss: 9.7041 - acc: 0.342 - ETA: 1s - loss: 9.6949 - acc: 0.343 - ETA: 1s - loss: 9.6904 - acc: 0.343 - ETA: 1s - loss: 9.6724 - acc: 0.345 - ETA: 1s - loss: 9.6694 - acc: 0.346 - ETA: 1s - loss: 9.6816 - acc: 0.345 - ETA: 1s - loss: 9.6822 - acc: 0.344 - ETA: 1s - loss: 9.6821 - acc: 0.344 - ETA: 1s - loss: 9.7198 - acc: 0.342 - ETA: 1s - loss: 9.7167 - acc: 0.342 - ETA: 1s - loss: 9.6811 - acc: 0.345 - ETA: 0s - loss: 9.6747 - acc: 0.346 - ETA: 0s - loss: 9.6814 - acc: 0.346 - ETA: 0s - loss: 9.6478 - acc: 0.348 - ETA: 0s - loss: 9.6386 - acc: 0.348 - ETA: 0s - loss: 9.6585 - acc: 0.347 - ETA: 0s - loss: 9.6773 - acc: 0.346 - ETA: 0s - loss: 9.6755 - acc: 0.347 - ETA: 0s - loss: 9.6763 - acc: 0.346 - ETA: 0s - loss: 9.6662 - acc: 0.346 - ETA: 0s - loss: 9.6730 - acc: 0.346 - ETA: 0s - loss: 9.6654 - acc: 0.346 - ETA: 0s - loss: 9.6673 - acc: 0.346 - ETA: 0s - loss: 9.6473 - acc: 0.347 - ETA: 0s - loss: 9.6629 - acc: 0.347 - ETA: 0s - loss: 9.6561 - acc: 0.348 - ETA: 0s - loss: 9.6663 - acc: 0.347 - ETA: 0s - loss: 9.6400 - acc: 0.349 - ETA: 0s - loss: 9.6372 - acc: 0.3500Epoch 00002: val_loss improved from 10.11589 to 9.86976, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 9.6755 - acc: 0.3479 - val_loss: 9.8698 - val_acc: 0.3114\n",
      "Epoch 4/100\n",
      "6620/6680 [============================>.] - ETA: 5s - loss: 9.7060 - acc: 0.400 - ETA: 3s - loss: 8.2901 - acc: 0.421 - ETA: 3s - loss: 8.6016 - acc: 0.423 - ETA: 2s - loss: 8.6228 - acc: 0.426 - ETA: 2s - loss: 9.0738 - acc: 0.406 - ETA: 2s - loss: 9.2792 - acc: 0.391 - ETA: 2s - loss: 9.5510 - acc: 0.376 - ETA: 2s - loss: 9.3157 - acc: 0.388 - ETA: 2s - loss: 9.2916 - acc: 0.392 - ETA: 2s - loss: 9.2431 - acc: 0.393 - ETA: 2s - loss: 9.2275 - acc: 0.392 - ETA: 2s - loss: 9.2252 - acc: 0.392 - ETA: 2s - loss: 9.2268 - acc: 0.392 - ETA: 2s - loss: 9.2206 - acc: 0.389 - ETA: 2s - loss: 9.1522 - acc: 0.393 - ETA: 2s - loss: 9.2153 - acc: 0.390 - ETA: 2s - loss: 9.2460 - acc: 0.387 - ETA: 2s - loss: 9.2502 - acc: 0.388 - ETA: 2s - loss: 9.2259 - acc: 0.388 - ETA: 2s - loss: 9.2489 - acc: 0.387 - ETA: 1s - loss: 9.1854 - acc: 0.392 - ETA: 1s - loss: 9.1927 - acc: 0.391 - ETA: 1s - loss: 9.2097 - acc: 0.391 - ETA: 1s - loss: 9.2527 - acc: 0.388 - ETA: 1s - loss: 9.2626 - acc: 0.388 - ETA: 1s - loss: 9.2542 - acc: 0.388 - ETA: 1s - loss: 9.2955 - acc: 0.386 - ETA: 1s - loss: 9.2942 - acc: 0.387 - ETA: 1s - loss: 9.3317 - acc: 0.384 - ETA: 1s - loss: 9.3606 - acc: 0.383 - ETA: 1s - loss: 9.3326 - acc: 0.384 - ETA: 1s - loss: 9.3212 - acc: 0.384 - ETA: 1s - loss: 9.3636 - acc: 0.380 - ETA: 1s - loss: 9.3672 - acc: 0.380 - ETA: 1s - loss: 9.3699 - acc: 0.380 - ETA: 1s - loss: 9.3783 - acc: 0.380 - ETA: 1s - loss: 9.3819 - acc: 0.379 - ETA: 1s - loss: 9.3814 - acc: 0.379 - ETA: 0s - loss: 9.3807 - acc: 0.379 - ETA: 0s - loss: 9.3318 - acc: 0.382 - ETA: 0s - loss: 9.3254 - acc: 0.382 - ETA: 0s - loss: 9.3298 - acc: 0.381 - ETA: 0s - loss: 9.3167 - acc: 0.382 - ETA: 0s - loss: 9.2981 - acc: 0.384 - ETA: 0s - loss: 9.3426 - acc: 0.381 - ETA: 0s - loss: 9.3619 - acc: 0.380 - ETA: 0s - loss: 9.3682 - acc: 0.379 - ETA: 0s - loss: 9.3818 - acc: 0.378 - ETA: 0s - loss: 9.3871 - acc: 0.378 - ETA: 0s - loss: 9.3909 - acc: 0.377 - ETA: 0s - loss: 9.3930 - acc: 0.377 - ETA: 0s - loss: 9.3870 - acc: 0.378 - ETA: 0s - loss: 9.3913 - acc: 0.377 - ETA: 0s - loss: 9.3950 - acc: 0.377 - ETA: 0s - loss: 9.3981 - acc: 0.376 - ETA: 0s - loss: 9.4120 - acc: 0.3758Epoch 00003: val_loss improved from 9.86976 to 9.70426, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 9.4054 - acc: 0.3763 - val_loss: 9.7043 - val_acc: 0.3305\n",
      "Epoch 5/100\n",
      "6620/6680 [============================>.] - ETA: 2s - loss: 10.8638 - acc: 0.30 - ETA: 2s - loss: 9.6601 - acc: 0.3812 - ETA: 2s - loss: 9.5982 - acc: 0.383 - ETA: 2s - loss: 9.2716 - acc: 0.400 - ETA: 2s - loss: 9.2648 - acc: 0.398 - ETA: 2s - loss: 9.2861 - acc: 0.398 - ETA: 2s - loss: 9.2863 - acc: 0.397 - ETA: 2s - loss: 9.4203 - acc: 0.388 - ETA: 2s - loss: 9.3547 - acc: 0.393 - ETA: 2s - loss: 9.4468 - acc: 0.385 - ETA: 2s - loss: 9.4867 - acc: 0.381 - ETA: 2s - loss: 9.4525 - acc: 0.382 - ETA: 1s - loss: 9.3510 - acc: 0.387 - ETA: 1s - loss: 9.2872 - acc: 0.392 - ETA: 1s - loss: 9.3145 - acc: 0.391 - ETA: 1s - loss: 9.3710 - acc: 0.388 - ETA: 1s - loss: 9.3554 - acc: 0.388 - ETA: 1s - loss: 9.3930 - acc: 0.386 - ETA: 1s - loss: 9.3821 - acc: 0.387 - ETA: 1s - loss: 9.3343 - acc: 0.389 - ETA: 1s - loss: 9.3521 - acc: 0.388 - ETA: 1s - loss: 9.3535 - acc: 0.388 - ETA: 1s - loss: 9.3600 - acc: 0.388 - ETA: 1s - loss: 9.3172 - acc: 0.392 - ETA: 1s - loss: 9.3095 - acc: 0.393 - ETA: 1s - loss: 9.3359 - acc: 0.392 - ETA: 1s - loss: 9.3421 - acc: 0.391 - ETA: 1s - loss: 9.3532 - acc: 0.391 - ETA: 1s - loss: 9.3380 - acc: 0.391 - ETA: 1s - loss: 9.3756 - acc: 0.389 - ETA: 1s - loss: 9.3931 - acc: 0.388 - ETA: 1s - loss: 9.3634 - acc: 0.390 - ETA: 1s - loss: 9.3758 - acc: 0.389 - ETA: 1s - loss: 9.3615 - acc: 0.389 - ETA: 0s - loss: 9.3835 - acc: 0.387 - ETA: 0s - loss: 9.3859 - acc: 0.388 - ETA: 0s - loss: 9.3982 - acc: 0.388 - ETA: 0s - loss: 9.3811 - acc: 0.389 - ETA: 0s - loss: 9.3705 - acc: 0.390 - ETA: 0s - loss: 9.3237 - acc: 0.393 - ETA: 0s - loss: 9.3178 - acc: 0.394 - ETA: 0s - loss: 9.3203 - acc: 0.394 - ETA: 0s - loss: 9.3240 - acc: 0.394 - ETA: 0s - loss: 9.3040 - acc: 0.394 - ETA: 0s - loss: 9.2856 - acc: 0.396 - ETA: 0s - loss: 9.2526 - acc: 0.398 - ETA: 0s - loss: 9.2601 - acc: 0.398 - ETA: 0s - loss: 9.2456 - acc: 0.399 - ETA: 0s - loss: 9.2410 - acc: 0.399 - ETA: 0s - loss: 9.2759 - acc: 0.397 - ETA: 0s - loss: 9.2812 - acc: 0.397 - ETA: 0s - loss: 9.2665 - acc: 0.3982Epoch 00004: val_loss improved from 9.70426 to 9.59453, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 9.2710 - acc: 0.3979 - val_loss: 9.5945 - val_acc: 0.3473\n",
      "Epoch 6/100\n",
      "6600/6680 [============================>.] - ETA: 3s - loss: 8.8693 - acc: 0.450 - ETA: 2s - loss: 8.4310 - acc: 0.468 - ETA: 2s - loss: 8.8668 - acc: 0.433 - ETA: 2s - loss: 8.9398 - acc: 0.429 - ETA: 2s - loss: 9.1666 - acc: 0.415 - ETA: 2s - loss: 9.2518 - acc: 0.408 - ETA: 2s - loss: 9.2777 - acc: 0.408 - ETA: 2s - loss: 9.2106 - acc: 0.412 - ETA: 2s - loss: 9.1837 - acc: 0.415 - ETA: 2s - loss: 9.1909 - acc: 0.412 - ETA: 2s - loss: 9.1748 - acc: 0.415 - ETA: 2s - loss: 9.1636 - acc: 0.416 - ETA: 2s - loss: 9.1408 - acc: 0.417 - ETA: 1s - loss: 9.1321 - acc: 0.418 - ETA: 1s - loss: 9.1259 - acc: 0.417 - ETA: 1s - loss: 9.1870 - acc: 0.412 - ETA: 1s - loss: 9.1113 - acc: 0.417 - ETA: 1s - loss: 9.0410 - acc: 0.421 - ETA: 1s - loss: 9.0382 - acc: 0.422 - ETA: 1s - loss: 9.0448 - acc: 0.421 - ETA: 1s - loss: 9.0526 - acc: 0.421 - ETA: 1s - loss: 9.0356 - acc: 0.422 - ETA: 1s - loss: 9.0420 - acc: 0.422 - ETA: 1s - loss: 9.0427 - acc: 0.422 - ETA: 1s - loss: 9.1104 - acc: 0.418 - ETA: 1s - loss: 9.1113 - acc: 0.418 - ETA: 1s - loss: 9.1450 - acc: 0.416 - ETA: 1s - loss: 9.1716 - acc: 0.415 - ETA: 1s - loss: 9.1767 - acc: 0.414 - ETA: 1s - loss: 9.1869 - acc: 0.413 - ETA: 1s - loss: 9.1994 - acc: 0.412 - ETA: 0s - loss: 9.1633 - acc: 0.414 - ETA: 0s - loss: 9.1647 - acc: 0.414 - ETA: 0s - loss: 9.1711 - acc: 0.413 - ETA: 0s - loss: 9.1961 - acc: 0.412 - ETA: 0s - loss: 9.2136 - acc: 0.410 - ETA: 0s - loss: 9.2348 - acc: 0.409 - ETA: 0s - loss: 9.2169 - acc: 0.410 - ETA: 0s - loss: 9.2231 - acc: 0.409 - ETA: 0s - loss: 9.2299 - acc: 0.408 - ETA: 0s - loss: 9.2349 - acc: 0.408 - ETA: 0s - loss: 9.2427 - acc: 0.408 - ETA: 0s - loss: 9.2430 - acc: 0.408 - ETA: 0s - loss: 9.2418 - acc: 0.408 - ETA: 0s - loss: 9.2344 - acc: 0.409 - ETA: 0s - loss: 9.2465 - acc: 0.408 - ETA: 0s - loss: 9.2264 - acc: 0.409 - ETA: 0s - loss: 9.2378 - acc: 0.408 - ETA: 0s - loss: 9.2225 - acc: 0.408 - ETA: 0s - loss: 9.2216 - acc: 0.4088Epoch 00005: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 9.2181 - acc: 0.4091 - val_loss: 9.6455 - val_acc: 0.3437\n",
      "Epoch 7/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6660/6680 [============================>.] - ETA: 2s - loss: 9.6712 - acc: 0.400 - ETA: 2s - loss: 9.0197 - acc: 0.428 - ETA: 2s - loss: 9.2385 - acc: 0.411 - ETA: 2s - loss: 9.2067 - acc: 0.418 - ETA: 2s - loss: 9.1292 - acc: 0.425 - ETA: 2s - loss: 9.3420 - acc: 0.410 - ETA: 2s - loss: 9.3348 - acc: 0.412 - ETA: 2s - loss: 9.3372 - acc: 0.413 - ETA: 2s - loss: 9.2481 - acc: 0.418 - ETA: 2s - loss: 9.2676 - acc: 0.416 - ETA: 2s - loss: 9.3031 - acc: 0.413 - ETA: 2s - loss: 9.2951 - acc: 0.414 - ETA: 2s - loss: 9.2996 - acc: 0.414 - ETA: 2s - loss: 9.2734 - acc: 0.415 - ETA: 2s - loss: 9.1469 - acc: 0.422 - ETA: 2s - loss: 9.1744 - acc: 0.420 - ETA: 2s - loss: 9.1941 - acc: 0.417 - ETA: 1s - loss: 9.2175 - acc: 0.416 - ETA: 1s - loss: 9.2013 - acc: 0.417 - ETA: 1s - loss: 9.1269 - acc: 0.421 - ETA: 1s - loss: 9.1327 - acc: 0.420 - ETA: 1s - loss: 9.1641 - acc: 0.417 - ETA: 1s - loss: 9.1636 - acc: 0.416 - ETA: 1s - loss: 9.1750 - acc: 0.416 - ETA: 1s - loss: 9.1309 - acc: 0.418 - ETA: 1s - loss: 9.1382 - acc: 0.418 - ETA: 1s - loss: 9.1355 - acc: 0.418 - ETA: 1s - loss: 9.1157 - acc: 0.419 - ETA: 1s - loss: 9.1472 - acc: 0.418 - ETA: 1s - loss: 9.1429 - acc: 0.418 - ETA: 1s - loss: 9.1196 - acc: 0.419 - ETA: 1s - loss: 9.1225 - acc: 0.419 - ETA: 1s - loss: 9.1439 - acc: 0.418 - ETA: 1s - loss: 9.1920 - acc: 0.415 - ETA: 0s - loss: 9.1363 - acc: 0.418 - ETA: 0s - loss: 9.1530 - acc: 0.418 - ETA: 0s - loss: 9.1629 - acc: 0.417 - ETA: 0s - loss: 9.1327 - acc: 0.418 - ETA: 0s - loss: 9.1431 - acc: 0.418 - ETA: 0s - loss: 9.1513 - acc: 0.417 - ETA: 0s - loss: 9.1453 - acc: 0.418 - ETA: 0s - loss: 9.1713 - acc: 0.416 - ETA: 0s - loss: 9.1980 - acc: 0.414 - ETA: 0s - loss: 9.1878 - acc: 0.415 - ETA: 0s - loss: 9.1922 - acc: 0.415 - ETA: 0s - loss: 9.2118 - acc: 0.413 - ETA: 0s - loss: 9.2061 - acc: 0.414 - ETA: 0s - loss: 9.1788 - acc: 0.415 - ETA: 0s - loss: 9.1865 - acc: 0.415 - ETA: 0s - loss: 9.1849 - acc: 0.415 - ETA: 0s - loss: 9.1861 - acc: 0.415 - ETA: 0s - loss: 9.1904 - acc: 0.414 - ETA: 0s - loss: 9.1712 - acc: 0.4158Epoch 00006: val_loss improved from 9.59453 to 9.57935, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 9.1775 - acc: 0.4154 - val_loss: 9.5793 - val_acc: 0.3629\n",
      "Epoch 8/100\n",
      "6620/6680 [============================>.] - ETA: 2s - loss: 10.5736 - acc: 0.30 - ETA: 2s - loss: 9.8289 - acc: 0.3750 - ETA: 2s - loss: 9.2694 - acc: 0.416 - ETA: 2s - loss: 9.2228 - acc: 0.420 - ETA: 2s - loss: 9.4222 - acc: 0.405 - ETA: 2s - loss: 9.4140 - acc: 0.405 - ETA: 2s - loss: 9.2142 - acc: 0.418 - ETA: 2s - loss: 9.3814 - acc: 0.407 - ETA: 2s - loss: 9.2800 - acc: 0.414 - ETA: 2s - loss: 9.3337 - acc: 0.410 - ETA: 2s - loss: 9.2157 - acc: 0.416 - ETA: 2s - loss: 9.2905 - acc: 0.412 - ETA: 2s - loss: 9.2185 - acc: 0.418 - ETA: 2s - loss: 9.2162 - acc: 0.417 - ETA: 2s - loss: 9.1720 - acc: 0.419 - ETA: 1s - loss: 9.1706 - acc: 0.419 - ETA: 1s - loss: 9.1493 - acc: 0.420 - ETA: 1s - loss: 9.1348 - acc: 0.421 - ETA: 1s - loss: 9.1222 - acc: 0.421 - ETA: 1s - loss: 9.0717 - acc: 0.424 - ETA: 1s - loss: 9.0978 - acc: 0.423 - ETA: 1s - loss: 9.0223 - acc: 0.427 - ETA: 1s - loss: 9.0732 - acc: 0.424 - ETA: 1s - loss: 9.0220 - acc: 0.428 - ETA: 1s - loss: 9.0979 - acc: 0.423 - ETA: 1s - loss: 9.1189 - acc: 0.422 - ETA: 1s - loss: 9.0913 - acc: 0.424 - ETA: 1s - loss: 9.1175 - acc: 0.423 - ETA: 1s - loss: 9.0375 - acc: 0.427 - ETA: 1s - loss: 9.0441 - acc: 0.427 - ETA: 1s - loss: 9.0764 - acc: 0.425 - ETA: 1s - loss: 9.0711 - acc: 0.425 - ETA: 0s - loss: 9.0869 - acc: 0.424 - ETA: 0s - loss: 9.0866 - acc: 0.423 - ETA: 0s - loss: 9.0963 - acc: 0.423 - ETA: 0s - loss: 9.1001 - acc: 0.423 - ETA: 0s - loss: 9.0751 - acc: 0.424 - ETA: 0s - loss: 9.0824 - acc: 0.424 - ETA: 0s - loss: 9.0645 - acc: 0.425 - ETA: 0s - loss: 9.0718 - acc: 0.425 - ETA: 0s - loss: 9.0845 - acc: 0.424 - ETA: 0s - loss: 9.0928 - acc: 0.423 - ETA: 0s - loss: 9.1068 - acc: 0.422 - ETA: 0s - loss: 9.1373 - acc: 0.420 - ETA: 0s - loss: 9.1142 - acc: 0.421 - ETA: 0s - loss: 9.1048 - acc: 0.421 - ETA: 0s - loss: 9.1224 - acc: 0.420 - ETA: 0s - loss: 9.1121 - acc: 0.420 - ETA: 0s - loss: 9.0913 - acc: 0.421 - ETA: 0s - loss: 9.1187 - acc: 0.420 - ETA: 0s - loss: 9.1119 - acc: 0.420 - ETA: 0s - loss: 9.1146 - acc: 0.420 - ETA: 0s - loss: 9.1124 - acc: 0.4205Epoch 00007: val_loss improved from 9.57935 to 9.46685, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 9.1103 - acc: 0.4205 - val_loss: 9.4668 - val_acc: 0.3485\n",
      "Epoch 9/100\n",
      "6560/6680 [============================>.] - ETA: 4s - loss: 10.6137 - acc: 0.30 - ETA: 3s - loss: 9.5196 - acc: 0.3833 - ETA: 3s - loss: 9.2984 - acc: 0.400 - ETA: 3s - loss: 9.3983 - acc: 0.396 - ETA: 3s - loss: 9.5837 - acc: 0.390 - ETA: 2s - loss: 9.4686 - acc: 0.400 - ETA: 2s - loss: 9.3409 - acc: 0.410 - ETA: 2s - loss: 9.3848 - acc: 0.407 - ETA: 2s - loss: 9.5415 - acc: 0.396 - ETA: 2s - loss: 9.5482 - acc: 0.395 - ETA: 2s - loss: 9.4079 - acc: 0.404 - ETA: 2s - loss: 9.4113 - acc: 0.405 - ETA: 2s - loss: 9.3645 - acc: 0.408 - ETA: 2s - loss: 9.2884 - acc: 0.413 - ETA: 2s - loss: 9.2597 - acc: 0.415 - ETA: 1s - loss: 9.1696 - acc: 0.419 - ETA: 1s - loss: 9.1115 - acc: 0.422 - ETA: 1s - loss: 9.1191 - acc: 0.420 - ETA: 1s - loss: 9.1614 - acc: 0.417 - ETA: 1s - loss: 9.1195 - acc: 0.421 - ETA: 1s - loss: 9.0835 - acc: 0.421 - ETA: 1s - loss: 9.1236 - acc: 0.417 - ETA: 1s - loss: 9.1250 - acc: 0.418 - ETA: 1s - loss: 9.0847 - acc: 0.420 - ETA: 1s - loss: 9.0910 - acc: 0.419 - ETA: 1s - loss: 9.0628 - acc: 0.421 - ETA: 1s - loss: 9.0602 - acc: 0.421 - ETA: 1s - loss: 9.0271 - acc: 0.422 - ETA: 1s - loss: 9.0396 - acc: 0.421 - ETA: 1s - loss: 9.0594 - acc: 0.420 - ETA: 1s - loss: 9.0166 - acc: 0.422 - ETA: 1s - loss: 9.0062 - acc: 0.423 - ETA: 1s - loss: 8.9982 - acc: 0.424 - ETA: 1s - loss: 8.9784 - acc: 0.425 - ETA: 1s - loss: 8.9687 - acc: 0.425 - ETA: 1s - loss: 8.9993 - acc: 0.423 - ETA: 1s - loss: 9.0127 - acc: 0.422 - ETA: 1s - loss: 9.0153 - acc: 0.422 - ETA: 0s - loss: 9.0070 - acc: 0.422 - ETA: 0s - loss: 8.9845 - acc: 0.423 - ETA: 0s - loss: 8.9755 - acc: 0.424 - ETA: 0s - loss: 8.9802 - acc: 0.423 - ETA: 0s - loss: 8.9607 - acc: 0.423 - ETA: 0s - loss: 9.0015 - acc: 0.421 - ETA: 0s - loss: 8.9962 - acc: 0.421 - ETA: 0s - loss: 8.9711 - acc: 0.422 - ETA: 0s - loss: 8.9521 - acc: 0.423 - ETA: 0s - loss: 8.9403 - acc: 0.424 - ETA: 0s - loss: 8.9320 - acc: 0.425 - ETA: 0s - loss: 8.9146 - acc: 0.426 - ETA: 0s - loss: 8.9015 - acc: 0.426 - ETA: 0s - loss: 8.9120 - acc: 0.426 - ETA: 0s - loss: 8.9088 - acc: 0.426 - ETA: 0s - loss: 8.8884 - acc: 0.427 - ETA: 0s - loss: 8.8827 - acc: 0.427 - ETA: 0s - loss: 8.8888 - acc: 0.427 - ETA: 0s - loss: 8.8851 - acc: 0.427 - ETA: 0s - loss: 8.8586 - acc: 0.4282Epoch 00008: val_loss improved from 9.46685 to 9.02939, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 8.8737 - acc: 0.4275 - val_loss: 9.0294 - val_acc: 0.3641\n",
      "Epoch 10/100\n",
      "6600/6680 [============================>.] - ETA: 2s - loss: 8.8651 - acc: 0.450 - ETA: 3s - loss: 8.4171 - acc: 0.471 - ETA: 3s - loss: 8.6227 - acc: 0.458 - ETA: 3s - loss: 8.7208 - acc: 0.447 - ETA: 2s - loss: 8.5757 - acc: 0.454 - ETA: 2s - loss: 8.4351 - acc: 0.456 - ETA: 2s - loss: 8.5284 - acc: 0.451 - ETA: 2s - loss: 8.4253 - acc: 0.454 - ETA: 2s - loss: 8.3976 - acc: 0.457 - ETA: 2s - loss: 8.3891 - acc: 0.459 - ETA: 2s - loss: 8.4382 - acc: 0.456 - ETA: 2s - loss: 8.4790 - acc: 0.454 - ETA: 2s - loss: 8.4683 - acc: 0.454 - ETA: 2s - loss: 8.4364 - acc: 0.456 - ETA: 2s - loss: 8.4200 - acc: 0.458 - ETA: 2s - loss: 8.4617 - acc: 0.455 - ETA: 2s - loss: 8.4307 - acc: 0.456 - ETA: 2s - loss: 8.3806 - acc: 0.459 - ETA: 2s - loss: 8.4458 - acc: 0.455 - ETA: 2s - loss: 8.4084 - acc: 0.456 - ETA: 2s - loss: 8.4495 - acc: 0.454 - ETA: 2s - loss: 8.4983 - acc: 0.452 - ETA: 2s - loss: 8.5026 - acc: 0.452 - ETA: 2s - loss: 8.5555 - acc: 0.449 - ETA: 1s - loss: 8.5253 - acc: 0.450 - ETA: 1s - loss: 8.5275 - acc: 0.449 - ETA: 1s - loss: 8.4959 - acc: 0.451 - ETA: 1s - loss: 8.4880 - acc: 0.451 - ETA: 1s - loss: 8.4970 - acc: 0.451 - ETA: 1s - loss: 8.4843 - acc: 0.451 - ETA: 1s - loss: 8.5070 - acc: 0.450 - ETA: 1s - loss: 8.4668 - acc: 0.452 - ETA: 1s - loss: 8.4683 - acc: 0.452 - ETA: 1s - loss: 8.4460 - acc: 0.453 - ETA: 1s - loss: 8.4628 - acc: 0.451 - ETA: 1s - loss: 8.5020 - acc: 0.449 - ETA: 1s - loss: 8.4747 - acc: 0.450 - ETA: 1s - loss: 8.4452 - acc: 0.452 - ETA: 1s - loss: 8.4783 - acc: 0.450 - ETA: 0s - loss: 8.4806 - acc: 0.450 - ETA: 0s - loss: 8.4869 - acc: 0.450 - ETA: 0s - loss: 8.4887 - acc: 0.450 - ETA: 0s - loss: 8.4678 - acc: 0.452 - ETA: 0s - loss: 8.4922 - acc: 0.449 - ETA: 0s - loss: 8.4875 - acc: 0.450 - ETA: 0s - loss: 8.4885 - acc: 0.450 - ETA: 0s - loss: 8.4770 - acc: 0.450 - ETA: 0s - loss: 8.4874 - acc: 0.449 - ETA: 0s - loss: 8.4797 - acc: 0.449 - ETA: 0s - loss: 8.4862 - acc: 0.448 - ETA: 0s - loss: 8.4945 - acc: 0.448 - ETA: 0s - loss: 8.4892 - acc: 0.448 - ETA: 0s - loss: 8.4868 - acc: 0.447 - ETA: 0s - loss: 8.5053 - acc: 0.446 - ETA: 0s - loss: 8.5120 - acc: 0.4461Epoch 00009: val_loss improved from 9.02939 to 8.73718, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 8.5077 - acc: 0.4463 - val_loss: 8.7372 - val_acc: 0.3737\n",
      "Epoch 11/100\n",
      "6620/6680 [============================>.] - ETA: 2s - loss: 8.3715 - acc: 0.450 - ETA: 2s - loss: 7.9757 - acc: 0.487 - ETA: 2s - loss: 7.6806 - acc: 0.490 - ETA: 2s - loss: 7.8663 - acc: 0.479 - ETA: 2s - loss: 7.7979 - acc: 0.479 - ETA: 2s - loss: 7.8347 - acc: 0.480 - ETA: 2s - loss: 7.7837 - acc: 0.486 - ETA: 2s - loss: 7.7981 - acc: 0.487 - ETA: 2s - loss: 7.9658 - acc: 0.478 - ETA: 2s - loss: 8.0543 - acc: 0.475 - ETA: 1s - loss: 8.1558 - acc: 0.471 - ETA: 1s - loss: 8.1615 - acc: 0.471 - ETA: 1s - loss: 8.2290 - acc: 0.468 - ETA: 1s - loss: 8.2656 - acc: 0.464 - ETA: 1s - loss: 8.2348 - acc: 0.466 - ETA: 1s - loss: 8.1951 - acc: 0.470 - ETA: 1s - loss: 8.2556 - acc: 0.467 - ETA: 1s - loss: 8.2387 - acc: 0.468 - ETA: 1s - loss: 8.2590 - acc: 0.466 - ETA: 1s - loss: 8.2669 - acc: 0.467 - ETA: 1s - loss: 8.2772 - acc: 0.467 - ETA: 1s - loss: 8.2597 - acc: 0.468 - ETA: 1s - loss: 8.2544 - acc: 0.468 - ETA: 1s - loss: 8.2333 - acc: 0.470 - ETA: 1s - loss: 8.2634 - acc: 0.468 - ETA: 1s - loss: 8.2419 - acc: 0.469 - ETA: 1s - loss: 8.2257 - acc: 0.470 - ETA: 1s - loss: 8.2321 - acc: 0.470 - ETA: 1s - loss: 8.2634 - acc: 0.468 - ETA: 1s - loss: 8.2605 - acc: 0.468 - ETA: 0s - loss: 8.2397 - acc: 0.468 - ETA: 0s - loss: 8.2507 - acc: 0.468 - ETA: 0s - loss: 8.2438 - acc: 0.469 - ETA: 0s - loss: 8.2427 - acc: 0.469 - ETA: 0s - loss: 8.2405 - acc: 0.469 - ETA: 0s - loss: 8.2750 - acc: 0.467 - ETA: 0s - loss: 8.2412 - acc: 0.469 - ETA: 0s - loss: 8.2244 - acc: 0.470 - ETA: 0s - loss: 8.2178 - acc: 0.470 - ETA: 0s - loss: 8.2091 - acc: 0.470 - ETA: 0s - loss: 8.2152 - acc: 0.469 - ETA: 0s - loss: 8.2198 - acc: 0.469 - ETA: 0s - loss: 8.2276 - acc: 0.468 - ETA: 0s - loss: 8.2240 - acc: 0.468 - ETA: 0s - loss: 8.2053 - acc: 0.469 - ETA: 0s - loss: 8.2110 - acc: 0.469 - ETA: 0s - loss: 8.2155 - acc: 0.469 - ETA: 0s - loss: 8.2237 - acc: 0.468 - ETA: 0s - loss: 8.2129 - acc: 0.4699Epoch 00010: val_loss improved from 8.73718 to 8.54229, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 8.2020 - acc: 0.4708 - val_loss: 8.5423 - val_acc: 0.4168\n",
      "Epoch 12/100\n",
      "6620/6680 [============================>.] - ETA: 2s - loss: 11.2920 - acc: 0.30 - ETA: 2s - loss: 8.2487 - acc: 0.4625 - ETA: 2s - loss: 7.7493 - acc: 0.503 - ETA: 2s - loss: 7.7413 - acc: 0.506 - ETA: 2s - loss: 7.9624 - acc: 0.494 - ETA: 2s - loss: 8.1509 - acc: 0.483 - ETA: 2s - loss: 8.1055 - acc: 0.484 - ETA: 2s - loss: 8.2343 - acc: 0.474 - ETA: 2s - loss: 8.1510 - acc: 0.480 - ETA: 2s - loss: 8.1466 - acc: 0.481 - ETA: 2s - loss: 8.0580 - acc: 0.487 - ETA: 2s - loss: 8.0739 - acc: 0.487 - ETA: 2s - loss: 8.1427 - acc: 0.483 - ETA: 1s - loss: 8.1565 - acc: 0.482 - ETA: 1s - loss: 8.1613 - acc: 0.482 - ETA: 1s - loss: 8.0922 - acc: 0.486 - ETA: 1s - loss: 8.1332 - acc: 0.484 - ETA: 1s - loss: 8.1801 - acc: 0.481 - ETA: 1s - loss: 8.1548 - acc: 0.483 - ETA: 1s - loss: 8.1778 - acc: 0.481 - ETA: 1s - loss: 8.2282 - acc: 0.478 - ETA: 1s - loss: 8.2204 - acc: 0.479 - ETA: 1s - loss: 8.2101 - acc: 0.479 - ETA: 1s - loss: 8.1921 - acc: 0.480 - ETA: 1s - loss: 8.1817 - acc: 0.481 - ETA: 1s - loss: 8.1682 - acc: 0.482 - ETA: 1s - loss: 8.1887 - acc: 0.480 - ETA: 1s - loss: 8.1631 - acc: 0.481 - ETA: 1s - loss: 8.2237 - acc: 0.478 - ETA: 1s - loss: 8.1899 - acc: 0.480 - ETA: 1s - loss: 8.1847 - acc: 0.480 - ETA: 1s - loss: 8.2089 - acc: 0.479 - ETA: 1s - loss: 8.2545 - acc: 0.476 - ETA: 0s - loss: 8.2433 - acc: 0.477 - ETA: 0s - loss: 8.2015 - acc: 0.480 - ETA: 0s - loss: 8.1740 - acc: 0.481 - ETA: 0s - loss: 8.1647 - acc: 0.482 - ETA: 0s - loss: 8.1846 - acc: 0.480 - ETA: 0s - loss: 8.1929 - acc: 0.480 - ETA: 0s - loss: 8.1636 - acc: 0.481 - ETA: 0s - loss: 8.1866 - acc: 0.480 - ETA: 0s - loss: 8.1532 - acc: 0.482 - ETA: 0s - loss: 8.1435 - acc: 0.482 - ETA: 0s - loss: 8.1324 - acc: 0.483 - ETA: 0s - loss: 8.1000 - acc: 0.485 - ETA: 0s - loss: 8.1225 - acc: 0.484 - ETA: 0s - loss: 8.1051 - acc: 0.484 - ETA: 0s - loss: 8.0783 - acc: 0.486 - ETA: 0s - loss: 8.0894 - acc: 0.485 - ETA: 0s - loss: 8.0999 - acc: 0.484 - ETA: 0s - loss: 8.0996 - acc: 0.484 - ETA: 0s - loss: 8.1114 - acc: 0.483 - ETA: 0s - loss: 8.1400 - acc: 0.4817Epoch 00011: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 8.1466 - acc: 0.4814 - val_loss: 8.6111 - val_acc: 0.4048\n",
      "Epoch 13/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6640/6680 [============================>.] - ETA: 6s - loss: 6.4498 - acc: 0.600 - ETA: 3s - loss: 7.9679 - acc: 0.500 - ETA: 2s - loss: 7.7616 - acc: 0.513 - ETA: 2s - loss: 7.7021 - acc: 0.516 - ETA: 2s - loss: 7.5441 - acc: 0.527 - ETA: 2s - loss: 7.8214 - acc: 0.508 - ETA: 2s - loss: 7.7452 - acc: 0.514 - ETA: 2s - loss: 8.0132 - acc: 0.494 - ETA: 2s - loss: 8.0780 - acc: 0.491 - ETA: 2s - loss: 8.1686 - acc: 0.486 - ETA: 2s - loss: 8.1441 - acc: 0.487 - ETA: 2s - loss: 8.2869 - acc: 0.478 - ETA: 2s - loss: 8.2826 - acc: 0.479 - ETA: 2s - loss: 8.2029 - acc: 0.484 - ETA: 2s - loss: 8.1525 - acc: 0.486 - ETA: 2s - loss: 8.1733 - acc: 0.485 - ETA: 2s - loss: 8.1265 - acc: 0.488 - ETA: 2s - loss: 8.1572 - acc: 0.486 - ETA: 1s - loss: 8.1687 - acc: 0.486 - ETA: 1s - loss: 8.1229 - acc: 0.489 - ETA: 1s - loss: 8.1213 - acc: 0.488 - ETA: 1s - loss: 8.1790 - acc: 0.485 - ETA: 1s - loss: 8.1396 - acc: 0.488 - ETA: 1s - loss: 8.1229 - acc: 0.489 - ETA: 1s - loss: 8.1266 - acc: 0.489 - ETA: 1s - loss: 8.1566 - acc: 0.487 - ETA: 1s - loss: 8.1490 - acc: 0.488 - ETA: 1s - loss: 8.1221 - acc: 0.489 - ETA: 1s - loss: 8.1874 - acc: 0.485 - ETA: 1s - loss: 8.2117 - acc: 0.483 - ETA: 1s - loss: 8.2077 - acc: 0.483 - ETA: 1s - loss: 8.2121 - acc: 0.483 - ETA: 1s - loss: 8.1976 - acc: 0.484 - ETA: 1s - loss: 8.1935 - acc: 0.484 - ETA: 1s - loss: 8.1709 - acc: 0.485 - ETA: 1s - loss: 8.1531 - acc: 0.486 - ETA: 1s - loss: 8.1374 - acc: 0.486 - ETA: 1s - loss: 8.1322 - acc: 0.487 - ETA: 1s - loss: 8.1673 - acc: 0.485 - ETA: 0s - loss: 8.1763 - acc: 0.484 - ETA: 0s - loss: 8.1469 - acc: 0.486 - ETA: 0s - loss: 8.1646 - acc: 0.484 - ETA: 0s - loss: 8.1524 - acc: 0.485 - ETA: 0s - loss: 8.1214 - acc: 0.488 - ETA: 0s - loss: 8.1143 - acc: 0.488 - ETA: 0s - loss: 8.1114 - acc: 0.488 - ETA: 0s - loss: 8.0870 - acc: 0.490 - ETA: 0s - loss: 8.0759 - acc: 0.490 - ETA: 0s - loss: 8.0822 - acc: 0.490 - ETA: 0s - loss: 8.0495 - acc: 0.492 - ETA: 0s - loss: 8.0506 - acc: 0.492 - ETA: 0s - loss: 8.0876 - acc: 0.490 - ETA: 0s - loss: 8.0947 - acc: 0.489 - ETA: 0s - loss: 8.1042 - acc: 0.489 - ETA: 0s - loss: 8.0968 - acc: 0.489 - ETA: 0s - loss: 8.1063 - acc: 0.4889Epoch 00012: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 8.1139 - acc: 0.4883 - val_loss: 8.5695 - val_acc: 0.4156\n",
      "Epoch 14/100\n",
      "6600/6680 [============================>.] - ETA: 3s - loss: 8.8843 - acc: 0.450 - ETA: 2s - loss: 7.8869 - acc: 0.506 - ETA: 2s - loss: 8.0910 - acc: 0.492 - ETA: 2s - loss: 7.9564 - acc: 0.502 - ETA: 2s - loss: 7.8505 - acc: 0.508 - ETA: 2s - loss: 7.7061 - acc: 0.518 - ETA: 2s - loss: 7.6699 - acc: 0.520 - ETA: 2s - loss: 7.5557 - acc: 0.527 - ETA: 2s - loss: 7.7426 - acc: 0.516 - ETA: 2s - loss: 7.6738 - acc: 0.521 - ETA: 2s - loss: 7.6240 - acc: 0.524 - ETA: 2s - loss: 7.5910 - acc: 0.526 - ETA: 2s - loss: 7.6063 - acc: 0.525 - ETA: 2s - loss: 7.6259 - acc: 0.524 - ETA: 2s - loss: 7.6140 - acc: 0.525 - ETA: 2s - loss: 7.6402 - acc: 0.523 - ETA: 2s - loss: 7.6504 - acc: 0.523 - ETA: 2s - loss: 7.6825 - acc: 0.520 - ETA: 2s - loss: 7.7383 - acc: 0.517 - ETA: 2s - loss: 7.7220 - acc: 0.518 - ETA: 2s - loss: 7.7315 - acc: 0.517 - ETA: 1s - loss: 7.7902 - acc: 0.513 - ETA: 1s - loss: 7.8402 - acc: 0.510 - ETA: 1s - loss: 7.9037 - acc: 0.507 - ETA: 1s - loss: 7.9637 - acc: 0.503 - ETA: 1s - loss: 7.9700 - acc: 0.502 - ETA: 1s - loss: 7.9908 - acc: 0.501 - ETA: 1s - loss: 7.9697 - acc: 0.502 - ETA: 1s - loss: 7.9276 - acc: 0.504 - ETA: 1s - loss: 7.9178 - acc: 0.504 - ETA: 1s - loss: 7.9541 - acc: 0.502 - ETA: 1s - loss: 7.9583 - acc: 0.502 - ETA: 1s - loss: 7.9746 - acc: 0.501 - ETA: 1s - loss: 7.9913 - acc: 0.500 - ETA: 1s - loss: 7.9823 - acc: 0.501 - ETA: 1s - loss: 8.0110 - acc: 0.499 - ETA: 0s - loss: 8.0029 - acc: 0.499 - ETA: 0s - loss: 8.0013 - acc: 0.500 - ETA: 0s - loss: 8.0036 - acc: 0.499 - ETA: 0s - loss: 8.0053 - acc: 0.499 - ETA: 0s - loss: 8.0518 - acc: 0.497 - ETA: 0s - loss: 8.0709 - acc: 0.495 - ETA: 0s - loss: 8.0474 - acc: 0.497 - ETA: 0s - loss: 8.0659 - acc: 0.496 - ETA: 0s - loss: 8.0943 - acc: 0.494 - ETA: 0s - loss: 8.0882 - acc: 0.494 - ETA: 0s - loss: 8.0879 - acc: 0.494 - ETA: 0s - loss: 8.0818 - acc: 0.494 - ETA: 0s - loss: 8.0780 - acc: 0.495 - ETA: 0s - loss: 8.0888 - acc: 0.494 - ETA: 0s - loss: 8.0886 - acc: 0.494 - ETA: 0s - loss: 8.0805 - acc: 0.494 - ETA: 0s - loss: 8.0885 - acc: 0.494 - ETA: 0s - loss: 8.0985 - acc: 0.493 - ETA: 0s - loss: 8.0954 - acc: 0.493 - ETA: 0s - loss: 8.1075 - acc: 0.493 - ETA: 0s - loss: 8.1025 - acc: 0.493 - ETA: 0s - loss: 8.1047 - acc: 0.4930Epoch 00013: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 8.1045 - acc: 0.4930 - val_loss: 8.5481 - val_acc: 0.4204\n",
      "Epoch 15/100\n",
      "6600/6680 [============================>.] - ETA: 3s - loss: 9.6711 - acc: 0.400 - ETA: 3s - loss: 7.6730 - acc: 0.516 - ETA: 3s - loss: 8.2145 - acc: 0.483 - ETA: 3s - loss: 8.3081 - acc: 0.477 - ETA: 3s - loss: 8.1962 - acc: 0.485 - ETA: 3s - loss: 7.8842 - acc: 0.506 - ETA: 3s - loss: 7.9923 - acc: 0.500 - ETA: 3s - loss: 8.0499 - acc: 0.497 - ETA: 3s - loss: 7.9303 - acc: 0.502 - ETA: 3s - loss: 7.9441 - acc: 0.501 - ETA: 3s - loss: 7.9196 - acc: 0.503 - ETA: 3s - loss: 7.8713 - acc: 0.506 - ETA: 3s - loss: 7.8248 - acc: 0.508 - ETA: 3s - loss: 7.8727 - acc: 0.505 - ETA: 2s - loss: 7.8790 - acc: 0.505 - ETA: 2s - loss: 7.7853 - acc: 0.510 - ETA: 2s - loss: 7.8326 - acc: 0.508 - ETA: 2s - loss: 7.8207 - acc: 0.509 - ETA: 2s - loss: 7.8627 - acc: 0.506 - ETA: 2s - loss: 7.8620 - acc: 0.506 - ETA: 2s - loss: 7.8538 - acc: 0.507 - ETA: 2s - loss: 7.8559 - acc: 0.507 - ETA: 2s - loss: 7.8875 - acc: 0.505 - ETA: 2s - loss: 7.9034 - acc: 0.503 - ETA: 2s - loss: 7.8966 - acc: 0.504 - ETA: 2s - loss: 7.8659 - acc: 0.506 - ETA: 2s - loss: 7.8874 - acc: 0.505 - ETA: 2s - loss: 7.8131 - acc: 0.509 - ETA: 2s - loss: 7.7711 - acc: 0.512 - ETA: 1s - loss: 7.8232 - acc: 0.509 - ETA: 1s - loss: 7.8310 - acc: 0.508 - ETA: 1s - loss: 7.8482 - acc: 0.508 - ETA: 1s - loss: 7.8902 - acc: 0.505 - ETA: 1s - loss: 7.9112 - acc: 0.503 - ETA: 1s - loss: 7.8890 - acc: 0.505 - ETA: 1s - loss: 7.8897 - acc: 0.505 - ETA: 1s - loss: 7.8861 - acc: 0.505 - ETA: 1s - loss: 7.8974 - acc: 0.504 - ETA: 1s - loss: 7.9014 - acc: 0.503 - ETA: 1s - loss: 7.9318 - acc: 0.501 - ETA: 1s - loss: 7.9318 - acc: 0.501 - ETA: 1s - loss: 7.9541 - acc: 0.500 - ETA: 1s - loss: 7.9657 - acc: 0.499 - ETA: 1s - loss: 7.9604 - acc: 0.499 - ETA: 1s - loss: 7.9672 - acc: 0.499 - ETA: 1s - loss: 7.9834 - acc: 0.498 - ETA: 1s - loss: 7.9790 - acc: 0.498 - ETA: 0s - loss: 8.0077 - acc: 0.496 - ETA: 0s - loss: 8.0484 - acc: 0.494 - ETA: 0s - loss: 8.0531 - acc: 0.493 - ETA: 0s - loss: 8.0787 - acc: 0.492 - ETA: 0s - loss: 8.0755 - acc: 0.492 - ETA: 0s - loss: 8.0878 - acc: 0.491 - ETA: 0s - loss: 8.1024 - acc: 0.490 - ETA: 0s - loss: 8.1082 - acc: 0.490 - ETA: 0s - loss: 8.1245 - acc: 0.489 - ETA: 0s - loss: 8.1425 - acc: 0.488 - ETA: 0s - loss: 8.1297 - acc: 0.489 - ETA: 0s - loss: 8.1363 - acc: 0.489 - ETA: 0s - loss: 8.1480 - acc: 0.488 - ETA: 0s - loss: 8.1389 - acc: 0.488 - ETA: 0s - loss: 8.1200 - acc: 0.489 - ETA: 0s - loss: 8.1142 - acc: 0.490 - ETA: 0s - loss: 8.1113 - acc: 0.490 - ETA: 0s - loss: 8.1033 - acc: 0.4911Epoch 00014: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 8.0956 - acc: 0.4916 - val_loss: 8.6321 - val_acc: 0.4096\n",
      "Epoch 16/100\n",
      "6620/6680 [============================>.] - ETA: 2s - loss: 8.8650 - acc: 0.450 - ETA: 2s - loss: 7.8330 - acc: 0.514 - ETA: 3s - loss: 7.9443 - acc: 0.504 - ETA: 3s - loss: 7.8042 - acc: 0.513 - ETA: 3s - loss: 8.0348 - acc: 0.500 - ETA: 3s - loss: 8.0127 - acc: 0.500 - ETA: 3s - loss: 8.0185 - acc: 0.500 - ETA: 3s - loss: 8.0274 - acc: 0.498 - ETA: 3s - loss: 8.0709 - acc: 0.496 - ETA: 3s - loss: 8.1615 - acc: 0.490 - ETA: 3s - loss: 8.4048 - acc: 0.476 - ETA: 3s - loss: 8.4405 - acc: 0.474 - ETA: 3s - loss: 8.4353 - acc: 0.474 - ETA: 3s - loss: 8.5221 - acc: 0.469 - ETA: 3s - loss: 8.4109 - acc: 0.475 - ETA: 3s - loss: 8.5304 - acc: 0.468 - ETA: 3s - loss: 8.5374 - acc: 0.468 - ETA: 3s - loss: 8.5039 - acc: 0.470 - ETA: 3s - loss: 8.4741 - acc: 0.471 - ETA: 2s - loss: 8.3974 - acc: 0.476 - ETA: 2s - loss: 8.3671 - acc: 0.478 - ETA: 2s - loss: 8.3717 - acc: 0.477 - ETA: 2s - loss: 8.3182 - acc: 0.481 - ETA: 2s - loss: 8.3355 - acc: 0.479 - ETA: 2s - loss: 8.3352 - acc: 0.479 - ETA: 2s - loss: 8.3242 - acc: 0.480 - ETA: 2s - loss: 8.3074 - acc: 0.481 - ETA: 2s - loss: 8.3255 - acc: 0.480 - ETA: 2s - loss: 8.3574 - acc: 0.478 - ETA: 2s - loss: 8.3354 - acc: 0.479 - ETA: 2s - loss: 8.3080 - acc: 0.481 - ETA: 2s - loss: 8.2722 - acc: 0.483 - ETA: 2s - loss: 8.2450 - acc: 0.485 - ETA: 1s - loss: 8.2185 - acc: 0.486 - ETA: 1s - loss: 8.1757 - acc: 0.489 - ETA: 1s - loss: 8.1299 - acc: 0.492 - ETA: 1s - loss: 8.1280 - acc: 0.492 - ETA: 1s - loss: 8.1216 - acc: 0.492 - ETA: 1s - loss: 8.1360 - acc: 0.492 - ETA: 1s - loss: 8.1018 - acc: 0.494 - ETA: 1s - loss: 8.0990 - acc: 0.494 - ETA: 1s - loss: 8.0980 - acc: 0.494 - ETA: 1s - loss: 8.0827 - acc: 0.495 - ETA: 1s - loss: 8.1213 - acc: 0.493 - ETA: 1s - loss: 8.1577 - acc: 0.490 - ETA: 0s - loss: 8.1596 - acc: 0.490 - ETA: 0s - loss: 8.1374 - acc: 0.491 - ETA: 0s - loss: 8.1424 - acc: 0.491 - ETA: 0s - loss: 8.1496 - acc: 0.491 - ETA: 0s - loss: 8.1338 - acc: 0.492 - ETA: 0s - loss: 8.1507 - acc: 0.491 - ETA: 0s - loss: 8.1458 - acc: 0.491 - ETA: 0s - loss: 8.1243 - acc: 0.492 - ETA: 0s - loss: 8.1389 - acc: 0.491 - ETA: 0s - loss: 8.1575 - acc: 0.490 - ETA: 0s - loss: 8.1365 - acc: 0.491 - ETA: 0s - loss: 8.1246 - acc: 0.492 - ETA: 0s - loss: 8.1290 - acc: 0.492 - ETA: 0s - loss: 8.1227 - acc: 0.492 - ETA: 0s - loss: 8.1141 - acc: 0.493 - ETA: 0s - loss: 8.1131 - acc: 0.493 - ETA: 0s - loss: 8.1074 - acc: 0.4938Epoch 00015: val_loss improved from 8.54229 to 8.50864, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 8.0877 - acc: 0.4951 - val_loss: 8.5086 - val_acc: 0.4251\n",
      "Epoch 17/100\n",
      "6620/6680 [============================>.] - ETA: 4s - loss: 8.8703 - acc: 0.450 - ETA: 2s - loss: 8.2617 - acc: 0.487 - ETA: 2s - loss: 8.1137 - acc: 0.496 - ETA: 2s - loss: 8.5364 - acc: 0.469 - ETA: 2s - loss: 8.5329 - acc: 0.470 - ETA: 2s - loss: 8.4439 - acc: 0.475 - ETA: 2s - loss: 8.5552 - acc: 0.468 - ETA: 2s - loss: 8.5354 - acc: 0.470 - ETA: 2s - loss: 8.4236 - acc: 0.477 - ETA: 1s - loss: 8.4027 - acc: 0.477 - ETA: 1s - loss: 8.4144 - acc: 0.476 - ETA: 1s - loss: 8.4346 - acc: 0.474 - ETA: 1s - loss: 8.3574 - acc: 0.478 - ETA: 1s - loss: 8.1688 - acc: 0.490 - ETA: 1s - loss: 8.0993 - acc: 0.494 - ETA: 1s - loss: 8.0919 - acc: 0.494 - ETA: 1s - loss: 8.0545 - acc: 0.496 - ETA: 1s - loss: 8.1010 - acc: 0.493 - ETA: 1s - loss: 8.0799 - acc: 0.492 - ETA: 1s - loss: 8.0675 - acc: 0.493 - ETA: 1s - loss: 8.0893 - acc: 0.492 - ETA: 1s - loss: 8.0707 - acc: 0.493 - ETA: 1s - loss: 8.0474 - acc: 0.495 - ETA: 1s - loss: 8.0407 - acc: 0.495 - ETA: 1s - loss: 8.0511 - acc: 0.494 - ETA: 1s - loss: 8.0756 - acc: 0.492 - ETA: 1s - loss: 8.0821 - acc: 0.492 - ETA: 1s - loss: 8.0080 - acc: 0.497 - ETA: 1s - loss: 7.9974 - acc: 0.497 - ETA: 1s - loss: 8.0074 - acc: 0.497 - ETA: 0s - loss: 7.9791 - acc: 0.499 - ETA: 0s - loss: 7.9822 - acc: 0.498 - ETA: 0s - loss: 7.9771 - acc: 0.498 - ETA: 0s - loss: 7.9762 - acc: 0.498 - ETA: 0s - loss: 7.9382 - acc: 0.500 - ETA: 0s - loss: 7.9272 - acc: 0.501 - ETA: 0s - loss: 7.9506 - acc: 0.500 - ETA: 0s - loss: 7.9919 - acc: 0.497 - ETA: 0s - loss: 8.0041 - acc: 0.496 - ETA: 0s - loss: 8.0006 - acc: 0.496 - ETA: 0s - loss: 7.9985 - acc: 0.497 - ETA: 0s - loss: 8.0051 - acc: 0.496 - ETA: 0s - loss: 8.0190 - acc: 0.495 - ETA: 0s - loss: 8.0380 - acc: 0.493 - ETA: 0s - loss: 8.0329 - acc: 0.494 - ETA: 0s - loss: 8.0273 - acc: 0.494 - ETA: 0s - loss: 8.0360 - acc: 0.494 - ETA: 0s - loss: 8.0359 - acc: 0.494 - ETA: 0s - loss: 8.0211 - acc: 0.495 - ETA: 0s - loss: 8.0294 - acc: 0.494 - ETA: 0s - loss: 8.0160 - acc: 0.495 - ETA: 0s - loss: 8.0236 - acc: 0.4950Epoch 00016: val_loss improved from 8.50864 to 8.47297, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 8.0217 - acc: 0.4952 - val_loss: 8.4730 - val_acc: 0.4108\n",
      "Epoch 18/100\n",
      "6580/6680 [============================>.] - ETA: 2s - loss: 5.6413 - acc: 0.650 - ETA: 2s - loss: 7.2606 - acc: 0.550 - ETA: 2s - loss: 7.6561 - acc: 0.516 - ETA: 2s - loss: 7.9263 - acc: 0.502 - ETA: 2s - loss: 7.9172 - acc: 0.498 - ETA: 2s - loss: 7.8537 - acc: 0.504 - ETA: 2s - loss: 8.0958 - acc: 0.489 - ETA: 2s - loss: 8.1457 - acc: 0.483 - ETA: 2s - loss: 8.0321 - acc: 0.489 - ETA: 2s - loss: 8.1150 - acc: 0.482 - ETA: 2s - loss: 8.0702 - acc: 0.484 - ETA: 2s - loss: 7.9269 - acc: 0.492 - ETA: 2s - loss: 7.9112 - acc: 0.493 - ETA: 2s - loss: 7.9535 - acc: 0.491 - ETA: 1s - loss: 7.9175 - acc: 0.493 - ETA: 1s - loss: 7.9546 - acc: 0.491 - ETA: 1s - loss: 7.9394 - acc: 0.492 - ETA: 1s - loss: 7.9550 - acc: 0.492 - ETA: 1s - loss: 7.9589 - acc: 0.492 - ETA: 1s - loss: 7.9558 - acc: 0.492 - ETA: 1s - loss: 7.9554 - acc: 0.492 - ETA: 1s - loss: 7.8985 - acc: 0.496 - ETA: 1s - loss: 7.8251 - acc: 0.501 - ETA: 1s - loss: 7.8786 - acc: 0.497 - ETA: 1s - loss: 7.8598 - acc: 0.499 - ETA: 1s - loss: 7.8128 - acc: 0.501 - ETA: 1s - loss: 7.8638 - acc: 0.498 - ETA: 1s - loss: 7.8472 - acc: 0.500 - ETA: 1s - loss: 7.8488 - acc: 0.500 - ETA: 1s - loss: 7.8383 - acc: 0.500 - ETA: 1s - loss: 7.8188 - acc: 0.501 - ETA: 1s - loss: 7.8128 - acc: 0.501 - ETA: 1s - loss: 7.8297 - acc: 0.501 - ETA: 1s - loss: 7.8318 - acc: 0.500 - ETA: 0s - loss: 7.8175 - acc: 0.501 - ETA: 0s - loss: 7.8010 - acc: 0.502 - ETA: 0s - loss: 7.8420 - acc: 0.499 - ETA: 0s - loss: 7.8382 - acc: 0.500 - ETA: 0s - loss: 7.8382 - acc: 0.500 - ETA: 0s - loss: 7.8402 - acc: 0.500 - ETA: 0s - loss: 7.8501 - acc: 0.500 - ETA: 0s - loss: 7.8467 - acc: 0.500 - ETA: 0s - loss: 7.8330 - acc: 0.500 - ETA: 0s - loss: 7.8326 - acc: 0.500 - ETA: 0s - loss: 7.8448 - acc: 0.499 - ETA: 0s - loss: 7.8268 - acc: 0.500 - ETA: 0s - loss: 7.8246 - acc: 0.500 - ETA: 0s - loss: 7.8048 - acc: 0.501 - ETA: 0s - loss: 7.8427 - acc: 0.499 - ETA: 0s - loss: 7.8468 - acc: 0.499 - ETA: 0s - loss: 7.8471 - acc: 0.499 - ETA: 0s - loss: 7.8478 - acc: 0.4989Epoch 00017: val_loss improved from 8.47297 to 8.33682, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 7.8632 - acc: 0.4981 - val_loss: 8.3368 - val_acc: 0.4144\n",
      "Epoch 19/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6620/6680 [============================>.] - ETA: 3s - loss: 6.4472 - acc: 0.600 - ETA: 4s - loss: 6.2614 - acc: 0.600 - ETA: 3s - loss: 7.5676 - acc: 0.525 - ETA: 3s - loss: 8.0325 - acc: 0.494 - ETA: 2s - loss: 7.9363 - acc: 0.500 - ETA: 2s - loss: 7.8569 - acc: 0.503 - ETA: 2s - loss: 7.8416 - acc: 0.504 - ETA: 2s - loss: 7.7032 - acc: 0.511 - ETA: 2s - loss: 7.6513 - acc: 0.515 - ETA: 2s - loss: 7.6264 - acc: 0.514 - ETA: 2s - loss: 7.5537 - acc: 0.518 - ETA: 2s - loss: 7.5764 - acc: 0.517 - ETA: 2s - loss: 7.5438 - acc: 0.517 - ETA: 2s - loss: 7.5098 - acc: 0.519 - ETA: 2s - loss: 7.5539 - acc: 0.516 - ETA: 2s - loss: 7.6135 - acc: 0.511 - ETA: 1s - loss: 7.6527 - acc: 0.509 - ETA: 1s - loss: 7.7592 - acc: 0.504 - ETA: 1s - loss: 7.7301 - acc: 0.506 - ETA: 1s - loss: 7.7266 - acc: 0.506 - ETA: 1s - loss: 7.6808 - acc: 0.509 - ETA: 1s - loss: 7.6654 - acc: 0.510 - ETA: 1s - loss: 7.7078 - acc: 0.508 - ETA: 1s - loss: 7.7043 - acc: 0.508 - ETA: 1s - loss: 7.7217 - acc: 0.507 - ETA: 1s - loss: 7.7360 - acc: 0.506 - ETA: 1s - loss: 7.6997 - acc: 0.509 - ETA: 1s - loss: 7.7027 - acc: 0.508 - ETA: 1s - loss: 7.7121 - acc: 0.508 - ETA: 1s - loss: 7.7514 - acc: 0.505 - ETA: 1s - loss: 7.7304 - acc: 0.507 - ETA: 1s - loss: 7.7294 - acc: 0.507 - ETA: 0s - loss: 7.7538 - acc: 0.506 - ETA: 0s - loss: 7.7310 - acc: 0.507 - ETA: 0s - loss: 7.7270 - acc: 0.508 - ETA: 0s - loss: 7.7308 - acc: 0.507 - ETA: 0s - loss: 7.6938 - acc: 0.510 - ETA: 0s - loss: 7.7118 - acc: 0.509 - ETA: 0s - loss: 7.7106 - acc: 0.509 - ETA: 0s - loss: 7.6661 - acc: 0.511 - ETA: 0s - loss: 7.6735 - acc: 0.511 - ETA: 0s - loss: 7.7194 - acc: 0.509 - ETA: 0s - loss: 7.6997 - acc: 0.510 - ETA: 0s - loss: 7.6830 - acc: 0.511 - ETA: 0s - loss: 7.6594 - acc: 0.513 - ETA: 0s - loss: 7.6766 - acc: 0.512 - ETA: 0s - loss: 7.6896 - acc: 0.511 - ETA: 0s - loss: 7.6917 - acc: 0.511 - ETA: 0s - loss: 7.7021 - acc: 0.511 - ETA: 0s - loss: 7.6845 - acc: 0.5122Epoch 00018: val_loss improved from 8.33682 to 8.24643, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 7.6880 - acc: 0.5120 - val_loss: 8.2464 - val_acc: 0.4311\n",
      "Epoch 20/100\n",
      "6620/6680 [============================>.] - ETA: 2s - loss: 4.0324 - acc: 0.750 - ETA: 2s - loss: 7.6739 - acc: 0.518 - ETA: 2s - loss: 7.5726 - acc: 0.526 - ETA: 2s - loss: 8.1308 - acc: 0.493 - ETA: 2s - loss: 8.0728 - acc: 0.496 - ETA: 2s - loss: 8.0067 - acc: 0.500 - ETA: 2s - loss: 8.0721 - acc: 0.496 - ETA: 2s - loss: 8.0712 - acc: 0.496 - ETA: 2s - loss: 7.9711 - acc: 0.502 - ETA: 2s - loss: 7.9423 - acc: 0.504 - ETA: 2s - loss: 7.9639 - acc: 0.501 - ETA: 1s - loss: 7.9032 - acc: 0.505 - ETA: 1s - loss: 7.7762 - acc: 0.512 - ETA: 1s - loss: 7.8103 - acc: 0.507 - ETA: 1s - loss: 7.8120 - acc: 0.506 - ETA: 1s - loss: 7.8060 - acc: 0.507 - ETA: 1s - loss: 7.7639 - acc: 0.509 - ETA: 1s - loss: 7.7697 - acc: 0.509 - ETA: 1s - loss: 7.7549 - acc: 0.510 - ETA: 1s - loss: 7.7068 - acc: 0.513 - ETA: 1s - loss: 7.7132 - acc: 0.513 - ETA: 1s - loss: 7.7309 - acc: 0.512 - ETA: 1s - loss: 7.7474 - acc: 0.511 - ETA: 1s - loss: 7.7330 - acc: 0.512 - ETA: 1s - loss: 7.7300 - acc: 0.512 - ETA: 1s - loss: 7.6854 - acc: 0.515 - ETA: 1s - loss: 7.6600 - acc: 0.516 - ETA: 1s - loss: 7.6954 - acc: 0.514 - ETA: 1s - loss: 7.6752 - acc: 0.515 - ETA: 1s - loss: 7.6725 - acc: 0.516 - ETA: 1s - loss: 7.6442 - acc: 0.517 - ETA: 0s - loss: 7.6207 - acc: 0.519 - ETA: 0s - loss: 7.6391 - acc: 0.518 - ETA: 0s - loss: 7.6559 - acc: 0.517 - ETA: 0s - loss: 7.6718 - acc: 0.516 - ETA: 0s - loss: 7.6928 - acc: 0.515 - ETA: 0s - loss: 7.6973 - acc: 0.514 - ETA: 0s - loss: 7.7056 - acc: 0.514 - ETA: 0s - loss: 7.6945 - acc: 0.514 - ETA: 0s - loss: 7.6894 - acc: 0.515 - ETA: 0s - loss: 7.6634 - acc: 0.517 - ETA: 0s - loss: 7.6753 - acc: 0.516 - ETA: 0s - loss: 7.6656 - acc: 0.516 - ETA: 0s - loss: 7.6436 - acc: 0.517 - ETA: 0s - loss: 7.6287 - acc: 0.518 - ETA: 0s - loss: 7.6079 - acc: 0.519 - ETA: 0s - loss: 7.6343 - acc: 0.518 - ETA: 0s - loss: 7.6478 - acc: 0.517 - ETA: 0s - loss: 7.6432 - acc: 0.517 - ETA: 0s - loss: 7.6468 - acc: 0.517 - ETA: 0s - loss: 7.6470 - acc: 0.5171Epoch 00019: val_loss improved from 8.24643 to 8.16556, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 7.6482 - acc: 0.5171 - val_loss: 8.1656 - val_acc: 0.4407\n",
      "Epoch 21/100\n",
      "6660/6680 [============================>.] - ETA: 3s - loss: 9.6710 - acc: 0.400 - ETA: 2s - loss: 7.8776 - acc: 0.506 - ETA: 2s - loss: 8.2212 - acc: 0.486 - ETA: 2s - loss: 7.9543 - acc: 0.500 - ETA: 2s - loss: 7.8596 - acc: 0.505 - ETA: 2s - loss: 7.8686 - acc: 0.504 - ETA: 2s - loss: 7.6128 - acc: 0.517 - ETA: 2s - loss: 7.5437 - acc: 0.521 - ETA: 2s - loss: 7.6459 - acc: 0.515 - ETA: 2s - loss: 7.6784 - acc: 0.514 - ETA: 2s - loss: 7.5009 - acc: 0.525 - ETA: 2s - loss: 7.3765 - acc: 0.533 - ETA: 2s - loss: 7.4101 - acc: 0.531 - ETA: 1s - loss: 7.4070 - acc: 0.532 - ETA: 1s - loss: 7.4263 - acc: 0.530 - ETA: 1s - loss: 7.3603 - acc: 0.534 - ETA: 1s - loss: 7.4861 - acc: 0.527 - ETA: 1s - loss: 7.4661 - acc: 0.528 - ETA: 1s - loss: 7.4944 - acc: 0.526 - ETA: 1s - loss: 7.4998 - acc: 0.525 - ETA: 1s - loss: 7.5147 - acc: 0.524 - ETA: 1s - loss: 7.5178 - acc: 0.523 - ETA: 1s - loss: 7.4912 - acc: 0.525 - ETA: 1s - loss: 7.4532 - acc: 0.526 - ETA: 1s - loss: 7.4396 - acc: 0.527 - ETA: 1s - loss: 7.4463 - acc: 0.526 - ETA: 1s - loss: 7.4468 - acc: 0.526 - ETA: 1s - loss: 7.4492 - acc: 0.526 - ETA: 1s - loss: 7.4883 - acc: 0.524 - ETA: 1s - loss: 7.4702 - acc: 0.524 - ETA: 1s - loss: 7.4993 - acc: 0.523 - ETA: 0s - loss: 7.5283 - acc: 0.521 - ETA: 0s - loss: 7.5310 - acc: 0.521 - ETA: 0s - loss: 7.5126 - acc: 0.522 - ETA: 0s - loss: 7.4922 - acc: 0.523 - ETA: 0s - loss: 7.4819 - acc: 0.524 - ETA: 0s - loss: 7.4778 - acc: 0.524 - ETA: 0s - loss: 7.4767 - acc: 0.524 - ETA: 0s - loss: 7.4683 - acc: 0.525 - ETA: 0s - loss: 7.4821 - acc: 0.524 - ETA: 0s - loss: 7.4569 - acc: 0.526 - ETA: 0s - loss: 7.4844 - acc: 0.524 - ETA: 0s - loss: 7.4990 - acc: 0.523 - ETA: 0s - loss: 7.4793 - acc: 0.525 - ETA: 0s - loss: 7.4880 - acc: 0.524 - ETA: 0s - loss: 7.5016 - acc: 0.523 - ETA: 0s - loss: 7.5086 - acc: 0.523 - ETA: 0s - loss: 7.5082 - acc: 0.523 - ETA: 0s - loss: 7.4849 - acc: 0.524 - ETA: 0s - loss: 7.5047 - acc: 0.523 - ETA: 0s - loss: 7.5041 - acc: 0.523 - ETA: 0s - loss: 7.5108 - acc: 0.523 - ETA: 0s - loss: 7.5099 - acc: 0.5237Epoch 00020: val_loss improved from 8.16556 to 8.12377, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 7.5163 - acc: 0.5234 - val_loss: 8.1238 - val_acc: 0.4407\n",
      "Epoch 22/100\n",
      "6660/6680 [============================>.] - ETA: 3s - loss: 8.0594 - acc: 0.500 - ETA: 3s - loss: 7.2920 - acc: 0.542 - ETA: 3s - loss: 7.2781 - acc: 0.545 - ETA: 3s - loss: 7.2532 - acc: 0.535 - ETA: 3s - loss: 7.3637 - acc: 0.531 - ETA: 3s - loss: 7.4888 - acc: 0.525 - ETA: 3s - loss: 7.3116 - acc: 0.537 - ETA: 3s - loss: 6.9371 - acc: 0.562 - ETA: 3s - loss: 6.8738 - acc: 0.567 - ETA: 3s - loss: 6.9102 - acc: 0.565 - ETA: 3s - loss: 6.9296 - acc: 0.564 - ETA: 3s - loss: 6.8761 - acc: 0.568 - ETA: 3s - loss: 6.8242 - acc: 0.571 - ETA: 3s - loss: 6.7878 - acc: 0.574 - ETA: 3s - loss: 6.8469 - acc: 0.570 - ETA: 2s - loss: 6.8841 - acc: 0.567 - ETA: 2s - loss: 6.9020 - acc: 0.567 - ETA: 2s - loss: 6.9766 - acc: 0.562 - ETA: 2s - loss: 6.9945 - acc: 0.561 - ETA: 2s - loss: 7.1221 - acc: 0.553 - ETA: 2s - loss: 7.2009 - acc: 0.548 - ETA: 2s - loss: 7.2187 - acc: 0.547 - ETA: 2s - loss: 7.2208 - acc: 0.546 - ETA: 2s - loss: 7.1995 - acc: 0.547 - ETA: 2s - loss: 7.2240 - acc: 0.545 - ETA: 2s - loss: 7.2624 - acc: 0.543 - ETA: 1s - loss: 7.2794 - acc: 0.542 - ETA: 1s - loss: 7.3055 - acc: 0.541 - ETA: 1s - loss: 7.2900 - acc: 0.541 - ETA: 1s - loss: 7.3049 - acc: 0.541 - ETA: 1s - loss: 7.3070 - acc: 0.540 - ETA: 1s - loss: 7.2957 - acc: 0.541 - ETA: 1s - loss: 7.3471 - acc: 0.538 - ETA: 1s - loss: 7.3491 - acc: 0.538 - ETA: 1s - loss: 7.3869 - acc: 0.536 - ETA: 1s - loss: 7.3927 - acc: 0.536 - ETA: 1s - loss: 7.3978 - acc: 0.535 - ETA: 1s - loss: 7.4041 - acc: 0.535 - ETA: 1s - loss: 7.4159 - acc: 0.534 - ETA: 1s - loss: 7.4443 - acc: 0.532 - ETA: 1s - loss: 7.4840 - acc: 0.530 - ETA: 1s - loss: 7.4824 - acc: 0.530 - ETA: 1s - loss: 7.4854 - acc: 0.530 - ETA: 1s - loss: 7.4895 - acc: 0.529 - ETA: 1s - loss: 7.5010 - acc: 0.528 - ETA: 1s - loss: 7.5005 - acc: 0.528 - ETA: 1s - loss: 7.4971 - acc: 0.528 - ETA: 1s - loss: 7.4791 - acc: 0.529 - ETA: 1s - loss: 7.4715 - acc: 0.530 - ETA: 1s - loss: 7.4633 - acc: 0.530 - ETA: 0s - loss: 7.4754 - acc: 0.529 - ETA: 0s - loss: 7.4879 - acc: 0.528 - ETA: 0s - loss: 7.4681 - acc: 0.529 - ETA: 0s - loss: 7.4704 - acc: 0.529 - ETA: 0s - loss: 7.4720 - acc: 0.529 - ETA: 0s - loss: 7.4633 - acc: 0.530 - ETA: 0s - loss: 7.4751 - acc: 0.529 - ETA: 0s - loss: 7.4785 - acc: 0.529 - ETA: 0s - loss: 7.4818 - acc: 0.529 - ETA: 0s - loss: 7.4795 - acc: 0.529 - ETA: 0s - loss: 7.4488 - acc: 0.531 - ETA: 0s - loss: 7.4577 - acc: 0.530 - ETA: 0s - loss: 7.4725 - acc: 0.529 - ETA: 0s - loss: 7.4919 - acc: 0.528 - ETA: 0s - loss: 7.4949 - acc: 0.528 - ETA: 0s - loss: 7.4787 - acc: 0.529 - ETA: 0s - loss: 7.4661 - acc: 0.530 - ETA: 0s - loss: 7.4685 - acc: 0.530 - ETA: 0s - loss: 7.4734 - acc: 0.530 - ETA: 0s - loss: 7.4664 - acc: 0.530 - ETA: 0s - loss: 7.4567 - acc: 0.531 - ETA: 0s - loss: 7.4470 - acc: 0.532 - ETA: 0s - loss: 7.4549 - acc: 0.5315Epoch 00021: val_loss improved from 8.12377 to 7.98995, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 7.4543 - acc: 0.5316 - val_loss: 7.9900 - val_acc: 0.4563\n",
      "Epoch 23/100\n",
      "6600/6680 [============================>.] - ETA: 3s - loss: 9.6709 - acc: 0.400 - ETA: 5s - loss: 8.4785 - acc: 0.462 - ETA: 4s - loss: 8.1681 - acc: 0.487 - ETA: 4s - loss: 7.8025 - acc: 0.508 - ETA: 4s - loss: 7.6150 - acc: 0.521 - ETA: 4s - loss: 7.4909 - acc: 0.531 - ETA: 4s - loss: 7.6339 - acc: 0.520 - ETA: 3s - loss: 7.7674 - acc: 0.511 - ETA: 3s - loss: 7.6850 - acc: 0.517 - ETA: 3s - loss: 7.7570 - acc: 0.513 - ETA: 3s - loss: 7.6713 - acc: 0.519 - ETA: 3s - loss: 7.6435 - acc: 0.521 - ETA: 3s - loss: 7.7174 - acc: 0.517 - ETA: 3s - loss: 7.7198 - acc: 0.517 - ETA: 3s - loss: 7.7565 - acc: 0.515 - ETA: 3s - loss: 7.6828 - acc: 0.519 - ETA: 3s - loss: 7.5704 - acc: 0.526 - ETA: 3s - loss: 7.5324 - acc: 0.528 - ETA: 3s - loss: 7.4996 - acc: 0.530 - ETA: 3s - loss: 7.4653 - acc: 0.532 - ETA: 3s - loss: 7.3892 - acc: 0.537 - ETA: 3s - loss: 7.4263 - acc: 0.534 - ETA: 2s - loss: 7.3442 - acc: 0.539 - ETA: 2s - loss: 7.3320 - acc: 0.540 - ETA: 2s - loss: 7.3292 - acc: 0.541 - ETA: 2s - loss: 7.2976 - acc: 0.542 - ETA: 2s - loss: 7.3021 - acc: 0.542 - ETA: 2s - loss: 7.3273 - acc: 0.540 - ETA: 2s - loss: 7.3710 - acc: 0.538 - ETA: 2s - loss: 7.4051 - acc: 0.536 - ETA: 2s - loss: 7.4029 - acc: 0.536 - ETA: 2s - loss: 7.4142 - acc: 0.535 - ETA: 2s - loss: 7.4313 - acc: 0.534 - ETA: 3s - loss: 7.4194 - acc: 0.535 - ETA: 3s - loss: 7.4121 - acc: 0.535 - ETA: 3s - loss: 7.4375 - acc: 0.534 - ETA: 2s - loss: 7.4174 - acc: 0.535 - ETA: 2s - loss: 7.4320 - acc: 0.533 - ETA: 2s - loss: 7.3922 - acc: 0.535 - ETA: 2s - loss: 7.4090 - acc: 0.534 - ETA: 2s - loss: 7.3913 - acc: 0.534 - ETA: 2s - loss: 7.3885 - acc: 0.534 - ETA: 2s - loss: 7.3610 - acc: 0.536 - ETA: 2s - loss: 7.3631 - acc: 0.536 - ETA: 2s - loss: 7.4019 - acc: 0.533 - ETA: 2s - loss: 7.3767 - acc: 0.535 - ETA: 2s - loss: 7.3988 - acc: 0.533 - ETA: 1s - loss: 7.4238 - acc: 0.532 - ETA: 1s - loss: 7.4189 - acc: 0.533 - ETA: 1s - loss: 7.4150 - acc: 0.533 - ETA: 1s - loss: 7.4152 - acc: 0.533 - ETA: 1s - loss: 7.4309 - acc: 0.532 - ETA: 1s - loss: 7.4265 - acc: 0.533 - ETA: 1s - loss: 7.4093 - acc: 0.534 - ETA: 1s - loss: 7.3991 - acc: 0.534 - ETA: 1s - loss: 7.3975 - acc: 0.534 - ETA: 1s - loss: 7.4056 - acc: 0.533 - ETA: 1s - loss: 7.4073 - acc: 0.533 - ETA: 1s - loss: 7.3914 - acc: 0.534 - ETA: 1s - loss: 7.3919 - acc: 0.534 - ETA: 1s - loss: 7.3669 - acc: 0.536 - ETA: 0s - loss: 7.3520 - acc: 0.536 - ETA: 0s - loss: 7.3413 - acc: 0.537 - ETA: 0s - loss: 7.3409 - acc: 0.537 - ETA: 0s - loss: 7.2993 - acc: 0.540 - ETA: 0s - loss: 7.2958 - acc: 0.540 - ETA: 0s - loss: 7.3014 - acc: 0.540 - ETA: 0s - loss: 7.3125 - acc: 0.539 - ETA: 0s - loss: 7.3123 - acc: 0.539 - ETA: 0s - loss: 7.3033 - acc: 0.540 - ETA: 0s - loss: 7.3112 - acc: 0.539 - ETA: 0s - loss: 7.3202 - acc: 0.538 - ETA: 0s - loss: 7.3298 - acc: 0.538 - ETA: 0s - loss: 7.3399 - acc: 0.537 - ETA: 0s - loss: 7.3469 - acc: 0.537 - ETA: 0s - loss: 7.3749 - acc: 0.535 - ETA: 0s - loss: 7.3684 - acc: 0.535 - ETA: 0s - loss: 7.3571 - acc: 0.536 - ETA: 0s - loss: 7.3751 - acc: 0.5355Epoch 00022: val_loss improved from 7.98995 to 7.87507, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 7.3761 - acc: 0.5355 - val_loss: 7.8751 - val_acc: 0.4611\n",
      "Epoch 24/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6580/6680 [============================>.] - ETA: 2s - loss: 10.4776 - acc: 0.35 - ETA: 4s - loss: 6.7707 - acc: 0.5800 - ETA: 3s - loss: 7.7565 - acc: 0.515 - ETA: 3s - loss: 7.7719 - acc: 0.515 - ETA: 3s - loss: 7.8144 - acc: 0.513 - ETA: 3s - loss: 7.9513 - acc: 0.503 - ETA: 3s - loss: 7.8986 - acc: 0.507 - ETA: 3s - loss: 7.7511 - acc: 0.517 - ETA: 3s - loss: 7.6748 - acc: 0.522 - ETA: 2s - loss: 7.5974 - acc: 0.527 - ETA: 2s - loss: 7.4737 - acc: 0.534 - ETA: 2s - loss: 7.4833 - acc: 0.533 - ETA: 2s - loss: 7.4018 - acc: 0.537 - ETA: 2s - loss: 7.3547 - acc: 0.540 - ETA: 2s - loss: 7.2984 - acc: 0.543 - ETA: 2s - loss: 7.2038 - acc: 0.550 - ETA: 2s - loss: 7.2218 - acc: 0.548 - ETA: 2s - loss: 7.2700 - acc: 0.545 - ETA: 2s - loss: 7.2876 - acc: 0.543 - ETA: 2s - loss: 7.2943 - acc: 0.543 - ETA: 2s - loss: 7.2663 - acc: 0.544 - ETA: 2s - loss: 7.2366 - acc: 0.546 - ETA: 2s - loss: 7.1954 - acc: 0.549 - ETA: 2s - loss: 7.1845 - acc: 0.550 - ETA: 2s - loss: 7.2130 - acc: 0.548 - ETA: 2s - loss: 7.2631 - acc: 0.545 - ETA: 2s - loss: 7.2927 - acc: 0.544 - ETA: 2s - loss: 7.2571 - acc: 0.546 - ETA: 1s - loss: 7.2702 - acc: 0.544 - ETA: 1s - loss: 7.2802 - acc: 0.544 - ETA: 1s - loss: 7.2794 - acc: 0.544 - ETA: 1s - loss: 7.2837 - acc: 0.544 - ETA: 1s - loss: 7.3226 - acc: 0.541 - ETA: 1s - loss: 7.3358 - acc: 0.540 - ETA: 1s - loss: 7.3441 - acc: 0.539 - ETA: 1s - loss: 7.3455 - acc: 0.539 - ETA: 1s - loss: 7.3659 - acc: 0.538 - ETA: 1s - loss: 7.3417 - acc: 0.539 - ETA: 1s - loss: 7.3433 - acc: 0.540 - ETA: 1s - loss: 7.3171 - acc: 0.541 - ETA: 1s - loss: 7.3156 - acc: 0.541 - ETA: 1s - loss: 7.2988 - acc: 0.542 - ETA: 1s - loss: 7.2799 - acc: 0.543 - ETA: 1s - loss: 7.2809 - acc: 0.543 - ETA: 1s - loss: 7.3082 - acc: 0.542 - ETA: 1s - loss: 7.3260 - acc: 0.541 - ETA: 1s - loss: 7.3211 - acc: 0.541 - ETA: 1s - loss: 7.3499 - acc: 0.539 - ETA: 0s - loss: 7.3413 - acc: 0.540 - ETA: 0s - loss: 7.3653 - acc: 0.539 - ETA: 0s - loss: 7.3618 - acc: 0.539 - ETA: 0s - loss: 7.3727 - acc: 0.538 - ETA: 0s - loss: 7.3856 - acc: 0.537 - ETA: 0s - loss: 7.3781 - acc: 0.538 - ETA: 0s - loss: 7.3704 - acc: 0.538 - ETA: 0s - loss: 7.3637 - acc: 0.538 - ETA: 0s - loss: 7.3482 - acc: 0.539 - ETA: 0s - loss: 7.3361 - acc: 0.540 - ETA: 0s - loss: 7.3542 - acc: 0.538 - ETA: 0s - loss: 7.3319 - acc: 0.540 - ETA: 0s - loss: 7.3333 - acc: 0.540 - ETA: 0s - loss: 7.3477 - acc: 0.539 - ETA: 0s - loss: 7.3371 - acc: 0.540 - ETA: 0s - loss: 7.3410 - acc: 0.539 - ETA: 0s - loss: 7.3298 - acc: 0.540 - ETA: 0s - loss: 7.3361 - acc: 0.5401Epoch 00023: val_loss improved from 7.87507 to 7.86337, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 7.3276 - acc: 0.5407 - val_loss: 7.8634 - val_acc: 0.4659\n",
      "Epoch 25/100\n",
      "6600/6680 [============================>.] - ETA: 4s - loss: 8.0602 - acc: 0.500 - ETA: 4s - loss: 7.4146 - acc: 0.540 - ETA: 3s - loss: 7.2601 - acc: 0.545 - ETA: 3s - loss: 7.2081 - acc: 0.550 - ETA: 3s - loss: 7.5155 - acc: 0.530 - ETA: 3s - loss: 7.3019 - acc: 0.544 - ETA: 3s - loss: 7.1824 - acc: 0.551 - ETA: 3s - loss: 6.9592 - acc: 0.565 - ETA: 3s - loss: 6.8980 - acc: 0.568 - ETA: 3s - loss: 6.9525 - acc: 0.565 - ETA: 2s - loss: 7.0562 - acc: 0.557 - ETA: 2s - loss: 7.1285 - acc: 0.553 - ETA: 2s - loss: 7.0895 - acc: 0.554 - ETA: 2s - loss: 7.0187 - acc: 0.559 - ETA: 2s - loss: 7.1565 - acc: 0.550 - ETA: 2s - loss: 7.1609 - acc: 0.550 - ETA: 2s - loss: 7.1300 - acc: 0.551 - ETA: 2s - loss: 7.0959 - acc: 0.553 - ETA: 2s - loss: 7.1528 - acc: 0.550 - ETA: 2s - loss: 7.2622 - acc: 0.542 - ETA: 2s - loss: 7.2958 - acc: 0.540 - ETA: 2s - loss: 7.2642 - acc: 0.541 - ETA: 2s - loss: 7.2492 - acc: 0.543 - ETA: 2s - loss: 7.2578 - acc: 0.542 - ETA: 2s - loss: 7.2721 - acc: 0.541 - ETA: 2s - loss: 7.2719 - acc: 0.541 - ETA: 2s - loss: 7.2717 - acc: 0.541 - ETA: 2s - loss: 7.2593 - acc: 0.542 - ETA: 2s - loss: 7.2372 - acc: 0.544 - ETA: 1s - loss: 7.2436 - acc: 0.543 - ETA: 1s - loss: 7.2227 - acc: 0.545 - ETA: 1s - loss: 7.2083 - acc: 0.546 - ETA: 1s - loss: 7.1997 - acc: 0.546 - ETA: 1s - loss: 7.2117 - acc: 0.546 - ETA: 1s - loss: 7.2459 - acc: 0.544 - ETA: 1s - loss: 7.2104 - acc: 0.546 - ETA: 1s - loss: 7.2036 - acc: 0.546 - ETA: 1s - loss: 7.1967 - acc: 0.547 - ETA: 1s - loss: 7.1899 - acc: 0.547 - ETA: 1s - loss: 7.2050 - acc: 0.546 - ETA: 1s - loss: 7.1666 - acc: 0.549 - ETA: 1s - loss: 7.1586 - acc: 0.549 - ETA: 1s - loss: 7.1570 - acc: 0.550 - ETA: 1s - loss: 7.1598 - acc: 0.549 - ETA: 1s - loss: 7.1733 - acc: 0.548 - ETA: 1s - loss: 7.1646 - acc: 0.549 - ETA: 1s - loss: 7.1905 - acc: 0.548 - ETA: 0s - loss: 7.1694 - acc: 0.549 - ETA: 0s - loss: 7.1753 - acc: 0.548 - ETA: 0s - loss: 7.1674 - acc: 0.549 - ETA: 0s - loss: 7.1849 - acc: 0.548 - ETA: 0s - loss: 7.1677 - acc: 0.549 - ETA: 0s - loss: 7.1573 - acc: 0.550 - ETA: 0s - loss: 7.1683 - acc: 0.549 - ETA: 0s - loss: 7.1728 - acc: 0.549 - ETA: 0s - loss: 7.1730 - acc: 0.549 - ETA: 0s - loss: 7.1913 - acc: 0.548 - ETA: 0s - loss: 7.2007 - acc: 0.547 - ETA: 0s - loss: 7.2025 - acc: 0.547 - ETA: 0s - loss: 7.2141 - acc: 0.546 - ETA: 0s - loss: 7.2125 - acc: 0.546 - ETA: 0s - loss: 7.2241 - acc: 0.546 - ETA: 0s - loss: 7.2501 - acc: 0.544 - ETA: 0s - loss: 7.2459 - acc: 0.544 - ETA: 0s - loss: 7.2585 - acc: 0.544 - ETA: 0s - loss: 7.2393 - acc: 0.5453Epoch 00024: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 7.2395 - acc: 0.5454 - val_loss: 7.8670 - val_acc: 0.4623\n",
      "Epoch 26/100\n",
      "6660/6680 [============================>.] - ETA: 3s - loss: 6.4477 - acc: 0.600 - ETA: 3s - loss: 7.2633 - acc: 0.542 - ETA: 3s - loss: 7.2604 - acc: 0.545 - ETA: 4s - loss: 6.8562 - acc: 0.570 - ETA: 6s - loss: 7.0107 - acc: 0.561 - ETA: 5s - loss: 7.5285 - acc: 0.529 - ETA: 5s - loss: 7.3994 - acc: 0.538 - ETA: 5s - loss: 7.4727 - acc: 0.534 - ETA: 5s - loss: 7.5814 - acc: 0.527 - ETA: 4s - loss: 7.7031 - acc: 0.519 - ETA: 4s - loss: 7.3979 - acc: 0.538 - ETA: 4s - loss: 7.4145 - acc: 0.537 - ETA: 3s - loss: 7.2061 - acc: 0.550 - ETA: 3s - loss: 7.1261 - acc: 0.554 - ETA: 3s - loss: 7.1248 - acc: 0.554 - ETA: 3s - loss: 7.1253 - acc: 0.554 - ETA: 3s - loss: 7.1159 - acc: 0.555 - ETA: 2s - loss: 7.0562 - acc: 0.558 - ETA: 2s - loss: 7.0809 - acc: 0.556 - ETA: 2s - loss: 7.1103 - acc: 0.554 - ETA: 2s - loss: 7.1422 - acc: 0.552 - ETA: 2s - loss: 7.1273 - acc: 0.553 - ETA: 2s - loss: 7.0864 - acc: 0.556 - ETA: 2s - loss: 7.1540 - acc: 0.552 - ETA: 2s - loss: 7.1179 - acc: 0.554 - ETA: 1s - loss: 7.0894 - acc: 0.556 - ETA: 1s - loss: 7.1134 - acc: 0.554 - ETA: 1s - loss: 7.1790 - acc: 0.551 - ETA: 1s - loss: 7.1669 - acc: 0.551 - ETA: 1s - loss: 7.1497 - acc: 0.553 - ETA: 1s - loss: 7.1431 - acc: 0.553 - ETA: 1s - loss: 7.1525 - acc: 0.552 - ETA: 1s - loss: 7.1559 - acc: 0.552 - ETA: 1s - loss: 7.1321 - acc: 0.554 - ETA: 1s - loss: 7.1471 - acc: 0.553 - ETA: 1s - loss: 7.1494 - acc: 0.553 - ETA: 1s - loss: 7.1606 - acc: 0.552 - ETA: 1s - loss: 7.1710 - acc: 0.551 - ETA: 1s - loss: 7.1886 - acc: 0.550 - ETA: 1s - loss: 7.2058 - acc: 0.549 - ETA: 1s - loss: 7.1728 - acc: 0.551 - ETA: 1s - loss: 7.1858 - acc: 0.551 - ETA: 1s - loss: 7.1834 - acc: 0.551 - ETA: 1s - loss: 7.1990 - acc: 0.550 - ETA: 1s - loss: 7.2140 - acc: 0.549 - ETA: 1s - loss: 7.2253 - acc: 0.548 - ETA: 0s - loss: 7.2261 - acc: 0.549 - ETA: 0s - loss: 7.2267 - acc: 0.549 - ETA: 0s - loss: 7.2075 - acc: 0.550 - ETA: 0s - loss: 7.1954 - acc: 0.551 - ETA: 0s - loss: 7.2122 - acc: 0.550 - ETA: 0s - loss: 7.2071 - acc: 0.550 - ETA: 0s - loss: 7.2352 - acc: 0.548 - ETA: 0s - loss: 7.2334 - acc: 0.548 - ETA: 0s - loss: 7.2277 - acc: 0.548 - ETA: 0s - loss: 7.2427 - acc: 0.548 - ETA: 0s - loss: 7.2315 - acc: 0.548 - ETA: 0s - loss: 7.2271 - acc: 0.549 - ETA: 0s - loss: 7.2248 - acc: 0.549 - ETA: 0s - loss: 7.2256 - acc: 0.549 - ETA: 0s - loss: 7.2421 - acc: 0.548 - ETA: 0s - loss: 7.2318 - acc: 0.548 - ETA: 0s - loss: 7.2248 - acc: 0.549 - ETA: 0s - loss: 7.2019 - acc: 0.550 - ETA: 0s - loss: 7.1987 - acc: 0.550 - ETA: 0s - loss: 7.2019 - acc: 0.550 - ETA: 0s - loss: 7.2045 - acc: 0.550 - ETA: 0s - loss: 7.2062 - acc: 0.550 - ETA: 0s - loss: 7.2092 - acc: 0.5498Epoch 00025: val_loss improved from 7.86337 to 7.81631, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 7.2045 - acc: 0.5501 - val_loss: 7.8163 - val_acc: 0.4515\n",
      "Epoch 27/100\n",
      "6620/6680 [============================>.] - ETA: 2s - loss: 7.2534 - acc: 0.550 - ETA: 3s - loss: 8.4626 - acc: 0.475 - ETA: 3s - loss: 7.8984 - acc: 0.510 - ETA: 4s - loss: 7.7279 - acc: 0.520 - ETA: 4s - loss: 7.5426 - acc: 0.532 - ETA: 3s - loss: 7.5988 - acc: 0.527 - ETA: 4s - loss: 7.6697 - acc: 0.523 - ETA: 3s - loss: 7.5506 - acc: 0.530 - ETA: 3s - loss: 7.3935 - acc: 0.540 - ETA: 3s - loss: 7.5763 - acc: 0.529 - ETA: 3s - loss: 7.4887 - acc: 0.534 - ETA: 3s - loss: 7.3571 - acc: 0.543 - ETA: 3s - loss: 7.4941 - acc: 0.534 - ETA: 3s - loss: 7.4474 - acc: 0.537 - ETA: 3s - loss: 7.3614 - acc: 0.542 - ETA: 3s - loss: 7.3444 - acc: 0.543 - ETA: 3s - loss: 7.3162 - acc: 0.545 - ETA: 3s - loss: 7.3019 - acc: 0.546 - ETA: 3s - loss: 7.2788 - acc: 0.548 - ETA: 3s - loss: 7.2780 - acc: 0.548 - ETA: 3s - loss: 7.2964 - acc: 0.547 - ETA: 3s - loss: 7.2853 - acc: 0.547 - ETA: 3s - loss: 7.3196 - acc: 0.545 - ETA: 3s - loss: 7.2658 - acc: 0.549 - ETA: 3s - loss: 7.2253 - acc: 0.551 - ETA: 3s - loss: 7.2337 - acc: 0.551 - ETA: 3s - loss: 7.2267 - acc: 0.551 - ETA: 3s - loss: 7.2194 - acc: 0.551 - ETA: 3s - loss: 7.2052 - acc: 0.552 - ETA: 3s - loss: 7.2015 - acc: 0.552 - ETA: 3s - loss: 7.2092 - acc: 0.552 - ETA: 3s - loss: 7.2029 - acc: 0.552 - ETA: 3s - loss: 7.1829 - acc: 0.553 - ETA: 4s - loss: 7.1973 - acc: 0.553 - ETA: 4s - loss: 7.1847 - acc: 0.553 - ETA: 3s - loss: 7.1498 - acc: 0.555 - ETA: 3s - loss: 7.1298 - acc: 0.556 - ETA: 3s - loss: 7.1096 - acc: 0.557 - ETA: 3s - loss: 7.1266 - acc: 0.556 - ETA: 3s - loss: 7.1363 - acc: 0.555 - ETA: 3s - loss: 7.1919 - acc: 0.552 - ETA: 3s - loss: 7.2201 - acc: 0.550 - ETA: 2s - loss: 7.2161 - acc: 0.550 - ETA: 2s - loss: 7.2272 - acc: 0.550 - ETA: 2s - loss: 7.2231 - acc: 0.550 - ETA: 2s - loss: 7.2190 - acc: 0.550 - ETA: 2s - loss: 7.2430 - acc: 0.549 - ETA: 2s - loss: 7.2458 - acc: 0.548 - ETA: 2s - loss: 7.2418 - acc: 0.549 - ETA: 2s - loss: 7.2551 - acc: 0.548 - ETA: 2s - loss: 7.2594 - acc: 0.548 - ETA: 2s - loss: 7.2344 - acc: 0.549 - ETA: 2s - loss: 7.2252 - acc: 0.550 - ETA: 2s - loss: 7.2293 - acc: 0.549 - ETA: 2s - loss: 7.2214 - acc: 0.550 - ETA: 2s - loss: 7.2103 - acc: 0.550 - ETA: 2s - loss: 7.1960 - acc: 0.551 - ETA: 2s - loss: 7.1678 - acc: 0.553 - ETA: 1s - loss: 7.1771 - acc: 0.552 - ETA: 1s - loss: 7.1745 - acc: 0.552 - ETA: 1s - loss: 7.1803 - acc: 0.552 - ETA: 1s - loss: 7.1956 - acc: 0.551 - ETA: 1s - loss: 7.2165 - acc: 0.550 - ETA: 1s - loss: 7.2177 - acc: 0.549 - ETA: 1s - loss: 7.2319 - acc: 0.548 - ETA: 1s - loss: 7.2298 - acc: 0.548 - ETA: 1s - loss: 7.2086 - acc: 0.549 - ETA: 1s - loss: 7.2249 - acc: 0.548 - ETA: 1s - loss: 7.2188 - acc: 0.549 - ETA: 1s - loss: 7.1923 - acc: 0.550 - ETA: 0s - loss: 7.1972 - acc: 0.550 - ETA: 0s - loss: 7.1894 - acc: 0.551 - ETA: 0s - loss: 7.1707 - acc: 0.552 - ETA: 0s - loss: 7.1722 - acc: 0.552 - ETA: 0s - loss: 7.1598 - acc: 0.552 - ETA: 0s - loss: 7.1586 - acc: 0.553 - ETA: 0s - loss: 7.1708 - acc: 0.552 - ETA: 0s - loss: 7.1698 - acc: 0.552 - ETA: 0s - loss: 7.1555 - acc: 0.553 - ETA: 0s - loss: 7.1700 - acc: 0.552 - ETA: 0s - loss: 7.1598 - acc: 0.553 - ETA: 0s - loss: 7.1687 - acc: 0.552 - ETA: 0s - loss: 7.1678 - acc: 0.5524Epoch 00026: val_loss improved from 7.81631 to 7.71577, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 7.1806 - acc: 0.5516 - val_loss: 7.7158 - val_acc: 0.4719\n",
      "Epoch 28/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6600/6680 [============================>.] - ETA: 2s - loss: 7.2532 - acc: 0.550 - ETA: 3s - loss: 6.7164 - acc: 0.583 - ETA: 3s - loss: 7.0433 - acc: 0.559 - ETA: 3s - loss: 7.0586 - acc: 0.559 - ETA: 3s - loss: 6.9515 - acc: 0.566 - ETA: 3s - loss: 7.0420 - acc: 0.561 - ETA: 3s - loss: 6.7903 - acc: 0.577 - ETA: 3s - loss: 6.9217 - acc: 0.569 - ETA: 3s - loss: 7.1034 - acc: 0.558 - ETA: 3s - loss: 7.0337 - acc: 0.562 - ETA: 2s - loss: 7.0479 - acc: 0.560 - ETA: 2s - loss: 7.1407 - acc: 0.554 - ETA: 2s - loss: 7.0882 - acc: 0.557 - ETA: 2s - loss: 7.0410 - acc: 0.560 - ETA: 2s - loss: 6.9574 - acc: 0.565 - ETA: 2s - loss: 6.9557 - acc: 0.565 - ETA: 2s - loss: 7.0251 - acc: 0.561 - ETA: 2s - loss: 7.0763 - acc: 0.557 - ETA: 2s - loss: 7.0772 - acc: 0.558 - ETA: 2s - loss: 7.0535 - acc: 0.559 - ETA: 2s - loss: 7.0321 - acc: 0.561 - ETA: 2s - loss: 7.0128 - acc: 0.562 - ETA: 2s - loss: 7.0162 - acc: 0.562 - ETA: 2s - loss: 6.9992 - acc: 0.563 - ETA: 2s - loss: 7.0679 - acc: 0.559 - ETA: 2s - loss: 7.1220 - acc: 0.555 - ETA: 2s - loss: 7.1030 - acc: 0.557 - ETA: 2s - loss: 7.1084 - acc: 0.556 - ETA: 1s - loss: 7.1925 - acc: 0.551 - ETA: 1s - loss: 7.1961 - acc: 0.551 - ETA: 1s - loss: 7.1980 - acc: 0.551 - ETA: 1s - loss: 7.1803 - acc: 0.551 - ETA: 1s - loss: 7.1432 - acc: 0.554 - ETA: 1s - loss: 7.1178 - acc: 0.555 - ETA: 1s - loss: 7.1271 - acc: 0.555 - ETA: 1s - loss: 7.1352 - acc: 0.554 - ETA: 1s - loss: 7.0861 - acc: 0.557 - ETA: 1s - loss: 7.0905 - acc: 0.557 - ETA: 1s - loss: 7.1122 - acc: 0.556 - ETA: 1s - loss: 7.0851 - acc: 0.557 - ETA: 1s - loss: 7.1088 - acc: 0.555 - ETA: 1s - loss: 7.1123 - acc: 0.555 - ETA: 1s - loss: 7.1349 - acc: 0.554 - ETA: 1s - loss: 7.1637 - acc: 0.552 - ETA: 1s - loss: 7.1449 - acc: 0.553 - ETA: 1s - loss: 7.1126 - acc: 0.555 - ETA: 1s - loss: 7.1122 - acc: 0.555 - ETA: 0s - loss: 7.1018 - acc: 0.556 - ETA: 1s - loss: 7.0991 - acc: 0.556 - ETA: 1s - loss: 7.1036 - acc: 0.556 - ETA: 0s - loss: 7.1087 - acc: 0.556 - ETA: 0s - loss: 7.1060 - acc: 0.556 - ETA: 0s - loss: 7.0970 - acc: 0.557 - ETA: 0s - loss: 7.0713 - acc: 0.558 - ETA: 0s - loss: 7.0695 - acc: 0.558 - ETA: 0s - loss: 7.0835 - acc: 0.557 - ETA: 0s - loss: 7.0748 - acc: 0.558 - ETA: 0s - loss: 7.0724 - acc: 0.558 - ETA: 0s - loss: 7.0700 - acc: 0.558 - ETA: 0s - loss: 7.0652 - acc: 0.558 - ETA: 0s - loss: 7.0659 - acc: 0.558 - ETA: 0s - loss: 7.0605 - acc: 0.559 - ETA: 0s - loss: 7.0580 - acc: 0.559 - ETA: 0s - loss: 7.0527 - acc: 0.559 - ETA: 0s - loss: 7.0625 - acc: 0.559 - ETA: 0s - loss: 7.0632 - acc: 0.559 - ETA: 0s - loss: 7.0572 - acc: 0.559 - ETA: 0s - loss: 7.0717 - acc: 0.558 - ETA: 0s - loss: 7.0827 - acc: 0.557 - ETA: 0s - loss: 7.0897 - acc: 0.557 - ETA: 0s - loss: 7.0920 - acc: 0.556 - ETA: 0s - loss: 7.0838 - acc: 0.557 - ETA: 0s - loss: 7.0725 - acc: 0.558 - ETA: 0s - loss: 7.0883 - acc: 0.557 - ETA: 0s - loss: 7.1025 - acc: 0.556 - ETA: 0s - loss: 7.1023 - acc: 0.556 - ETA: 0s - loss: 7.1257 - acc: 0.554 - ETA: 0s - loss: 7.1349 - acc: 0.554 - ETA: 0s - loss: 7.1389 - acc: 0.553 - ETA: 0s - loss: 7.1457 - acc: 0.5533Epoch 00027: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 7.1573 - acc: 0.5525 - val_loss: 7.7362 - val_acc: 0.4671\n",
      "Epoch 29/100\n",
      "6640/6680 [============================>.] - ETA: 3s - loss: 8.0590 - acc: 0.500 - ETA: 3s - loss: 8.5332 - acc: 0.457 - ETA: 3s - loss: 8.1441 - acc: 0.483 - ETA: 3s - loss: 7.9206 - acc: 0.497 - ETA: 3s - loss: 7.3324 - acc: 0.534 - ETA: 3s - loss: 7.4728 - acc: 0.525 - ETA: 3s - loss: 7.3881 - acc: 0.532 - ETA: 3s - loss: 7.3736 - acc: 0.533 - ETA: 3s - loss: 7.3792 - acc: 0.534 - ETA: 3s - loss: 7.3320 - acc: 0.538 - ETA: 3s - loss: 7.2624 - acc: 0.543 - ETA: 3s - loss: 7.2935 - acc: 0.540 - ETA: 3s - loss: 7.3707 - acc: 0.535 - ETA: 3s - loss: 7.4123 - acc: 0.533 - ETA: 3s - loss: 7.4731 - acc: 0.529 - ETA: 2s - loss: 7.5328 - acc: 0.525 - ETA: 2s - loss: 7.3661 - acc: 0.536 - ETA: 2s - loss: 7.2804 - acc: 0.541 - ETA: 2s - loss: 7.1890 - acc: 0.547 - ETA: 2s - loss: 7.1676 - acc: 0.549 - ETA: 2s - loss: 7.1075 - acc: 0.553 - ETA: 2s - loss: 7.1443 - acc: 0.551 - ETA: 2s - loss: 7.1345 - acc: 0.552 - ETA: 2s - loss: 7.1397 - acc: 0.552 - ETA: 2s - loss: 7.1663 - acc: 0.550 - ETA: 2s - loss: 7.1528 - acc: 0.551 - ETA: 2s - loss: 7.1511 - acc: 0.550 - ETA: 2s - loss: 7.1428 - acc: 0.551 - ETA: 2s - loss: 7.1166 - acc: 0.553 - ETA: 2s - loss: 7.1044 - acc: 0.554 - ETA: 2s - loss: 7.1149 - acc: 0.553 - ETA: 2s - loss: 7.0886 - acc: 0.555 - ETA: 1s - loss: 7.0888 - acc: 0.555 - ETA: 1s - loss: 7.0591 - acc: 0.557 - ETA: 1s - loss: 7.0565 - acc: 0.557 - ETA: 1s - loss: 7.0622 - acc: 0.557 - ETA: 1s - loss: 7.0359 - acc: 0.559 - ETA: 1s - loss: 7.0242 - acc: 0.560 - ETA: 1s - loss: 7.0593 - acc: 0.558 - ETA: 1s - loss: 7.0643 - acc: 0.557 - ETA: 1s - loss: 7.0691 - acc: 0.557 - ETA: 1s - loss: 7.0786 - acc: 0.557 - ETA: 1s - loss: 7.0795 - acc: 0.557 - ETA: 1s - loss: 7.1026 - acc: 0.555 - ETA: 1s - loss: 7.1394 - acc: 0.553 - ETA: 1s - loss: 7.1022 - acc: 0.555 - ETA: 1s - loss: 7.0872 - acc: 0.556 - ETA: 1s - loss: 7.0838 - acc: 0.557 - ETA: 1s - loss: 7.0867 - acc: 0.557 - ETA: 1s - loss: 7.0964 - acc: 0.556 - ETA: 0s - loss: 7.1036 - acc: 0.556 - ETA: 0s - loss: 7.1016 - acc: 0.556 - ETA: 0s - loss: 7.1153 - acc: 0.555 - ETA: 0s - loss: 7.1057 - acc: 0.555 - ETA: 0s - loss: 7.1025 - acc: 0.556 - ETA: 0s - loss: 7.1142 - acc: 0.555 - ETA: 0s - loss: 7.1196 - acc: 0.555 - ETA: 0s - loss: 7.1367 - acc: 0.554 - ETA: 0s - loss: 7.1411 - acc: 0.553 - ETA: 0s - loss: 7.1347 - acc: 0.554 - ETA: 0s - loss: 7.1513 - acc: 0.553 - ETA: 0s - loss: 7.1741 - acc: 0.551 - ETA: 0s - loss: 7.1663 - acc: 0.552 - ETA: 0s - loss: 7.1623 - acc: 0.552 - ETA: 0s - loss: 7.1652 - acc: 0.552 - ETA: 0s - loss: 7.1580 - acc: 0.552 - ETA: 0s - loss: 7.1618 - acc: 0.552 - ETA: 0s - loss: 7.1595 - acc: 0.552 - ETA: 0s - loss: 7.1572 - acc: 0.552 - ETA: 0s - loss: 7.1534 - acc: 0.553 - ETA: 0s - loss: 7.1744 - acc: 0.551 - ETA: 0s - loss: 7.1673 - acc: 0.552 - ETA: 0s - loss: 7.1701 - acc: 0.552 - ETA: 0s - loss: 7.1628 - acc: 0.552 - ETA: 0s - loss: 7.1731 - acc: 0.551 - ETA: 0s - loss: 7.1639 - acc: 0.552 - ETA: 0s - loss: 7.1478 - acc: 0.553 - ETA: 0s - loss: 7.1393 - acc: 0.5541Epoch 00028: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 7.1448 - acc: 0.5537 - val_loss: 7.7930 - val_acc: 0.4623\n",
      "Epoch 30/100\n",
      "6580/6680 [============================>.] - ETA: 2s - loss: 8.0919 - acc: 0.500 - ETA: 3s - loss: 7.0959 - acc: 0.558 - ETA: 3s - loss: 7.0083 - acc: 0.559 - ETA: 3s - loss: 7.2869 - acc: 0.543 - ETA: 3s - loss: 7.3175 - acc: 0.542 - ETA: 3s - loss: 7.4196 - acc: 0.536 - ETA: 3s - loss: 7.2553 - acc: 0.546 - ETA: 3s - loss: 7.3224 - acc: 0.542 - ETA: 3s - loss: 7.4240 - acc: 0.536 - ETA: 3s - loss: 7.3295 - acc: 0.541 - ETA: 3s - loss: 7.2756 - acc: 0.545 - ETA: 3s - loss: 7.3175 - acc: 0.542 - ETA: 2s - loss: 7.3092 - acc: 0.542 - ETA: 2s - loss: 7.2690 - acc: 0.543 - ETA: 2s - loss: 7.2219 - acc: 0.546 - ETA: 2s - loss: 7.2143 - acc: 0.547 - ETA: 2s - loss: 7.2193 - acc: 0.547 - ETA: 2s - loss: 7.2382 - acc: 0.545 - ETA: 2s - loss: 7.1575 - acc: 0.551 - ETA: 2s - loss: 7.1145 - acc: 0.554 - ETA: 2s - loss: 7.1424 - acc: 0.552 - ETA: 2s - loss: 7.1704 - acc: 0.550 - ETA: 2s - loss: 7.1291 - acc: 0.552 - ETA: 2s - loss: 7.0727 - acc: 0.556 - ETA: 2s - loss: 7.1134 - acc: 0.554 - ETA: 2s - loss: 7.1512 - acc: 0.551 - ETA: 2s - loss: 7.1384 - acc: 0.552 - ETA: 2s - loss: 7.1257 - acc: 0.552 - ETA: 2s - loss: 7.1679 - acc: 0.550 - ETA: 2s - loss: 7.1769 - acc: 0.549 - ETA: 1s - loss: 7.1642 - acc: 0.550 - ETA: 1s - loss: 7.1638 - acc: 0.549 - ETA: 1s - loss: 7.1508 - acc: 0.550 - ETA: 1s - loss: 7.1952 - acc: 0.547 - ETA: 1s - loss: 7.2276 - acc: 0.545 - ETA: 1s - loss: 7.1988 - acc: 0.546 - ETA: 1s - loss: 7.2091 - acc: 0.546 - ETA: 1s - loss: 7.2009 - acc: 0.546 - ETA: 1s - loss: 7.2440 - acc: 0.544 - ETA: 1s - loss: 7.2191 - acc: 0.545 - ETA: 1s - loss: 7.2209 - acc: 0.545 - ETA: 1s - loss: 7.2057 - acc: 0.546 - ETA: 1s - loss: 7.1981 - acc: 0.546 - ETA: 1s - loss: 7.1536 - acc: 0.548 - ETA: 1s - loss: 7.1257 - acc: 0.550 - ETA: 1s - loss: 7.1463 - acc: 0.548 - ETA: 1s - loss: 7.1198 - acc: 0.550 - ETA: 1s - loss: 7.1016 - acc: 0.551 - ETA: 1s - loss: 7.0947 - acc: 0.552 - ETA: 1s - loss: 7.0955 - acc: 0.552 - ETA: 1s - loss: 7.0818 - acc: 0.553 - ETA: 1s - loss: 7.0918 - acc: 0.552 - ETA: 0s - loss: 7.0836 - acc: 0.552 - ETA: 0s - loss: 7.0741 - acc: 0.553 - ETA: 0s - loss: 7.0899 - acc: 0.552 - ETA: 0s - loss: 7.1041 - acc: 0.551 - ETA: 0s - loss: 7.0908 - acc: 0.552 - ETA: 0s - loss: 7.1026 - acc: 0.551 - ETA: 0s - loss: 7.0790 - acc: 0.552 - ETA: 0s - loss: 7.0655 - acc: 0.553 - ETA: 0s - loss: 7.0703 - acc: 0.552 - ETA: 0s - loss: 7.0799 - acc: 0.552 - ETA: 0s - loss: 7.0615 - acc: 0.553 - ETA: 0s - loss: 7.0731 - acc: 0.552 - ETA: 0s - loss: 7.0605 - acc: 0.553 - ETA: 0s - loss: 7.0818 - acc: 0.552 - ETA: 0s - loss: 7.0795 - acc: 0.552 - ETA: 0s - loss: 7.0779 - acc: 0.5530Epoch 00029: val_loss improved from 7.71577 to 7.66030, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 7.0469 - acc: 0.5549 - val_loss: 7.6603 - val_acc: 0.4731\n",
      "Epoch 31/100\n",
      "6660/6680 [============================>.] - ETA: 2s - loss: 8.8650 - acc: 0.450 - ETA: 3s - loss: 6.0064 - acc: 0.621 - ETA: 3s - loss: 6.6760 - acc: 0.579 - ETA: 3s - loss: 6.7513 - acc: 0.575 - ETA: 3s - loss: 6.8932 - acc: 0.565 - ETA: 3s - loss: 7.1267 - acc: 0.551 - ETA: 3s - loss: 7.1358 - acc: 0.550 - ETA: 3s - loss: 7.2420 - acc: 0.543 - ETA: 3s - loss: 7.4171 - acc: 0.532 - ETA: 3s - loss: 7.3427 - acc: 0.537 - ETA: 2s - loss: 7.2256 - acc: 0.545 - ETA: 2s - loss: 7.2688 - acc: 0.542 - ETA: 2s - loss: 7.2839 - acc: 0.541 - ETA: 2s - loss: 7.2888 - acc: 0.541 - ETA: 2s - loss: 7.2822 - acc: 0.540 - ETA: 2s - loss: 7.2210 - acc: 0.544 - ETA: 2s - loss: 7.1030 - acc: 0.551 - ETA: 2s - loss: 7.1009 - acc: 0.551 - ETA: 2s - loss: 7.1169 - acc: 0.549 - ETA: 2s - loss: 7.0421 - acc: 0.554 - ETA: 2s - loss: 7.0855 - acc: 0.551 - ETA: 2s - loss: 7.1084 - acc: 0.550 - ETA: 2s - loss: 7.0642 - acc: 0.553 - ETA: 2s - loss: 7.0678 - acc: 0.552 - ETA: 2s - loss: 7.0876 - acc: 0.551 - ETA: 2s - loss: 7.1039 - acc: 0.550 - ETA: 2s - loss: 7.1400 - acc: 0.548 - ETA: 2s - loss: 7.1356 - acc: 0.548 - ETA: 2s - loss: 7.1342 - acc: 0.549 - ETA: 1s - loss: 7.1556 - acc: 0.547 - ETA: 1s - loss: 7.1430 - acc: 0.548 - ETA: 1s - loss: 7.1664 - acc: 0.547 - ETA: 1s - loss: 7.1840 - acc: 0.546 - ETA: 1s - loss: 7.1529 - acc: 0.548 - ETA: 1s - loss: 7.1868 - acc: 0.546 - ETA: 1s - loss: 7.1847 - acc: 0.546 - ETA: 1s - loss: 7.1687 - acc: 0.547 - ETA: 1s - loss: 7.1618 - acc: 0.546 - ETA: 1s - loss: 7.1439 - acc: 0.547 - ETA: 1s - loss: 7.1318 - acc: 0.548 - ETA: 1s - loss: 7.1263 - acc: 0.548 - ETA: 1s - loss: 7.1428 - acc: 0.547 - ETA: 1s - loss: 7.1269 - acc: 0.548 - ETA: 1s - loss: 7.1006 - acc: 0.550 - ETA: 1s - loss: 7.1122 - acc: 0.549 - ETA: 1s - loss: 7.1310 - acc: 0.548 - ETA: 1s - loss: 7.1275 - acc: 0.548 - ETA: 1s - loss: 7.1271 - acc: 0.548 - ETA: 0s - loss: 7.1061 - acc: 0.549 - ETA: 0s - loss: 7.1136 - acc: 0.549 - ETA: 0s - loss: 7.0942 - acc: 0.550 - ETA: 0s - loss: 7.0753 - acc: 0.552 - ETA: 0s - loss: 7.0484 - acc: 0.553 - ETA: 0s - loss: 7.0432 - acc: 0.554 - ETA: 0s - loss: 7.0279 - acc: 0.555 - ETA: 0s - loss: 7.0146 - acc: 0.556 - ETA: 0s - loss: 7.0275 - acc: 0.555 - ETA: 0s - loss: 7.0159 - acc: 0.556 - ETA: 0s - loss: 7.0035 - acc: 0.556 - ETA: 0s - loss: 6.9859 - acc: 0.557 - ETA: 0s - loss: 6.9818 - acc: 0.558 - ETA: 0s - loss: 6.9803 - acc: 0.558 - ETA: 0s - loss: 6.9718 - acc: 0.558 - ETA: 0s - loss: 6.9696 - acc: 0.558 - ETA: 0s - loss: 6.9609 - acc: 0.559 - ETA: 0s - loss: 6.9260 - acc: 0.561 - ETA: 0s - loss: 6.9435 - acc: 0.560 - ETA: 0s - loss: 6.9506 - acc: 0.5599Epoch 00030: val_loss improved from 7.66030 to 7.63142, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 6.9539 - acc: 0.5597 - val_loss: 7.6314 - val_acc: 0.4599\n",
      "Epoch 32/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6580/6680 [============================>.] - ETA: 2s - loss: 5.6413 - acc: 0.650 - ETA: 3s - loss: 6.7938 - acc: 0.578 - ETA: 3s - loss: 7.2539 - acc: 0.550 - ETA: 3s - loss: 7.2849 - acc: 0.541 - ETA: 3s - loss: 7.1312 - acc: 0.552 - ETA: 3s - loss: 7.1032 - acc: 0.553 - ETA: 3s - loss: 7.0755 - acc: 0.556 - ETA: 3s - loss: 7.2345 - acc: 0.547 - ETA: 3s - loss: 7.1978 - acc: 0.550 - ETA: 3s - loss: 7.2030 - acc: 0.550 - ETA: 3s - loss: 7.1114 - acc: 0.556 - ETA: 3s - loss: 7.0217 - acc: 0.561 - ETA: 2s - loss: 6.8403 - acc: 0.572 - ETA: 2s - loss: 6.8106 - acc: 0.574 - ETA: 2s - loss: 6.7731 - acc: 0.577 - ETA: 2s - loss: 6.7544 - acc: 0.578 - ETA: 2s - loss: 6.8349 - acc: 0.573 - ETA: 2s - loss: 6.8880 - acc: 0.570 - ETA: 2s - loss: 6.8417 - acc: 0.572 - ETA: 2s - loss: 6.8168 - acc: 0.572 - ETA: 2s - loss: 6.8182 - acc: 0.573 - ETA: 2s - loss: 6.8473 - acc: 0.571 - ETA: 2s - loss: 6.8364 - acc: 0.571 - ETA: 2s - loss: 6.9317 - acc: 0.566 - ETA: 2s - loss: 6.9066 - acc: 0.567 - ETA: 2s - loss: 6.8773 - acc: 0.568 - ETA: 2s - loss: 6.8613 - acc: 0.569 - ETA: 2s - loss: 6.8853 - acc: 0.567 - ETA: 2s - loss: 6.8430 - acc: 0.570 - ETA: 2s - loss: 6.8919 - acc: 0.567 - ETA: 2s - loss: 6.8877 - acc: 0.568 - ETA: 2s - loss: 6.8791 - acc: 0.568 - ETA: 2s - loss: 6.8286 - acc: 0.571 - ETA: 1s - loss: 6.8292 - acc: 0.571 - ETA: 1s - loss: 6.8175 - acc: 0.572 - ETA: 1s - loss: 6.8195 - acc: 0.571 - ETA: 1s - loss: 6.8414 - acc: 0.570 - ETA: 1s - loss: 6.8417 - acc: 0.569 - ETA: 1s - loss: 6.8310 - acc: 0.570 - ETA: 1s - loss: 6.8273 - acc: 0.570 - ETA: 1s - loss: 6.8256 - acc: 0.571 - ETA: 1s - loss: 6.8530 - acc: 0.569 - ETA: 1s - loss: 6.8417 - acc: 0.570 - ETA: 1s - loss: 6.8616 - acc: 0.569 - ETA: 1s - loss: 6.8500 - acc: 0.569 - ETA: 1s - loss: 6.8178 - acc: 0.571 - ETA: 1s - loss: 6.8513 - acc: 0.569 - ETA: 1s - loss: 6.8567 - acc: 0.568 - ETA: 1s - loss: 6.8649 - acc: 0.568 - ETA: 1s - loss: 6.8453 - acc: 0.569 - ETA: 1s - loss: 6.8353 - acc: 0.570 - ETA: 1s - loss: 6.8519 - acc: 0.569 - ETA: 0s - loss: 6.8648 - acc: 0.568 - ETA: 0s - loss: 6.8709 - acc: 0.568 - ETA: 0s - loss: 6.8791 - acc: 0.568 - ETA: 0s - loss: 6.8878 - acc: 0.567 - ETA: 0s - loss: 6.9006 - acc: 0.566 - ETA: 0s - loss: 6.9129 - acc: 0.566 - ETA: 0s - loss: 6.9137 - acc: 0.565 - ETA: 0s - loss: 6.9210 - acc: 0.565 - ETA: 0s - loss: 6.9221 - acc: 0.565 - ETA: 0s - loss: 6.9231 - acc: 0.565 - ETA: 0s - loss: 6.9128 - acc: 0.565 - ETA: 0s - loss: 6.8987 - acc: 0.566 - ETA: 0s - loss: 6.9018 - acc: 0.566 - ETA: 0s - loss: 6.9087 - acc: 0.566 - ETA: 0s - loss: 6.9018 - acc: 0.566 - ETA: 0s - loss: 6.9211 - acc: 0.565 - ETA: 0s - loss: 6.9197 - acc: 0.5652Epoch 00031: val_loss improved from 7.63142 to 7.61114, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 6.9197 - acc: 0.5651 - val_loss: 7.6111 - val_acc: 0.4731\n",
      "Epoch 33/100\n",
      "6640/6680 [============================>.] - ETA: 2s - loss: 6.4472 - acc: 0.600 - ETA: 3s - loss: 7.0008 - acc: 0.558 - ETA: 3s - loss: 6.6247 - acc: 0.575 - ETA: 3s - loss: 6.0551 - acc: 0.611 - ETA: 3s - loss: 6.2450 - acc: 0.602 - ETA: 3s - loss: 6.2310 - acc: 0.605 - ETA: 3s - loss: 6.3914 - acc: 0.596 - ETA: 3s - loss: 6.3327 - acc: 0.600 - ETA: 3s - loss: 6.3041 - acc: 0.602 - ETA: 3s - loss: 6.3441 - acc: 0.600 - ETA: 3s - loss: 6.5170 - acc: 0.589 - ETA: 3s - loss: 6.4723 - acc: 0.590 - ETA: 2s - loss: 6.4694 - acc: 0.591 - ETA: 2s - loss: 6.5317 - acc: 0.587 - ETA: 2s - loss: 6.5817 - acc: 0.584 - ETA: 2s - loss: 6.5751 - acc: 0.584 - ETA: 2s - loss: 6.6269 - acc: 0.582 - ETA: 2s - loss: 6.6469 - acc: 0.581 - ETA: 2s - loss: 6.6896 - acc: 0.578 - ETA: 2s - loss: 6.6869 - acc: 0.578 - ETA: 2s - loss: 6.6706 - acc: 0.579 - ETA: 2s - loss: 6.6940 - acc: 0.578 - ETA: 2s - loss: 6.7215 - acc: 0.576 - ETA: 2s - loss: 6.6925 - acc: 0.577 - ETA: 2s - loss: 6.7198 - acc: 0.575 - ETA: 2s - loss: 6.6939 - acc: 0.577 - ETA: 2s - loss: 6.6759 - acc: 0.578 - ETA: 2s - loss: 6.7054 - acc: 0.577 - ETA: 2s - loss: 6.7022 - acc: 0.577 - ETA: 2s - loss: 6.7215 - acc: 0.576 - ETA: 2s - loss: 6.7451 - acc: 0.574 - ETA: 1s - loss: 6.7492 - acc: 0.574 - ETA: 1s - loss: 6.7598 - acc: 0.573 - ETA: 1s - loss: 6.7595 - acc: 0.573 - ETA: 1s - loss: 6.7761 - acc: 0.572 - ETA: 1s - loss: 6.7967 - acc: 0.570 - ETA: 1s - loss: 6.8249 - acc: 0.568 - ETA: 1s - loss: 6.8415 - acc: 0.567 - ETA: 1s - loss: 6.8845 - acc: 0.565 - ETA: 1s - loss: 6.8867 - acc: 0.564 - ETA: 1s - loss: 6.8939 - acc: 0.564 - ETA: 1s - loss: 6.8982 - acc: 0.564 - ETA: 1s - loss: 6.9185 - acc: 0.563 - ETA: 1s - loss: 6.9189 - acc: 0.563 - ETA: 1s - loss: 6.8800 - acc: 0.565 - ETA: 1s - loss: 6.8795 - acc: 0.565 - ETA: 1s - loss: 6.8933 - acc: 0.565 - ETA: 1s - loss: 6.8868 - acc: 0.565 - ETA: 1s - loss: 6.8862 - acc: 0.565 - ETA: 1s - loss: 6.8654 - acc: 0.566 - ETA: 1s - loss: 6.8695 - acc: 0.566 - ETA: 0s - loss: 6.8725 - acc: 0.566 - ETA: 0s - loss: 6.8548 - acc: 0.567 - ETA: 0s - loss: 6.8573 - acc: 0.567 - ETA: 0s - loss: 6.8702 - acc: 0.566 - ETA: 0s - loss: 6.8839 - acc: 0.565 - ETA: 0s - loss: 6.8782 - acc: 0.566 - ETA: 0s - loss: 6.8473 - acc: 0.567 - ETA: 0s - loss: 6.8417 - acc: 0.568 - ETA: 0s - loss: 6.8392 - acc: 0.568 - ETA: 0s - loss: 6.8453 - acc: 0.567 - ETA: 0s - loss: 6.8379 - acc: 0.568 - ETA: 0s - loss: 6.8351 - acc: 0.568 - ETA: 0s - loss: 6.8294 - acc: 0.568 - ETA: 0s - loss: 6.8440 - acc: 0.567 - ETA: 0s - loss: 6.8380 - acc: 0.568 - ETA: 0s - loss: 6.8377 - acc: 0.568 - ETA: 0s - loss: 6.8405 - acc: 0.568 - ETA: 0s - loss: 6.8418 - acc: 0.5682Epoch 00032: val_loss improved from 7.61114 to 7.58822, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 6.8395 - acc: 0.5684 - val_loss: 7.5882 - val_acc: 0.4695\n",
      "Epoch 34/100\n",
      "6640/6680 [============================>.] - ETA: 3s - loss: 6.4472 - acc: 0.600 - ETA: 3s - loss: 7.3876 - acc: 0.541 - ETA: 3s - loss: 7.6072 - acc: 0.525 - ETA: 3s - loss: 7.2764 - acc: 0.546 - ETA: 3s - loss: 7.3127 - acc: 0.544 - ETA: 3s - loss: 7.1059 - acc: 0.554 - ETA: 3s - loss: 7.2000 - acc: 0.548 - ETA: 3s - loss: 7.0088 - acc: 0.558 - ETA: 3s - loss: 6.8545 - acc: 0.569 - ETA: 3s - loss: 6.9264 - acc: 0.565 - ETA: 3s - loss: 6.9039 - acc: 0.567 - ETA: 3s - loss: 6.9383 - acc: 0.564 - ETA: 2s - loss: 6.8050 - acc: 0.571 - ETA: 2s - loss: 6.7898 - acc: 0.572 - ETA: 2s - loss: 6.8606 - acc: 0.567 - ETA: 2s - loss: 6.8756 - acc: 0.566 - ETA: 2s - loss: 6.8638 - acc: 0.567 - ETA: 2s - loss: 6.8773 - acc: 0.567 - ETA: 2s - loss: 6.7675 - acc: 0.572 - ETA: 2s - loss: 6.7428 - acc: 0.574 - ETA: 2s - loss: 6.7497 - acc: 0.573 - ETA: 2s - loss: 6.8039 - acc: 0.570 - ETA: 2s - loss: 6.8869 - acc: 0.565 - ETA: 2s - loss: 6.8998 - acc: 0.564 - ETA: 2s - loss: 6.9045 - acc: 0.564 - ETA: 2s - loss: 6.9052 - acc: 0.564 - ETA: 2s - loss: 6.9014 - acc: 0.564 - ETA: 2s - loss: 6.8921 - acc: 0.565 - ETA: 2s - loss: 6.8668 - acc: 0.566 - ETA: 1s - loss: 6.9051 - acc: 0.563 - ETA: 1s - loss: 6.8955 - acc: 0.564 - ETA: 1s - loss: 6.9344 - acc: 0.561 - ETA: 1s - loss: 6.9157 - acc: 0.562 - ETA: 1s - loss: 6.8996 - acc: 0.563 - ETA: 1s - loss: 6.8869 - acc: 0.563 - ETA: 1s - loss: 6.8948 - acc: 0.562 - ETA: 1s - loss: 6.9153 - acc: 0.561 - ETA: 1s - loss: 6.9059 - acc: 0.561 - ETA: 1s - loss: 6.8984 - acc: 0.562 - ETA: 1s - loss: 6.8833 - acc: 0.563 - ETA: 1s - loss: 6.8806 - acc: 0.563 - ETA: 1s - loss: 6.8409 - acc: 0.566 - ETA: 1s - loss: 6.8250 - acc: 0.567 - ETA: 1s - loss: 6.8566 - acc: 0.565 - ETA: 1s - loss: 6.8207 - acc: 0.567 - ETA: 1s - loss: 6.7924 - acc: 0.569 - ETA: 1s - loss: 6.7798 - acc: 0.570 - ETA: 0s - loss: 6.7959 - acc: 0.569 - ETA: 0s - loss: 6.8033 - acc: 0.568 - ETA: 0s - loss: 6.8252 - acc: 0.567 - ETA: 0s - loss: 6.7990 - acc: 0.569 - ETA: 0s - loss: 6.8153 - acc: 0.568 - ETA: 0s - loss: 6.8146 - acc: 0.568 - ETA: 0s - loss: 6.8060 - acc: 0.569 - ETA: 0s - loss: 6.7899 - acc: 0.570 - ETA: 0s - loss: 6.8009 - acc: 0.569 - ETA: 0s - loss: 6.8172 - acc: 0.568 - ETA: 0s - loss: 6.7864 - acc: 0.570 - ETA: 0s - loss: 6.7602 - acc: 0.572 - ETA: 0s - loss: 6.7471 - acc: 0.573 - ETA: 0s - loss: 6.7392 - acc: 0.573 - ETA: 0s - loss: 6.7388 - acc: 0.573 - ETA: 0s - loss: 6.7455 - acc: 0.573 - ETA: 0s - loss: 6.7556 - acc: 0.572 - ETA: 0s - loss: 6.7380 - acc: 0.573 - ETA: 0s - loss: 6.7221 - acc: 0.574 - ETA: 0s - loss: 6.7251 - acc: 0.5744Epoch 00033: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 6.7306 - acc: 0.5741 - val_loss: 7.6063 - val_acc: 0.4719\n",
      "Epoch 35/100\n",
      "6640/6680 [============================>.] - ETA: 3s - loss: 5.6413 - acc: 0.650 - ETA: 3s - loss: 6.0526 - acc: 0.600 - ETA: 3s - loss: 6.8548 - acc: 0.562 - ETA: 3s - loss: 6.8772 - acc: 0.564 - ETA: 3s - loss: 6.6816 - acc: 0.578 - ETA: 3s - loss: 6.6124 - acc: 0.584 - ETA: 3s - loss: 6.5948 - acc: 0.585 - ETA: 3s - loss: 6.7576 - acc: 0.575 - ETA: 3s - loss: 6.8574 - acc: 0.569 - ETA: 3s - loss: 6.7694 - acc: 0.575 - ETA: 3s - loss: 6.7518 - acc: 0.574 - ETA: 3s - loss: 6.8384 - acc: 0.568 - ETA: 2s - loss: 6.8821 - acc: 0.566 - ETA: 2s - loss: 6.7541 - acc: 0.574 - ETA: 2s - loss: 6.6256 - acc: 0.582 - ETA: 2s - loss: 6.6590 - acc: 0.580 - ETA: 2s - loss: 6.6155 - acc: 0.583 - ETA: 2s - loss: 6.6590 - acc: 0.581 - ETA: 2s - loss: 6.6550 - acc: 0.581 - ETA: 2s - loss: 6.6884 - acc: 0.579 - ETA: 2s - loss: 6.6863 - acc: 0.579 - ETA: 2s - loss: 6.7193 - acc: 0.576 - ETA: 2s - loss: 6.7050 - acc: 0.577 - ETA: 2s - loss: 6.6679 - acc: 0.579 - ETA: 2s - loss: 6.6315 - acc: 0.582 - ETA: 2s - loss: 6.5989 - acc: 0.584 - ETA: 2s - loss: 6.6244 - acc: 0.582 - ETA: 2s - loss: 6.6239 - acc: 0.582 - ETA: 2s - loss: 6.6640 - acc: 0.580 - ETA: 2s - loss: 6.6651 - acc: 0.580 - ETA: 1s - loss: 6.5994 - acc: 0.584 - ETA: 1s - loss: 6.5855 - acc: 0.584 - ETA: 1s - loss: 6.6049 - acc: 0.583 - ETA: 1s - loss: 6.6225 - acc: 0.582 - ETA: 1s - loss: 6.6223 - acc: 0.582 - ETA: 1s - loss: 6.6179 - acc: 0.582 - ETA: 1s - loss: 6.6156 - acc: 0.582 - ETA: 1s - loss: 6.6029 - acc: 0.583 - ETA: 1s - loss: 6.6119 - acc: 0.581 - ETA: 1s - loss: 6.6324 - acc: 0.580 - ETA: 1s - loss: 6.6177 - acc: 0.581 - ETA: 1s - loss: 6.6175 - acc: 0.580 - ETA: 1s - loss: 6.6268 - acc: 0.580 - ETA: 1s - loss: 6.6268 - acc: 0.580 - ETA: 1s - loss: 6.6301 - acc: 0.580 - ETA: 1s - loss: 6.6288 - acc: 0.580 - ETA: 1s - loss: 6.6734 - acc: 0.577 - ETA: 1s - loss: 6.6485 - acc: 0.579 - ETA: 0s - loss: 6.6512 - acc: 0.579 - ETA: 0s - loss: 6.6477 - acc: 0.579 - ETA: 0s - loss: 6.6373 - acc: 0.580 - ETA: 0s - loss: 6.6487 - acc: 0.580 - ETA: 0s - loss: 6.6450 - acc: 0.580 - ETA: 0s - loss: 6.6390 - acc: 0.580 - ETA: 0s - loss: 6.6319 - acc: 0.581 - ETA: 0s - loss: 6.6576 - acc: 0.580 - ETA: 0s - loss: 6.6472 - acc: 0.580 - ETA: 0s - loss: 6.6297 - acc: 0.581 - ETA: 0s - loss: 6.6381 - acc: 0.581 - ETA: 0s - loss: 6.6348 - acc: 0.581 - ETA: 0s - loss: 6.6589 - acc: 0.579 - ETA: 0s - loss: 6.6599 - acc: 0.579 - ETA: 0s - loss: 6.6726 - acc: 0.578 - ETA: 0s - loss: 6.6678 - acc: 0.578 - ETA: 0s - loss: 6.6621 - acc: 0.578 - ETA: 0s - loss: 6.6586 - acc: 0.579 - ETA: 0s - loss: 6.6555 - acc: 0.5794Epoch 00034: val_loss improved from 7.58822 to 7.54669, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 6.6494 - acc: 0.5798 - val_loss: 7.5467 - val_acc: 0.4731\n",
      "Epoch 36/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6660/6680 [============================>.] - ETA: 3s - loss: 8.0602 - acc: 0.500 - ETA: 3s - loss: 6.6888 - acc: 0.578 - ETA: 3s - loss: 6.7899 - acc: 0.575 - ETA: 3s - loss: 6.7380 - acc: 0.579 - ETA: 3s - loss: 6.6389 - acc: 0.581 - ETA: 3s - loss: 6.7234 - acc: 0.577 - ETA: 3s - loss: 6.7189 - acc: 0.573 - ETA: 3s - loss: 6.6387 - acc: 0.579 - ETA: 3s - loss: 6.7103 - acc: 0.572 - ETA: 3s - loss: 6.5554 - acc: 0.580 - ETA: 2s - loss: 6.5651 - acc: 0.580 - ETA: 2s - loss: 6.6821 - acc: 0.573 - ETA: 2s - loss: 6.6763 - acc: 0.574 - ETA: 2s - loss: 6.7543 - acc: 0.570 - ETA: 2s - loss: 6.7047 - acc: 0.573 - ETA: 2s - loss: 6.6240 - acc: 0.579 - ETA: 2s - loss: 6.6519 - acc: 0.578 - ETA: 2s - loss: 6.6426 - acc: 0.579 - ETA: 2s - loss: 6.6164 - acc: 0.580 - ETA: 2s - loss: 6.6404 - acc: 0.579 - ETA: 2s - loss: 6.7552 - acc: 0.572 - ETA: 2s - loss: 6.7604 - acc: 0.572 - ETA: 2s - loss: 6.7330 - acc: 0.574 - ETA: 2s - loss: 6.7239 - acc: 0.575 - ETA: 2s - loss: 6.6673 - acc: 0.579 - ETA: 2s - loss: 6.6875 - acc: 0.578 - ETA: 2s - loss: 6.6845 - acc: 0.578 - ETA: 2s - loss: 6.7347 - acc: 0.575 - ETA: 2s - loss: 6.7493 - acc: 0.574 - ETA: 1s - loss: 6.7428 - acc: 0.575 - ETA: 1s - loss: 6.7463 - acc: 0.574 - ETA: 1s - loss: 6.7318 - acc: 0.575 - ETA: 1s - loss: 6.7385 - acc: 0.575 - ETA: 1s - loss: 6.7508 - acc: 0.574 - ETA: 1s - loss: 6.7380 - acc: 0.575 - ETA: 1s - loss: 6.7082 - acc: 0.577 - ETA: 1s - loss: 6.6930 - acc: 0.577 - ETA: 1s - loss: 6.6315 - acc: 0.581 - ETA: 1s - loss: 6.6351 - acc: 0.580 - ETA: 1s - loss: 6.6114 - acc: 0.582 - ETA: 1s - loss: 6.6212 - acc: 0.581 - ETA: 1s - loss: 6.6368 - acc: 0.580 - ETA: 1s - loss: 6.6454 - acc: 0.579 - ETA: 1s - loss: 6.6447 - acc: 0.579 - ETA: 1s - loss: 6.6481 - acc: 0.579 - ETA: 1s - loss: 6.6260 - acc: 0.580 - ETA: 1s - loss: 6.6330 - acc: 0.580 - ETA: 1s - loss: 6.6359 - acc: 0.580 - ETA: 0s - loss: 6.6267 - acc: 0.581 - ETA: 0s - loss: 6.5852 - acc: 0.583 - ETA: 0s - loss: 6.5699 - acc: 0.584 - ETA: 0s - loss: 6.5675 - acc: 0.585 - ETA: 0s - loss: 6.6113 - acc: 0.582 - ETA: 0s - loss: 6.5823 - acc: 0.584 - ETA: 0s - loss: 6.5838 - acc: 0.584 - ETA: 0s - loss: 6.5797 - acc: 0.584 - ETA: 0s - loss: 6.5838 - acc: 0.584 - ETA: 0s - loss: 6.5465 - acc: 0.586 - ETA: 0s - loss: 6.5437 - acc: 0.586 - ETA: 0s - loss: 6.5529 - acc: 0.585 - ETA: 0s - loss: 6.5724 - acc: 0.584 - ETA: 0s - loss: 6.5783 - acc: 0.584 - ETA: 0s - loss: 6.5761 - acc: 0.584 - ETA: 0s - loss: 6.5888 - acc: 0.583 - ETA: 0s - loss: 6.6012 - acc: 0.582 - ETA: 0s - loss: 6.5902 - acc: 0.583 - ETA: 0s - loss: 6.5887 - acc: 0.5835Epoch 00035: val_loss improved from 7.54669 to 7.46412, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 6.5810 - acc: 0.5840 - val_loss: 7.4641 - val_acc: 0.4790\n",
      "Epoch 37/100\n",
      "6660/6680 [============================>.] - ETA: 3s - loss: 7.2533 - acc: 0.550 - ETA: 3s - loss: 6.8905 - acc: 0.566 - ETA: 3s - loss: 6.4676 - acc: 0.595 - ETA: 3s - loss: 6.4501 - acc: 0.597 - ETA: 3s - loss: 6.4946 - acc: 0.593 - ETA: 3s - loss: 6.8442 - acc: 0.572 - ETA: 3s - loss: 6.8837 - acc: 0.570 - ETA: 3s - loss: 7.0427 - acc: 0.560 - ETA: 3s - loss: 7.0263 - acc: 0.559 - ETA: 3s - loss: 6.9396 - acc: 0.565 - ETA: 3s - loss: 6.8622 - acc: 0.569 - ETA: 2s - loss: 6.9329 - acc: 0.565 - ETA: 2s - loss: 6.9201 - acc: 0.566 - ETA: 2s - loss: 6.8200 - acc: 0.572 - ETA: 2s - loss: 6.8049 - acc: 0.573 - ETA: 2s - loss: 6.8085 - acc: 0.573 - ETA: 2s - loss: 6.8977 - acc: 0.567 - ETA: 2s - loss: 6.8769 - acc: 0.569 - ETA: 2s - loss: 6.9590 - acc: 0.564 - ETA: 2s - loss: 6.9491 - acc: 0.565 - ETA: 2s - loss: 6.9152 - acc: 0.567 - ETA: 2s - loss: 6.9133 - acc: 0.567 - ETA: 2s - loss: 6.8775 - acc: 0.569 - ETA: 2s - loss: 6.8247 - acc: 0.572 - ETA: 2s - loss: 6.8456 - acc: 0.571 - ETA: 2s - loss: 6.8362 - acc: 0.572 - ETA: 2s - loss: 6.8442 - acc: 0.571 - ETA: 2s - loss: 6.8639 - acc: 0.570 - ETA: 2s - loss: 6.8178 - acc: 0.572 - ETA: 2s - loss: 6.8493 - acc: 0.570 - ETA: 1s - loss: 6.8093 - acc: 0.573 - ETA: 1s - loss: 6.8132 - acc: 0.573 - ETA: 1s - loss: 6.8073 - acc: 0.573 - ETA: 1s - loss: 6.7808 - acc: 0.575 - ETA: 1s - loss: 6.7630 - acc: 0.575 - ETA: 1s - loss: 6.7503 - acc: 0.576 - ETA: 1s - loss: 6.7008 - acc: 0.579 - ETA: 1s - loss: 6.7028 - acc: 0.579 - ETA: 1s - loss: 6.7191 - acc: 0.578 - ETA: 1s - loss: 6.6970 - acc: 0.579 - ETA: 1s - loss: 6.7068 - acc: 0.579 - ETA: 1s - loss: 6.6966 - acc: 0.579 - ETA: 1s - loss: 6.7029 - acc: 0.578 - ETA: 1s - loss: 6.7324 - acc: 0.577 - ETA: 1s - loss: 6.7116 - acc: 0.578 - ETA: 1s - loss: 6.7082 - acc: 0.578 - ETA: 1s - loss: 6.6968 - acc: 0.579 - ETA: 1s - loss: 6.6712 - acc: 0.581 - ETA: 0s - loss: 6.6558 - acc: 0.582 - ETA: 0s - loss: 6.6518 - acc: 0.582 - ETA: 0s - loss: 6.6320 - acc: 0.583 - ETA: 0s - loss: 6.6106 - acc: 0.585 - ETA: 0s - loss: 6.6008 - acc: 0.585 - ETA: 0s - loss: 6.5833 - acc: 0.586 - ETA: 0s - loss: 6.6047 - acc: 0.585 - ETA: 0s - loss: 6.6093 - acc: 0.584 - ETA: 0s - loss: 6.6222 - acc: 0.583 - ETA: 0s - loss: 6.6140 - acc: 0.584 - ETA: 0s - loss: 6.6247 - acc: 0.583 - ETA: 0s - loss: 6.5918 - acc: 0.585 - ETA: 0s - loss: 6.6118 - acc: 0.584 - ETA: 0s - loss: 6.5955 - acc: 0.585 - ETA: 0s - loss: 6.5889 - acc: 0.585 - ETA: 0s - loss: 6.5715 - acc: 0.586 - ETA: 0s - loss: 6.5501 - acc: 0.587 - ETA: 0s - loss: 6.5432 - acc: 0.5880Epoch 00036: val_loss improved from 7.46412 to 7.39326, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 6.5430 - acc: 0.5880 - val_loss: 7.3933 - val_acc: 0.4898\n",
      "Epoch 38/100\n",
      "6660/6680 [============================>.] - ETA: 3s - loss: 8.0590 - acc: 0.500 - ETA: 3s - loss: 6.5796 - acc: 0.591 - ETA: 3s - loss: 6.8873 - acc: 0.568 - ETA: 3s - loss: 6.6659 - acc: 0.581 - ETA: 3s - loss: 6.8928 - acc: 0.561 - ETA: 3s - loss: 6.8471 - acc: 0.565 - ETA: 3s - loss: 6.8473 - acc: 0.566 - ETA: 3s - loss: 6.6328 - acc: 0.575 - ETA: 3s - loss: 6.8328 - acc: 0.564 - ETA: 2s - loss: 6.7871 - acc: 0.566 - ETA: 2s - loss: 6.7862 - acc: 0.567 - ETA: 2s - loss: 6.6535 - acc: 0.575 - ETA: 2s - loss: 6.6400 - acc: 0.577 - ETA: 2s - loss: 6.6257 - acc: 0.578 - ETA: 2s - loss: 6.6994 - acc: 0.574 - ETA: 2s - loss: 6.7309 - acc: 0.572 - ETA: 2s - loss: 6.7641 - acc: 0.569 - ETA: 2s - loss: 6.7051 - acc: 0.573 - ETA: 2s - loss: 6.7297 - acc: 0.571 - ETA: 2s - loss: 6.7335 - acc: 0.571 - ETA: 2s - loss: 6.6819 - acc: 0.574 - ETA: 2s - loss: 6.6451 - acc: 0.576 - ETA: 2s - loss: 6.6361 - acc: 0.577 - ETA: 2s - loss: 6.6138 - acc: 0.579 - ETA: 2s - loss: 6.6074 - acc: 0.579 - ETA: 2s - loss: 6.5732 - acc: 0.581 - ETA: 2s - loss: 6.5198 - acc: 0.584 - ETA: 2s - loss: 6.5323 - acc: 0.583 - ETA: 1s - loss: 6.5130 - acc: 0.583 - ETA: 1s - loss: 6.5063 - acc: 0.584 - ETA: 1s - loss: 6.5228 - acc: 0.582 - ETA: 1s - loss: 6.4993 - acc: 0.583 - ETA: 1s - loss: 6.4537 - acc: 0.586 - ETA: 1s - loss: 6.4285 - acc: 0.587 - ETA: 1s - loss: 6.3835 - acc: 0.590 - ETA: 1s - loss: 6.3375 - acc: 0.592 - ETA: 1s - loss: 6.3546 - acc: 0.591 - ETA: 1s - loss: 6.3367 - acc: 0.592 - ETA: 1s - loss: 6.3451 - acc: 0.591 - ETA: 1s - loss: 6.3292 - acc: 0.592 - ETA: 1s - loss: 6.3627 - acc: 0.590 - ETA: 1s - loss: 6.3710 - acc: 0.590 - ETA: 1s - loss: 6.3761 - acc: 0.590 - ETA: 1s - loss: 6.3958 - acc: 0.589 - ETA: 1s - loss: 6.3805 - acc: 0.590 - ETA: 0s - loss: 6.3918 - acc: 0.589 - ETA: 0s - loss: 6.3572 - acc: 0.591 - ETA: 0s - loss: 6.3509 - acc: 0.592 - ETA: 0s - loss: 6.3389 - acc: 0.592 - ETA: 0s - loss: 6.3317 - acc: 0.593 - ETA: 0s - loss: 6.3524 - acc: 0.592 - ETA: 0s - loss: 6.3516 - acc: 0.592 - ETA: 0s - loss: 6.3376 - acc: 0.592 - ETA: 0s - loss: 6.3630 - acc: 0.591 - ETA: 0s - loss: 6.3598 - acc: 0.591 - ETA: 0s - loss: 6.3724 - acc: 0.590 - ETA: 0s - loss: 6.4063 - acc: 0.588 - ETA: 0s - loss: 6.4367 - acc: 0.586 - ETA: 0s - loss: 6.4113 - acc: 0.588 - ETA: 0s - loss: 6.4197 - acc: 0.588 - ETA: 0s - loss: 6.3962 - acc: 0.589 - ETA: 0s - loss: 6.3917 - acc: 0.589 - ETA: 0s - loss: 6.4049 - acc: 0.589 - ETA: 0s - loss: 6.4087 - acc: 0.5887Epoch 00037: val_loss improved from 7.39326 to 7.33109, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 6.4064 - acc: 0.5889 - val_loss: 7.3311 - val_acc: 0.4790\n",
      "Epoch 39/100\n",
      "6600/6680 [============================>.] - ETA: 3s - loss: 7.2531 - acc: 0.550 - ETA: 3s - loss: 6.1323 - acc: 0.614 - ETA: 3s - loss: 6.7745 - acc: 0.576 - ETA: 3s - loss: 6.6889 - acc: 0.581 - ETA: 3s - loss: 6.4726 - acc: 0.595 - ETA: 3s - loss: 6.3288 - acc: 0.605 - ETA: 2s - loss: 6.3291 - acc: 0.602 - ETA: 2s - loss: 6.2875 - acc: 0.603 - ETA: 2s - loss: 6.2709 - acc: 0.604 - ETA: 2s - loss: 6.2065 - acc: 0.607 - ETA: 2s - loss: 6.1531 - acc: 0.609 - ETA: 2s - loss: 6.1649 - acc: 0.608 - ETA: 2s - loss: 6.0900 - acc: 0.614 - ETA: 2s - loss: 6.0643 - acc: 0.616 - ETA: 2s - loss: 6.1367 - acc: 0.612 - ETA: 2s - loss: 6.1361 - acc: 0.612 - ETA: 2s - loss: 6.0985 - acc: 0.615 - ETA: 2s - loss: 6.1311 - acc: 0.613 - ETA: 2s - loss: 6.2076 - acc: 0.608 - ETA: 2s - loss: 6.1960 - acc: 0.609 - ETA: 2s - loss: 6.1782 - acc: 0.611 - ETA: 2s - loss: 6.2194 - acc: 0.608 - ETA: 2s - loss: 6.2230 - acc: 0.608 - ETA: 2s - loss: 6.2309 - acc: 0.607 - ETA: 2s - loss: 6.1897 - acc: 0.610 - ETA: 2s - loss: 6.2489 - acc: 0.606 - ETA: 1s - loss: 6.2488 - acc: 0.606 - ETA: 1s - loss: 6.2614 - acc: 0.605 - ETA: 1s - loss: 6.2414 - acc: 0.606 - ETA: 1s - loss: 6.2746 - acc: 0.604 - ETA: 1s - loss: 6.2838 - acc: 0.604 - ETA: 1s - loss: 6.3140 - acc: 0.602 - ETA: 1s - loss: 6.2846 - acc: 0.604 - ETA: 1s - loss: 6.3006 - acc: 0.603 - ETA: 1s - loss: 6.3047 - acc: 0.603 - ETA: 1s - loss: 6.3042 - acc: 0.603 - ETA: 1s - loss: 6.2870 - acc: 0.604 - ETA: 1s - loss: 6.2896 - acc: 0.603 - ETA: 1s - loss: 6.2724 - acc: 0.604 - ETA: 1s - loss: 6.2806 - acc: 0.604 - ETA: 1s - loss: 6.2934 - acc: 0.603 - ETA: 1s - loss: 6.2958 - acc: 0.603 - ETA: 1s - loss: 6.2957 - acc: 0.603 - ETA: 1s - loss: 6.2852 - acc: 0.603 - ETA: 1s - loss: 6.3071 - acc: 0.602 - ETA: 0s - loss: 6.3150 - acc: 0.601 - ETA: 0s - loss: 6.3165 - acc: 0.601 - ETA: 0s - loss: 6.3124 - acc: 0.601 - ETA: 0s - loss: 6.2749 - acc: 0.603 - ETA: 0s - loss: 6.2844 - acc: 0.603 - ETA: 0s - loss: 6.2968 - acc: 0.602 - ETA: 0s - loss: 6.2894 - acc: 0.602 - ETA: 0s - loss: 6.2748 - acc: 0.604 - ETA: 0s - loss: 6.2947 - acc: 0.602 - ETA: 0s - loss: 6.2951 - acc: 0.602 - ETA: 0s - loss: 6.3142 - acc: 0.601 - ETA: 0s - loss: 6.3186 - acc: 0.601 - ETA: 0s - loss: 6.3164 - acc: 0.601 - ETA: 0s - loss: 6.3278 - acc: 0.600 - ETA: 0s - loss: 6.3425 - acc: 0.599 - ETA: 0s - loss: 6.3252 - acc: 0.600 - ETA: 0s - loss: 6.3151 - acc: 0.601 - ETA: 0s - loss: 6.3014 - acc: 0.6024Epoch 00038: val_loss improved from 7.33109 to 7.24893, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 6.2955 - acc: 0.6027 - val_loss: 7.2489 - val_acc: 0.4790\n",
      "Epoch 40/100\n",
      "6580/6680 [============================>.] - ETA: 2s - loss: 7.2531 - acc: 0.550 - ETA: 3s - loss: 6.2696 - acc: 0.600 - ETA: 3s - loss: 5.8378 - acc: 0.631 - ETA: 3s - loss: 5.9284 - acc: 0.628 - ETA: 3s - loss: 5.9997 - acc: 0.621 - ETA: 3s - loss: 6.1888 - acc: 0.611 - ETA: 3s - loss: 6.3791 - acc: 0.600 - ETA: 3s - loss: 6.2794 - acc: 0.606 - ETA: 3s - loss: 6.1933 - acc: 0.611 - ETA: 2s - loss: 6.4354 - acc: 0.596 - ETA: 2s - loss: 6.3454 - acc: 0.602 - ETA: 2s - loss: 6.3546 - acc: 0.602 - ETA: 2s - loss: 6.2484 - acc: 0.608 - ETA: 2s - loss: 6.2277 - acc: 0.610 - ETA: 2s - loss: 6.2194 - acc: 0.611 - ETA: 2s - loss: 6.1789 - acc: 0.613 - ETA: 2s - loss: 6.0902 - acc: 0.618 - ETA: 2s - loss: 6.1277 - acc: 0.615 - ETA: 2s - loss: 6.1537 - acc: 0.614 - ETA: 2s - loss: 6.1211 - acc: 0.616 - ETA: 2s - loss: 6.1419 - acc: 0.615 - ETA: 2s - loss: 6.0959 - acc: 0.618 - ETA: 2s - loss: 6.1253 - acc: 0.615 - ETA: 2s - loss: 6.1460 - acc: 0.614 - ETA: 2s - loss: 6.2079 - acc: 0.610 - ETA: 2s - loss: 6.1741 - acc: 0.612 - ETA: 2s - loss: 6.1906 - acc: 0.611 - ETA: 2s - loss: 6.1181 - acc: 0.616 - ETA: 2s - loss: 6.1436 - acc: 0.614 - ETA: 1s - loss: 6.1126 - acc: 0.616 - ETA: 1s - loss: 6.1549 - acc: 0.614 - ETA: 1s - loss: 6.1756 - acc: 0.613 - ETA: 1s - loss: 6.1697 - acc: 0.613 - ETA: 1s - loss: 6.1931 - acc: 0.611 - ETA: 1s - loss: 6.1866 - acc: 0.612 - ETA: 1s - loss: 6.1771 - acc: 0.612 - ETA: 1s - loss: 6.1583 - acc: 0.614 - ETA: 1s - loss: 6.1491 - acc: 0.614 - ETA: 1s - loss: 6.1451 - acc: 0.614 - ETA: 1s - loss: 6.1502 - acc: 0.614 - ETA: 1s - loss: 6.1358 - acc: 0.615 - ETA: 1s - loss: 6.1355 - acc: 0.615 - ETA: 1s - loss: 6.1406 - acc: 0.614 - ETA: 1s - loss: 6.1619 - acc: 0.613 - ETA: 1s - loss: 6.1541 - acc: 0.614 - ETA: 1s - loss: 6.1548 - acc: 0.613 - ETA: 1s - loss: 6.1751 - acc: 0.612 - ETA: 0s - loss: 6.1924 - acc: 0.611 - ETA: 0s - loss: 6.2356 - acc: 0.608 - ETA: 0s - loss: 6.2399 - acc: 0.608 - ETA: 0s - loss: 6.2445 - acc: 0.608 - ETA: 0s - loss: 6.2514 - acc: 0.607 - ETA: 0s - loss: 6.2468 - acc: 0.608 - ETA: 0s - loss: 6.2475 - acc: 0.608 - ETA: 0s - loss: 6.2395 - acc: 0.608 - ETA: 0s - loss: 6.2574 - acc: 0.607 - ETA: 0s - loss: 6.2598 - acc: 0.607 - ETA: 0s - loss: 6.2612 - acc: 0.607 - ETA: 0s - loss: 6.2483 - acc: 0.608 - ETA: 0s - loss: 6.2569 - acc: 0.607 - ETA: 0s - loss: 6.2654 - acc: 0.607 - ETA: 0s - loss: 6.2591 - acc: 0.607 - ETA: 0s - loss: 6.2722 - acc: 0.606 - ETA: 0s - loss: 6.2774 - acc: 0.606 - ETA: 0s - loss: 6.2727 - acc: 0.6067Epoch 00039: val_loss improved from 7.24893 to 7.23464, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 6.2681 - acc: 0.6070 - val_loss: 7.2346 - val_acc: 0.4946\n",
      "Epoch 41/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6640/6680 [============================>.] - ETA: 5s - loss: 4.8457 - acc: 0.700 - ETA: 3s - loss: 5.6429 - acc: 0.650 - ETA: 3s - loss: 5.6196 - acc: 0.650 - ETA: 3s - loss: 5.6754 - acc: 0.646 - ETA: 3s - loss: 5.5138 - acc: 0.657 - ETA: 3s - loss: 5.4568 - acc: 0.659 - ETA: 3s - loss: 5.3347 - acc: 0.667 - ETA: 3s - loss: 5.4632 - acc: 0.659 - ETA: 3s - loss: 5.5256 - acc: 0.655 - ETA: 3s - loss: 5.6385 - acc: 0.649 - ETA: 2s - loss: 5.7350 - acc: 0.642 - ETA: 2s - loss: 5.8451 - acc: 0.635 - ETA: 2s - loss: 5.9005 - acc: 0.631 - ETA: 2s - loss: 5.9869 - acc: 0.626 - ETA: 2s - loss: 6.0072 - acc: 0.625 - ETA: 2s - loss: 6.0001 - acc: 0.625 - ETA: 2s - loss: 6.0131 - acc: 0.623 - ETA: 2s - loss: 6.0096 - acc: 0.624 - ETA: 2s - loss: 6.0685 - acc: 0.620 - ETA: 2s - loss: 6.0518 - acc: 0.621 - ETA: 2s - loss: 6.0596 - acc: 0.621 - ETA: 2s - loss: 6.1159 - acc: 0.617 - ETA: 2s - loss: 6.1236 - acc: 0.617 - ETA: 2s - loss: 6.1390 - acc: 0.616 - ETA: 2s - loss: 6.1245 - acc: 0.617 - ETA: 2s - loss: 6.1322 - acc: 0.616 - ETA: 2s - loss: 6.1652 - acc: 0.614 - ETA: 2s - loss: 6.1403 - acc: 0.615 - ETA: 2s - loss: 6.1738 - acc: 0.613 - ETA: 2s - loss: 6.1625 - acc: 0.613 - ETA: 1s - loss: 6.1192 - acc: 0.616 - ETA: 1s - loss: 6.1300 - acc: 0.615 - ETA: 1s - loss: 6.0879 - acc: 0.618 - ETA: 1s - loss: 6.1428 - acc: 0.615 - ETA: 1s - loss: 6.1340 - acc: 0.615 - ETA: 1s - loss: 6.1665 - acc: 0.613 - ETA: 1s - loss: 6.1836 - acc: 0.612 - ETA: 1s - loss: 6.2089 - acc: 0.610 - ETA: 1s - loss: 6.2158 - acc: 0.610 - ETA: 1s - loss: 6.1982 - acc: 0.611 - ETA: 1s - loss: 6.2015 - acc: 0.610 - ETA: 1s - loss: 6.2015 - acc: 0.610 - ETA: 1s - loss: 6.1926 - acc: 0.611 - ETA: 1s - loss: 6.1941 - acc: 0.611 - ETA: 1s - loss: 6.1907 - acc: 0.610 - ETA: 1s - loss: 6.1788 - acc: 0.611 - ETA: 1s - loss: 6.2196 - acc: 0.609 - ETA: 1s - loss: 6.2311 - acc: 0.608 - ETA: 0s - loss: 6.2124 - acc: 0.609 - ETA: 0s - loss: 6.2205 - acc: 0.609 - ETA: 0s - loss: 6.2320 - acc: 0.608 - ETA: 0s - loss: 6.2268 - acc: 0.608 - ETA: 0s - loss: 6.2007 - acc: 0.610 - ETA: 0s - loss: 6.2326 - acc: 0.608 - ETA: 0s - loss: 6.2229 - acc: 0.608 - ETA: 0s - loss: 6.2654 - acc: 0.606 - ETA: 0s - loss: 6.2722 - acc: 0.605 - ETA: 0s - loss: 6.2819 - acc: 0.604 - ETA: 0s - loss: 6.2664 - acc: 0.605 - ETA: 0s - loss: 6.2398 - acc: 0.607 - ETA: 0s - loss: 6.2617 - acc: 0.606 - ETA: 0s - loss: 6.2666 - acc: 0.605 - ETA: 0s - loss: 6.2388 - acc: 0.607 - ETA: 0s - loss: 6.2349 - acc: 0.607 - ETA: 0s - loss: 6.2392 - acc: 0.607 - ETA: 0s - loss: 6.2262 - acc: 0.6083Epoch 00040: val_loss improved from 7.23464 to 7.17388, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 6.2275 - acc: 0.6082 - val_loss: 7.1739 - val_acc: 0.4922\n",
      "Epoch 42/100\n",
      "6660/6680 [============================>.] - ETA: 3s - loss: 5.0169 - acc: 0.650 - ETA: 3s - loss: 5.7776 - acc: 0.616 - ETA: 3s - loss: 6.4485 - acc: 0.587 - ETA: 3s - loss: 6.2792 - acc: 0.597 - ETA: 3s - loss: 6.2487 - acc: 0.600 - ETA: 3s - loss: 6.3732 - acc: 0.594 - ETA: 3s - loss: 6.1759 - acc: 0.608 - ETA: 3s - loss: 6.2699 - acc: 0.602 - ETA: 3s - loss: 6.1303 - acc: 0.612 - ETA: 3s - loss: 6.1784 - acc: 0.607 - ETA: 3s - loss: 6.1761 - acc: 0.607 - ETA: 2s - loss: 6.1798 - acc: 0.607 - ETA: 2s - loss: 6.2683 - acc: 0.603 - ETA: 2s - loss: 6.3397 - acc: 0.599 - ETA: 2s - loss: 6.4123 - acc: 0.595 - ETA: 2s - loss: 6.4251 - acc: 0.595 - ETA: 2s - loss: 6.3790 - acc: 0.598 - ETA: 2s - loss: 6.3211 - acc: 0.601 - ETA: 2s - loss: 6.2844 - acc: 0.603 - ETA: 2s - loss: 6.2649 - acc: 0.604 - ETA: 2s - loss: 6.2809 - acc: 0.604 - ETA: 2s - loss: 6.2722 - acc: 0.604 - ETA: 2s - loss: 6.2525 - acc: 0.606 - ETA: 2s - loss: 6.2453 - acc: 0.606 - ETA: 2s - loss: 6.1795 - acc: 0.610 - ETA: 2s - loss: 6.1290 - acc: 0.613 - ETA: 2s - loss: 6.1373 - acc: 0.613 - ETA: 1s - loss: 6.1695 - acc: 0.610 - ETA: 1s - loss: 6.1354 - acc: 0.613 - ETA: 1s - loss: 6.1685 - acc: 0.611 - ETA: 1s - loss: 6.1773 - acc: 0.611 - ETA: 1s - loss: 6.1920 - acc: 0.610 - ETA: 1s - loss: 6.1774 - acc: 0.611 - ETA: 1s - loss: 6.1622 - acc: 0.612 - ETA: 1s - loss: 6.1576 - acc: 0.612 - ETA: 1s - loss: 6.1741 - acc: 0.611 - ETA: 1s - loss: 6.1872 - acc: 0.611 - ETA: 1s - loss: 6.1748 - acc: 0.612 - ETA: 1s - loss: 6.2118 - acc: 0.609 - ETA: 1s - loss: 6.2098 - acc: 0.609 - ETA: 1s - loss: 6.1977 - acc: 0.610 - ETA: 1s - loss: 6.1704 - acc: 0.612 - ETA: 1s - loss: 6.1778 - acc: 0.611 - ETA: 1s - loss: 6.1711 - acc: 0.612 - ETA: 1s - loss: 6.1885 - acc: 0.611 - ETA: 0s - loss: 6.1946 - acc: 0.610 - ETA: 0s - loss: 6.2031 - acc: 0.610 - ETA: 0s - loss: 6.2076 - acc: 0.609 - ETA: 0s - loss: 6.1975 - acc: 0.610 - ETA: 0s - loss: 6.1747 - acc: 0.611 - ETA: 0s - loss: 6.1685 - acc: 0.612 - ETA: 0s - loss: 6.1718 - acc: 0.612 - ETA: 0s - loss: 6.1613 - acc: 0.612 - ETA: 0s - loss: 6.1645 - acc: 0.612 - ETA: 0s - loss: 6.1712 - acc: 0.612 - ETA: 0s - loss: 6.1677 - acc: 0.612 - ETA: 0s - loss: 6.1782 - acc: 0.611 - ETA: 0s - loss: 6.1559 - acc: 0.612 - ETA: 0s - loss: 6.1511 - acc: 0.612 - ETA: 0s - loss: 6.1586 - acc: 0.612 - ETA: 0s - loss: 6.1562 - acc: 0.612 - ETA: 0s - loss: 6.1682 - acc: 0.611 - ETA: 0s - loss: 6.1706 - acc: 0.611 - ETA: 0s - loss: 6.1681 - acc: 0.6117Epoch 00041: val_loss improved from 7.17388 to 7.07804, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 6.1665 - acc: 0.6118 - val_loss: 7.0780 - val_acc: 0.4994\n",
      "Epoch 43/100\n",
      "6660/6680 [============================>.] - ETA: 3s - loss: 4.0295 - acc: 0.750 - ETA: 3s - loss: 6.7494 - acc: 0.566 - ETA: 3s - loss: 6.9509 - acc: 0.560 - ETA: 3s - loss: 6.4607 - acc: 0.593 - ETA: 3s - loss: 6.2962 - acc: 0.605 - ETA: 3s - loss: 6.5264 - acc: 0.590 - ETA: 3s - loss: 6.3474 - acc: 0.600 - ETA: 3s - loss: 6.2046 - acc: 0.609 - ETA: 3s - loss: 6.0863 - acc: 0.617 - ETA: 3s - loss: 5.9707 - acc: 0.625 - ETA: 3s - loss: 5.9132 - acc: 0.629 - ETA: 2s - loss: 6.0238 - acc: 0.621 - ETA: 2s - loss: 6.0450 - acc: 0.620 - ETA: 2s - loss: 6.0750 - acc: 0.618 - ETA: 2s - loss: 6.1234 - acc: 0.616 - ETA: 2s - loss: 6.1379 - acc: 0.615 - ETA: 2s - loss: 6.1984 - acc: 0.611 - ETA: 2s - loss: 6.1790 - acc: 0.612 - ETA: 2s - loss: 6.0479 - acc: 0.621 - ETA: 2s - loss: 6.0600 - acc: 0.620 - ETA: 2s - loss: 6.0638 - acc: 0.619 - ETA: 2s - loss: 6.0543 - acc: 0.619 - ETA: 2s - loss: 6.0510 - acc: 0.619 - ETA: 2s - loss: 6.0884 - acc: 0.616 - ETA: 2s - loss: 6.0733 - acc: 0.618 - ETA: 2s - loss: 6.0632 - acc: 0.618 - ETA: 2s - loss: 6.0518 - acc: 0.619 - ETA: 2s - loss: 6.0749 - acc: 0.618 - ETA: 2s - loss: 6.0267 - acc: 0.621 - ETA: 1s - loss: 6.0409 - acc: 0.620 - ETA: 1s - loss: 6.0594 - acc: 0.619 - ETA: 1s - loss: 6.0536 - acc: 0.619 - ETA: 1s - loss: 6.0607 - acc: 0.619 - ETA: 1s - loss: 6.0622 - acc: 0.619 - ETA: 1s - loss: 6.0708 - acc: 0.619 - ETA: 1s - loss: 6.0660 - acc: 0.619 - ETA: 1s - loss: 6.0503 - acc: 0.620 - ETA: 1s - loss: 6.0608 - acc: 0.619 - ETA: 1s - loss: 6.0772 - acc: 0.618 - ETA: 1s - loss: 6.0838 - acc: 0.617 - ETA: 1s - loss: 6.0614 - acc: 0.619 - ETA: 1s - loss: 6.0839 - acc: 0.617 - ETA: 1s - loss: 6.0812 - acc: 0.618 - ETA: 1s - loss: 6.0647 - acc: 0.619 - ETA: 1s - loss: 6.0386 - acc: 0.620 - ETA: 1s - loss: 6.0260 - acc: 0.621 - ETA: 1s - loss: 6.0213 - acc: 0.621 - ETA: 0s - loss: 6.0633 - acc: 0.619 - ETA: 0s - loss: 6.0767 - acc: 0.618 - ETA: 0s - loss: 6.0935 - acc: 0.617 - ETA: 0s - loss: 6.0848 - acc: 0.618 - ETA: 0s - loss: 6.1004 - acc: 0.617 - ETA: 0s - loss: 6.0890 - acc: 0.617 - ETA: 0s - loss: 6.0720 - acc: 0.618 - ETA: 0s - loss: 6.0893 - acc: 0.617 - ETA: 0s - loss: 6.1052 - acc: 0.616 - ETA: 0s - loss: 6.0974 - acc: 0.617 - ETA: 0s - loss: 6.1060 - acc: 0.616 - ETA: 0s - loss: 6.1155 - acc: 0.616 - ETA: 0s - loss: 6.1341 - acc: 0.615 - ETA: 0s - loss: 6.1286 - acc: 0.615 - ETA: 0s - loss: 6.1523 - acc: 0.614 - ETA: 0s - loss: 6.1669 - acc: 0.613 - ETA: 0s - loss: 6.1619 - acc: 0.613 - ETA: 0s - loss: 6.1422 - acc: 0.6149Epoch 00042: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 6.1407 - acc: 0.6150 - val_loss: 7.1703 - val_acc: 0.4934\n",
      "Epoch 44/100\n",
      "6640/6680 [============================>.] - ETA: 3s - loss: 8.0590 - acc: 0.500 - ETA: 3s - loss: 6.6148 - acc: 0.575 - ETA: 3s - loss: 6.1013 - acc: 0.613 - ETA: 3s - loss: 6.2514 - acc: 0.603 - ETA: 3s - loss: 6.6228 - acc: 0.582 - ETA: 3s - loss: 6.4896 - acc: 0.592 - ETA: 3s - loss: 6.4567 - acc: 0.595 - ETA: 3s - loss: 6.5257 - acc: 0.590 - ETA: 3s - loss: 6.3995 - acc: 0.598 - ETA: 2s - loss: 6.4398 - acc: 0.596 - ETA: 2s - loss: 6.3951 - acc: 0.600 - ETA: 2s - loss: 6.3731 - acc: 0.601 - ETA: 2s - loss: 6.3537 - acc: 0.603 - ETA: 2s - loss: 6.3000 - acc: 0.606 - ETA: 2s - loss: 6.2555 - acc: 0.608 - ETA: 2s - loss: 6.2678 - acc: 0.608 - ETA: 2s - loss: 6.2136 - acc: 0.611 - ETA: 2s - loss: 6.1128 - acc: 0.618 - ETA: 2s - loss: 6.1007 - acc: 0.618 - ETA: 2s - loss: 6.1384 - acc: 0.616 - ETA: 2s - loss: 6.1500 - acc: 0.615 - ETA: 2s - loss: 6.1782 - acc: 0.613 - ETA: 2s - loss: 6.1268 - acc: 0.617 - ETA: 2s - loss: 6.1201 - acc: 0.617 - ETA: 2s - loss: 6.1296 - acc: 0.617 - ETA: 2s - loss: 6.1232 - acc: 0.617 - ETA: 2s - loss: 6.1374 - acc: 0.616 - ETA: 1s - loss: 6.1200 - acc: 0.618 - ETA: 1s - loss: 6.1279 - acc: 0.617 - ETA: 1s - loss: 6.0856 - acc: 0.620 - ETA: 1s - loss: 6.1379 - acc: 0.617 - ETA: 1s - loss: 6.1375 - acc: 0.617 - ETA: 1s - loss: 6.1605 - acc: 0.615 - ETA: 1s - loss: 6.1438 - acc: 0.616 - ETA: 1s - loss: 6.1387 - acc: 0.617 - ETA: 1s - loss: 6.1826 - acc: 0.614 - ETA: 1s - loss: 6.1466 - acc: 0.616 - ETA: 1s - loss: 6.1504 - acc: 0.616 - ETA: 1s - loss: 6.1334 - acc: 0.617 - ETA: 1s - loss: 6.1486 - acc: 0.616 - ETA: 1s - loss: 6.1562 - acc: 0.616 - ETA: 1s - loss: 6.1782 - acc: 0.614 - ETA: 1s - loss: 6.1640 - acc: 0.615 - ETA: 1s - loss: 6.1417 - acc: 0.617 - ETA: 1s - loss: 6.1416 - acc: 0.616 - ETA: 1s - loss: 6.1325 - acc: 0.617 - ETA: 0s - loss: 6.1203 - acc: 0.618 - ETA: 0s - loss: 6.1289 - acc: 0.617 - ETA: 0s - loss: 6.0968 - acc: 0.619 - ETA: 0s - loss: 6.1068 - acc: 0.619 - ETA: 0s - loss: 6.1047 - acc: 0.619 - ETA: 0s - loss: 6.0949 - acc: 0.619 - ETA: 0s - loss: 6.0882 - acc: 0.620 - ETA: 0s - loss: 6.0785 - acc: 0.620 - ETA: 0s - loss: 6.0966 - acc: 0.619 - ETA: 0s - loss: 6.1178 - acc: 0.618 - ETA: 0s - loss: 6.1235 - acc: 0.617 - ETA: 0s - loss: 6.1071 - acc: 0.618 - ETA: 0s - loss: 6.1182 - acc: 0.618 - ETA: 0s - loss: 6.0971 - acc: 0.619 - ETA: 0s - loss: 6.1134 - acc: 0.618 - ETA: 0s - loss: 6.1122 - acc: 0.618 - ETA: 0s - loss: 6.1229 - acc: 0.617 - ETA: 0s - loss: 6.1279 - acc: 0.617 - ETA: 0s - loss: 6.1304 - acc: 0.6172Epoch 00043: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 6.1275 - acc: 0.6174 - val_loss: 7.1669 - val_acc: 0.5090\n",
      "Epoch 45/100\n",
      "6600/6680 [============================>.] - ETA: 2s - loss: 5.6416 - acc: 0.650 - ETA: 3s - loss: 5.7760 - acc: 0.641 - ETA: 3s - loss: 6.9603 - acc: 0.568 - ETA: 3s - loss: 6.4000 - acc: 0.602 - ETA: 3s - loss: 6.2642 - acc: 0.611 - ETA: 3s - loss: 6.0153 - acc: 0.625 - ETA: 3s - loss: 5.9473 - acc: 0.630 - ETA: 2s - loss: 5.9325 - acc: 0.630 - ETA: 2s - loss: 5.9361 - acc: 0.630 - ETA: 2s - loss: 5.8718 - acc: 0.634 - ETA: 2s - loss: 5.8071 - acc: 0.638 - ETA: 2s - loss: 5.8068 - acc: 0.638 - ETA: 2s - loss: 5.8569 - acc: 0.635 - ETA: 2s - loss: 5.9220 - acc: 0.631 - ETA: 2s - loss: 5.9902 - acc: 0.627 - ETA: 2s - loss: 5.9340 - acc: 0.630 - ETA: 2s - loss: 5.9549 - acc: 0.629 - ETA: 2s - loss: 5.9554 - acc: 0.629 - ETA: 2s - loss: 5.9959 - acc: 0.626 - ETA: 2s - loss: 6.0104 - acc: 0.626 - ETA: 2s - loss: 5.9853 - acc: 0.627 - ETA: 2s - loss: 5.9606 - acc: 0.628 - ETA: 2s - loss: 5.9848 - acc: 0.627 - ETA: 2s - loss: 5.9774 - acc: 0.627 - ETA: 2s - loss: 6.0461 - acc: 0.622 - ETA: 2s - loss: 6.0306 - acc: 0.623 - ETA: 2s - loss: 6.0491 - acc: 0.622 - ETA: 1s - loss: 6.0981 - acc: 0.619 - ETA: 1s - loss: 6.1014 - acc: 0.619 - ETA: 1s - loss: 6.0703 - acc: 0.621 - ETA: 1s - loss: 6.0926 - acc: 0.619 - ETA: 1s - loss: 6.0810 - acc: 0.620 - ETA: 1s - loss: 6.0967 - acc: 0.619 - ETA: 1s - loss: 6.1208 - acc: 0.618 - ETA: 1s - loss: 6.1002 - acc: 0.619 - ETA: 1s - loss: 6.0790 - acc: 0.620 - ETA: 1s - loss: 6.1101 - acc: 0.619 - ETA: 1s - loss: 6.1045 - acc: 0.619 - ETA: 1s - loss: 6.1295 - acc: 0.617 - ETA: 1s - loss: 6.1214 - acc: 0.618 - ETA: 1s - loss: 6.1277 - acc: 0.618 - ETA: 1s - loss: 6.1263 - acc: 0.617 - ETA: 1s - loss: 6.1278 - acc: 0.617 - ETA: 1s - loss: 6.1241 - acc: 0.618 - ETA: 1s - loss: 6.1303 - acc: 0.617 - ETA: 1s - loss: 6.1383 - acc: 0.616 - ETA: 0s - loss: 6.1392 - acc: 0.616 - ETA: 0s - loss: 6.1140 - acc: 0.618 - ETA: 0s - loss: 6.1315 - acc: 0.617 - ETA: 0s - loss: 6.1220 - acc: 0.617 - ETA: 0s - loss: 6.1252 - acc: 0.617 - ETA: 0s - loss: 6.1234 - acc: 0.617 - ETA: 0s - loss: 6.1058 - acc: 0.619 - ETA: 0s - loss: 6.0950 - acc: 0.619 - ETA: 0s - loss: 6.0983 - acc: 0.619 - ETA: 0s - loss: 6.0939 - acc: 0.619 - ETA: 0s - loss: 6.0965 - acc: 0.619 - ETA: 0s - loss: 6.1187 - acc: 0.617 - ETA: 0s - loss: 6.1411 - acc: 0.616 - ETA: 0s - loss: 6.1408 - acc: 0.616 - ETA: 0s - loss: 6.1359 - acc: 0.616 - ETA: 0s - loss: 6.1141 - acc: 0.618 - ETA: 0s - loss: 6.1267 - acc: 0.617 - ETA: 0s - loss: 6.1218 - acc: 0.6177Epoch 00044: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 6.1257 - acc: 0.6175 - val_loss: 7.0873 - val_acc: 0.5030\n",
      "Epoch 46/100\n",
      "6660/6680 [============================>.] - ETA: 2s - loss: 4.8354 - acc: 0.700 - ETA: 2s - loss: 4.3749 - acc: 0.728 - ETA: 3s - loss: 5.1713 - acc: 0.679 - ETA: 3s - loss: 5.3573 - acc: 0.667 - ETA: 3s - loss: 5.2387 - acc: 0.675 - ETA: 3s - loss: 5.4937 - acc: 0.657 - ETA: 3s - loss: 5.7417 - acc: 0.640 - ETA: 3s - loss: 5.7753 - acc: 0.638 - ETA: 3s - loss: 6.0440 - acc: 0.622 - ETA: 2s - loss: 6.0012 - acc: 0.625 - ETA: 2s - loss: 6.1681 - acc: 0.615 - ETA: 2s - loss: 6.3776 - acc: 0.602 - ETA: 2s - loss: 6.4219 - acc: 0.600 - ETA: 2s - loss: 6.3887 - acc: 0.602 - ETA: 2s - loss: 6.3491 - acc: 0.604 - ETA: 2s - loss: 6.3364 - acc: 0.605 - ETA: 2s - loss: 6.3429 - acc: 0.605 - ETA: 2s - loss: 6.3838 - acc: 0.602 - ETA: 2s - loss: 6.3339 - acc: 0.604 - ETA: 2s - loss: 6.2859 - acc: 0.607 - ETA: 2s - loss: 6.2724 - acc: 0.608 - ETA: 2s - loss: 6.2462 - acc: 0.610 - ETA: 2s - loss: 6.2903 - acc: 0.608 - ETA: 2s - loss: 6.2728 - acc: 0.608 - ETA: 2s - loss: 6.2547 - acc: 0.610 - ETA: 2s - loss: 6.2260 - acc: 0.611 - ETA: 1s - loss: 6.2224 - acc: 0.612 - ETA: 1s - loss: 6.1671 - acc: 0.615 - ETA: 1s - loss: 6.1801 - acc: 0.614 - ETA: 1s - loss: 6.1836 - acc: 0.614 - ETA: 1s - loss: 6.1562 - acc: 0.616 - ETA: 1s - loss: 6.1866 - acc: 0.614 - ETA: 1s - loss: 6.1751 - acc: 0.615 - ETA: 1s - loss: 6.1696 - acc: 0.615 - ETA: 1s - loss: 6.1902 - acc: 0.614 - ETA: 1s - loss: 6.1840 - acc: 0.614 - ETA: 1s - loss: 6.1627 - acc: 0.615 - ETA: 1s - loss: 6.1535 - acc: 0.616 - ETA: 1s - loss: 6.1665 - acc: 0.615 - ETA: 1s - loss: 6.1655 - acc: 0.615 - ETA: 1s - loss: 6.1722 - acc: 0.615 - ETA: 1s - loss: 6.1297 - acc: 0.618 - ETA: 1s - loss: 6.1296 - acc: 0.618 - ETA: 1s - loss: 6.1403 - acc: 0.617 - ETA: 1s - loss: 6.1553 - acc: 0.616 - ETA: 1s - loss: 6.1684 - acc: 0.615 - ETA: 0s - loss: 6.1419 - acc: 0.617 - ETA: 0s - loss: 6.1486 - acc: 0.616 - ETA: 0s - loss: 6.1181 - acc: 0.618 - ETA: 0s - loss: 6.1266 - acc: 0.618 - ETA: 0s - loss: 6.1296 - acc: 0.617 - ETA: 0s - loss: 6.1263 - acc: 0.617 - ETA: 0s - loss: 6.1200 - acc: 0.618 - ETA: 0s - loss: 6.1096 - acc: 0.618 - ETA: 0s - loss: 6.1164 - acc: 0.618 - ETA: 0s - loss: 6.1166 - acc: 0.618 - ETA: 0s - loss: 6.1111 - acc: 0.618 - ETA: 0s - loss: 6.1207 - acc: 0.618 - ETA: 0s - loss: 6.1181 - acc: 0.618 - ETA: 0s - loss: 6.1024 - acc: 0.619 - ETA: 0s - loss: 6.1048 - acc: 0.619 - ETA: 0s - loss: 6.0967 - acc: 0.619 - ETA: 0s - loss: 6.1040 - acc: 0.619 - ETA: 0s - loss: 6.1113 - acc: 0.618 - ETA: 0s - loss: 6.1169 - acc: 0.618 - ETA: 0s - loss: 6.1214 - acc: 0.6177Epoch 00045: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 6.1200 - acc: 0.6178 - val_loss: 7.1764 - val_acc: 0.4970\n",
      "Epoch 47/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6620/6680 [============================>.] - ETA: 3s - loss: 6.4492 - acc: 0.600 - ETA: 3s - loss: 5.9707 - acc: 0.625 - ETA: 3s - loss: 5.6835 - acc: 0.640 - ETA: 3s - loss: 5.4472 - acc: 0.653 - ETA: 3s - loss: 5.5808 - acc: 0.645 - ETA: 3s - loss: 5.6856 - acc: 0.640 - ETA: 3s - loss: 5.7335 - acc: 0.638 - ETA: 3s - loss: 5.5838 - acc: 0.648 - ETA: 3s - loss: 5.6112 - acc: 0.647 - ETA: 2s - loss: 5.6316 - acc: 0.646 - ETA: 2s - loss: 5.7100 - acc: 0.642 - ETA: 2s - loss: 5.7051 - acc: 0.642 - ETA: 2s - loss: 5.6256 - acc: 0.646 - ETA: 2s - loss: 5.6643 - acc: 0.644 - ETA: 2s - loss: 5.7584 - acc: 0.637 - ETA: 2s - loss: 5.8130 - acc: 0.634 - ETA: 2s - loss: 5.8578 - acc: 0.631 - ETA: 2s - loss: 5.8447 - acc: 0.631 - ETA: 2s - loss: 5.8773 - acc: 0.630 - ETA: 2s - loss: 5.9030 - acc: 0.628 - ETA: 2s - loss: 5.9664 - acc: 0.624 - ETA: 2s - loss: 5.9571 - acc: 0.624 - ETA: 2s - loss: 6.0037 - acc: 0.622 - ETA: 2s - loss: 6.0837 - acc: 0.617 - ETA: 2s - loss: 6.0883 - acc: 0.616 - ETA: 2s - loss: 6.0311 - acc: 0.620 - ETA: 2s - loss: 6.0086 - acc: 0.621 - ETA: 1s - loss: 6.0070 - acc: 0.622 - ETA: 1s - loss: 5.9946 - acc: 0.623 - ETA: 1s - loss: 6.0066 - acc: 0.622 - ETA: 1s - loss: 5.9838 - acc: 0.624 - ETA: 1s - loss: 5.9960 - acc: 0.623 - ETA: 1s - loss: 6.0192 - acc: 0.622 - ETA: 1s - loss: 6.0046 - acc: 0.623 - ETA: 1s - loss: 5.9925 - acc: 0.623 - ETA: 1s - loss: 6.0006 - acc: 0.623 - ETA: 1s - loss: 6.0039 - acc: 0.623 - ETA: 1s - loss: 5.9862 - acc: 0.624 - ETA: 1s - loss: 6.0174 - acc: 0.622 - ETA: 1s - loss: 6.0319 - acc: 0.621 - ETA: 1s - loss: 6.0418 - acc: 0.621 - ETA: 1s - loss: 6.0427 - acc: 0.621 - ETA: 1s - loss: 6.0285 - acc: 0.621 - ETA: 1s - loss: 6.0342 - acc: 0.621 - ETA: 1s - loss: 6.0467 - acc: 0.621 - ETA: 0s - loss: 6.0638 - acc: 0.620 - ETA: 0s - loss: 6.0829 - acc: 0.618 - ETA: 0s - loss: 6.0725 - acc: 0.619 - ETA: 0s - loss: 6.0736 - acc: 0.619 - ETA: 0s - loss: 6.0799 - acc: 0.618 - ETA: 0s - loss: 6.0615 - acc: 0.620 - ETA: 0s - loss: 6.0749 - acc: 0.619 - ETA: 0s - loss: 6.1037 - acc: 0.617 - ETA: 0s - loss: 6.0793 - acc: 0.619 - ETA: 0s - loss: 6.0832 - acc: 0.618 - ETA: 0s - loss: 6.1058 - acc: 0.617 - ETA: 0s - loss: 6.0937 - acc: 0.618 - ETA: 0s - loss: 6.1104 - acc: 0.617 - ETA: 0s - loss: 6.1160 - acc: 0.616 - ETA: 0s - loss: 6.1106 - acc: 0.616 - ETA: 0s - loss: 6.1098 - acc: 0.616 - ETA: 0s - loss: 6.0984 - acc: 0.617 - ETA: 0s - loss: 6.1002 - acc: 0.617 - ETA: 0s - loss: 6.0793 - acc: 0.6183Epoch 00046: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 6.0850 - acc: 0.6180 - val_loss: 7.0829 - val_acc: 0.4994\n",
      "Epoch 48/100\n",
      "6620/6680 [============================>.] - ETA: 4s - loss: 6.4501 - acc: 0.600 - ETA: 3s - loss: 6.5914 - acc: 0.583 - ETA: 3s - loss: 6.7208 - acc: 0.579 - ETA: 3s - loss: 6.6920 - acc: 0.577 - ETA: 3s - loss: 6.8207 - acc: 0.569 - ETA: 3s - loss: 6.4381 - acc: 0.594 - ETA: 2s - loss: 6.3288 - acc: 0.600 - ETA: 2s - loss: 6.2227 - acc: 0.607 - ETA: 2s - loss: 6.2835 - acc: 0.604 - ETA: 2s - loss: 6.2080 - acc: 0.609 - ETA: 2s - loss: 6.2744 - acc: 0.605 - ETA: 2s - loss: 6.1828 - acc: 0.611 - ETA: 2s - loss: 6.2915 - acc: 0.604 - ETA: 2s - loss: 6.2476 - acc: 0.607 - ETA: 2s - loss: 6.2403 - acc: 0.607 - ETA: 2s - loss: 6.2531 - acc: 0.606 - ETA: 2s - loss: 6.2806 - acc: 0.605 - ETA: 2s - loss: 6.2541 - acc: 0.607 - ETA: 2s - loss: 6.2305 - acc: 0.608 - ETA: 2s - loss: 6.2238 - acc: 0.609 - ETA: 2s - loss: 6.1958 - acc: 0.611 - ETA: 2s - loss: 6.1408 - acc: 0.614 - ETA: 2s - loss: 6.0765 - acc: 0.618 - ETA: 2s - loss: 6.0130 - acc: 0.622 - ETA: 2s - loss: 5.9981 - acc: 0.623 - ETA: 2s - loss: 5.9667 - acc: 0.625 - ETA: 2s - loss: 5.9746 - acc: 0.625 - ETA: 2s - loss: 5.9823 - acc: 0.624 - ETA: 2s - loss: 5.9428 - acc: 0.626 - ETA: 2s - loss: 5.9346 - acc: 0.627 - ETA: 1s - loss: 5.9006 - acc: 0.629 - ETA: 1s - loss: 5.9472 - acc: 0.625 - ETA: 1s - loss: 5.9532 - acc: 0.625 - ETA: 1s - loss: 5.9427 - acc: 0.626 - ETA: 1s - loss: 5.9667 - acc: 0.624 - ETA: 1s - loss: 5.9712 - acc: 0.624 - ETA: 1s - loss: 5.9955 - acc: 0.622 - ETA: 1s - loss: 6.0336 - acc: 0.620 - ETA: 1s - loss: 6.0383 - acc: 0.620 - ETA: 1s - loss: 6.0529 - acc: 0.619 - ETA: 1s - loss: 6.0610 - acc: 0.619 - ETA: 1s - loss: 6.0865 - acc: 0.617 - ETA: 1s - loss: 6.0915 - acc: 0.616 - ETA: 1s - loss: 6.0750 - acc: 0.617 - ETA: 1s - loss: 6.0945 - acc: 0.616 - ETA: 1s - loss: 6.0773 - acc: 0.617 - ETA: 1s - loss: 6.0873 - acc: 0.617 - ETA: 1s - loss: 6.0916 - acc: 0.617 - ETA: 1s - loss: 6.0822 - acc: 0.617 - ETA: 0s - loss: 6.0698 - acc: 0.618 - ETA: 0s - loss: 6.0564 - acc: 0.619 - ETA: 0s - loss: 6.0357 - acc: 0.620 - ETA: 0s - loss: 6.0218 - acc: 0.621 - ETA: 0s - loss: 6.0068 - acc: 0.622 - ETA: 0s - loss: 5.9953 - acc: 0.623 - ETA: 0s - loss: 6.0214 - acc: 0.621 - ETA: 0s - loss: 6.0128 - acc: 0.622 - ETA: 0s - loss: 5.9905 - acc: 0.623 - ETA: 0s - loss: 6.0126 - acc: 0.622 - ETA: 0s - loss: 6.0035 - acc: 0.622 - ETA: 0s - loss: 5.9974 - acc: 0.623 - ETA: 0s - loss: 6.0209 - acc: 0.621 - ETA: 0s - loss: 6.0280 - acc: 0.621 - ETA: 0s - loss: 6.0502 - acc: 0.620 - ETA: 0s - loss: 6.0489 - acc: 0.620 - ETA: 0s - loss: 6.0400 - acc: 0.621 - ETA: 0s - loss: 6.0389 - acc: 0.621 - ETA: 0s - loss: 6.0596 - acc: 0.6199Epoch 00047: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 6.0536 - acc: 0.6204 - val_loss: 7.0783 - val_acc: 0.5018\n",
      "Epoch 49/100\n",
      "6600/6680 [============================>.] - ETA: 2s - loss: 11.2830 - acc: 0.30 - ETA: 3s - loss: 6.3131 - acc: 0.6083 - ETA: 3s - loss: 5.4954 - acc: 0.659 - ETA: 3s - loss: 5.5923 - acc: 0.653 - ETA: 3s - loss: 5.4123 - acc: 0.664 - ETA: 3s - loss: 5.7973 - acc: 0.640 - ETA: 3s - loss: 5.8502 - acc: 0.637 - ETA: 3s - loss: 5.8880 - acc: 0.633 - ETA: 3s - loss: 6.0044 - acc: 0.626 - ETA: 3s - loss: 6.0655 - acc: 0.622 - ETA: 3s - loss: 6.1954 - acc: 0.614 - ETA: 3s - loss: 6.1905 - acc: 0.613 - ETA: 3s - loss: 6.1571 - acc: 0.615 - ETA: 3s - loss: 6.1293 - acc: 0.617 - ETA: 3s - loss: 6.0762 - acc: 0.620 - ETA: 3s - loss: 6.1086 - acc: 0.619 - ETA: 3s - loss: 6.1945 - acc: 0.613 - ETA: 3s - loss: 6.1266 - acc: 0.618 - ETA: 2s - loss: 6.0981 - acc: 0.620 - ETA: 2s - loss: 6.0548 - acc: 0.622 - ETA: 2s - loss: 6.1148 - acc: 0.619 - ETA: 2s - loss: 6.1570 - acc: 0.616 - ETA: 2s - loss: 6.1400 - acc: 0.617 - ETA: 2s - loss: 6.1024 - acc: 0.619 - ETA: 2s - loss: 6.1175 - acc: 0.618 - ETA: 2s - loss: 6.1585 - acc: 0.616 - ETA: 2s - loss: 6.2149 - acc: 0.612 - ETA: 2s - loss: 6.2177 - acc: 0.612 - ETA: 2s - loss: 6.2144 - acc: 0.612 - ETA: 2s - loss: 6.2154 - acc: 0.612 - ETA: 2s - loss: 6.2009 - acc: 0.613 - ETA: 2s - loss: 6.2365 - acc: 0.611 - ETA: 2s - loss: 6.1802 - acc: 0.614 - ETA: 2s - loss: 6.1818 - acc: 0.614 - ETA: 2s - loss: 6.1590 - acc: 0.615 - ETA: 1s - loss: 6.1628 - acc: 0.615 - ETA: 1s - loss: 6.1523 - acc: 0.616 - ETA: 1s - loss: 6.1699 - acc: 0.615 - ETA: 1s - loss: 6.2087 - acc: 0.613 - ETA: 1s - loss: 6.2110 - acc: 0.612 - ETA: 1s - loss: 6.1693 - acc: 0.615 - ETA: 1s - loss: 6.1829 - acc: 0.614 - ETA: 1s - loss: 6.1720 - acc: 0.615 - ETA: 1s - loss: 6.1298 - acc: 0.618 - ETA: 1s - loss: 6.1413 - acc: 0.617 - ETA: 1s - loss: 6.1298 - acc: 0.618 - ETA: 1s - loss: 6.1297 - acc: 0.618 - ETA: 1s - loss: 6.1461 - acc: 0.617 - ETA: 1s - loss: 6.1337 - acc: 0.617 - ETA: 1s - loss: 6.1513 - acc: 0.616 - ETA: 1s - loss: 6.1507 - acc: 0.616 - ETA: 1s - loss: 6.1269 - acc: 0.618 - ETA: 1s - loss: 6.1321 - acc: 0.617 - ETA: 0s - loss: 6.1224 - acc: 0.618 - ETA: 0s - loss: 6.0855 - acc: 0.620 - ETA: 0s - loss: 6.0677 - acc: 0.621 - ETA: 0s - loss: 6.0498 - acc: 0.622 - ETA: 0s - loss: 6.0542 - acc: 0.622 - ETA: 0s - loss: 6.0584 - acc: 0.622 - ETA: 0s - loss: 6.0768 - acc: 0.620 - ETA: 0s - loss: 6.0608 - acc: 0.621 - ETA: 0s - loss: 6.0563 - acc: 0.622 - ETA: 0s - loss: 6.0602 - acc: 0.622 - ETA: 0s - loss: 6.0747 - acc: 0.621 - ETA: 0s - loss: 6.0861 - acc: 0.620 - ETA: 0s - loss: 6.0645 - acc: 0.621 - ETA: 0s - loss: 6.0552 - acc: 0.622 - ETA: 0s - loss: 6.0521 - acc: 0.622 - ETA: 0s - loss: 6.0508 - acc: 0.622 - ETA: 0s - loss: 6.0544 - acc: 0.6224Epoch 00048: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 6.0470 - acc: 0.6229 - val_loss: 7.1170 - val_acc: 0.5066\n",
      "Epoch 50/100\n",
      "6620/6680 [============================>.] - ETA: 4s - loss: 6.4472 - acc: 0.600 - ETA: 3s - loss: 5.8006 - acc: 0.633 - ETA: 3s - loss: 6.8459 - acc: 0.568 - ETA: 3s - loss: 6.3687 - acc: 0.600 - ETA: 3s - loss: 6.2232 - acc: 0.610 - ETA: 3s - loss: 6.3648 - acc: 0.602 - ETA: 3s - loss: 6.0887 - acc: 0.618 - ETA: 3s - loss: 6.0479 - acc: 0.621 - ETA: 3s - loss: 6.0978 - acc: 0.618 - ETA: 3s - loss: 6.0650 - acc: 0.621 - ETA: 3s - loss: 6.0360 - acc: 0.622 - ETA: 3s - loss: 5.9744 - acc: 0.625 - ETA: 3s - loss: 5.9075 - acc: 0.628 - ETA: 3s - loss: 5.9430 - acc: 0.626 - ETA: 3s - loss: 6.0047 - acc: 0.622 - ETA: 3s - loss: 6.0242 - acc: 0.621 - ETA: 2s - loss: 5.9894 - acc: 0.623 - ETA: 2s - loss: 5.9684 - acc: 0.625 - ETA: 2s - loss: 5.9134 - acc: 0.629 - ETA: 2s - loss: 5.9074 - acc: 0.629 - ETA: 2s - loss: 5.9350 - acc: 0.628 - ETA: 2s - loss: 5.9990 - acc: 0.624 - ETA: 2s - loss: 6.0346 - acc: 0.622 - ETA: 2s - loss: 6.0638 - acc: 0.620 - ETA: 2s - loss: 6.0602 - acc: 0.620 - ETA: 2s - loss: 6.0496 - acc: 0.621 - ETA: 2s - loss: 6.0464 - acc: 0.621 - ETA: 2s - loss: 6.0432 - acc: 0.622 - ETA: 2s - loss: 6.0462 - acc: 0.621 - ETA: 2s - loss: 6.0831 - acc: 0.619 - ETA: 2s - loss: 6.1064 - acc: 0.618 - ETA: 2s - loss: 6.0941 - acc: 0.619 - ETA: 2s - loss: 6.0825 - acc: 0.620 - ETA: 2s - loss: 6.0866 - acc: 0.619 - ETA: 1s - loss: 6.0927 - acc: 0.619 - ETA: 1s - loss: 6.0793 - acc: 0.620 - ETA: 1s - loss: 6.0482 - acc: 0.622 - ETA: 1s - loss: 6.0234 - acc: 0.624 - ETA: 1s - loss: 6.0305 - acc: 0.623 - ETA: 1s - loss: 6.0415 - acc: 0.623 - ETA: 1s - loss: 6.0561 - acc: 0.622 - ETA: 1s - loss: 6.0519 - acc: 0.622 - ETA: 1s - loss: 6.0457 - acc: 0.622 - ETA: 1s - loss: 6.0554 - acc: 0.622 - ETA: 1s - loss: 6.0722 - acc: 0.621 - ETA: 1s - loss: 6.0554 - acc: 0.622 - ETA: 1s - loss: 6.0353 - acc: 0.623 - ETA: 1s - loss: 6.0443 - acc: 0.623 - ETA: 1s - loss: 6.0374 - acc: 0.623 - ETA: 1s - loss: 6.0272 - acc: 0.624 - ETA: 1s - loss: 6.0226 - acc: 0.624 - ETA: 1s - loss: 5.9945 - acc: 0.625 - ETA: 0s - loss: 5.9842 - acc: 0.626 - ETA: 0s - loss: 5.9775 - acc: 0.627 - ETA: 0s - loss: 5.9772 - acc: 0.627 - ETA: 0s - loss: 5.9558 - acc: 0.628 - ETA: 0s - loss: 5.9769 - acc: 0.627 - ETA: 0s - loss: 5.9956 - acc: 0.626 - ETA: 0s - loss: 6.0095 - acc: 0.625 - ETA: 0s - loss: 6.0187 - acc: 0.624 - ETA: 0s - loss: 6.0262 - acc: 0.624 - ETA: 0s - loss: 6.0265 - acc: 0.624 - ETA: 0s - loss: 6.0256 - acc: 0.624 - ETA: 0s - loss: 6.0565 - acc: 0.622 - ETA: 0s - loss: 6.0603 - acc: 0.622 - ETA: 0s - loss: 6.0485 - acc: 0.622 - ETA: 0s - loss: 6.0612 - acc: 0.622 - ETA: 0s - loss: 6.0775 - acc: 0.621 - ETA: 0s - loss: 6.0759 - acc: 0.621 - ETA: 0s - loss: 6.0582 - acc: 0.622 - ETA: 0s - loss: 6.0398 - acc: 0.6236Epoch 00049: val_loss improved from 7.07804 to 7.04041, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 6.0386 - acc: 0.6237 - val_loss: 7.0404 - val_acc: 0.5126\n",
      "Epoch 51/100\n",
      "6620/6680 [============================>.] - ETA: 2s - loss: 5.6413 - acc: 0.650 - ETA: 3s - loss: 4.1794 - acc: 0.733 - ETA: 3s - loss: 5.5459 - acc: 0.650 - ETA: 3s - loss: 5.4246 - acc: 0.659 - ETA: 3s - loss: 5.3471 - acc: 0.665 - ETA: 3s - loss: 5.5396 - acc: 0.652 - ETA: 3s - loss: 5.6640 - acc: 0.645 - ETA: 3s - loss: 5.6854 - acc: 0.642 - ETA: 3s - loss: 5.8256 - acc: 0.634 - ETA: 3s - loss: 5.8048 - acc: 0.636 - ETA: 3s - loss: 5.8751 - acc: 0.632 - ETA: 3s - loss: 5.9605 - acc: 0.627 - ETA: 3s - loss: 5.9612 - acc: 0.627 - ETA: 3s - loss: 5.9358 - acc: 0.629 - ETA: 3s - loss: 5.8668 - acc: 0.633 - ETA: 2s - loss: 5.8742 - acc: 0.632 - ETA: 2s - loss: 5.8697 - acc: 0.633 - ETA: 2s - loss: 5.8656 - acc: 0.633 - ETA: 2s - loss: 5.8620 - acc: 0.634 - ETA: 2s - loss: 5.8675 - acc: 0.633 - ETA: 2s - loss: 5.9330 - acc: 0.629 - ETA: 2s - loss: 5.9374 - acc: 0.629 - ETA: 2s - loss: 5.9614 - acc: 0.628 - ETA: 2s - loss: 6.0196 - acc: 0.624 - ETA: 2s - loss: 6.0485 - acc: 0.623 - ETA: 2s - loss: 6.0047 - acc: 0.625 - ETA: 2s - loss: 5.9735 - acc: 0.627 - ETA: 2s - loss: 6.0168 - acc: 0.625 - ETA: 2s - loss: 6.0449 - acc: 0.623 - ETA: 2s - loss: 6.0652 - acc: 0.622 - ETA: 2s - loss: 6.0083 - acc: 0.625 - ETA: 2s - loss: 5.9905 - acc: 0.627 - ETA: 2s - loss: 5.9690 - acc: 0.628 - ETA: 2s - loss: 5.9739 - acc: 0.627 - ETA: 1s - loss: 6.0280 - acc: 0.624 - ETA: 1s - loss: 6.0117 - acc: 0.625 - ETA: 1s - loss: 6.0103 - acc: 0.625 - ETA: 1s - loss: 6.0065 - acc: 0.626 - ETA: 1s - loss: 6.0099 - acc: 0.625 - ETA: 1s - loss: 6.0305 - acc: 0.624 - ETA: 1s - loss: 6.0455 - acc: 0.623 - ETA: 1s - loss: 6.0271 - acc: 0.624 - ETA: 1s - loss: 6.0336 - acc: 0.624 - ETA: 1s - loss: 6.0482 - acc: 0.623 - ETA: 1s - loss: 6.0290 - acc: 0.624 - ETA: 1s - loss: 6.0424 - acc: 0.623 - ETA: 1s - loss: 6.0589 - acc: 0.622 - ETA: 1s - loss: 6.0514 - acc: 0.623 - ETA: 1s - loss: 6.0408 - acc: 0.623 - ETA: 1s - loss: 6.0460 - acc: 0.623 - ETA: 1s - loss: 6.0342 - acc: 0.624 - ETA: 1s - loss: 6.0430 - acc: 0.623 - ETA: 0s - loss: 6.0363 - acc: 0.623 - ETA: 0s - loss: 6.0349 - acc: 0.624 - ETA: 0s - loss: 6.0211 - acc: 0.624 - ETA: 0s - loss: 6.0170 - acc: 0.625 - ETA: 0s - loss: 6.0258 - acc: 0.624 - ETA: 0s - loss: 6.0233 - acc: 0.624 - ETA: 0s - loss: 6.0401 - acc: 0.623 - ETA: 0s - loss: 6.0178 - acc: 0.625 - ETA: 0s - loss: 6.0224 - acc: 0.624 - ETA: 0s - loss: 6.0323 - acc: 0.624 - ETA: 0s - loss: 6.0420 - acc: 0.623 - ETA: 0s - loss: 6.0460 - acc: 0.623 - ETA: 0s - loss: 6.0563 - acc: 0.622 - ETA: 0s - loss: 6.0293 - acc: 0.624 - ETA: 0s - loss: 6.0358 - acc: 0.623 - ETA: 0s - loss: 6.0496 - acc: 0.623 - ETA: 0s - loss: 6.0385 - acc: 0.6237Epoch 00050: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 6.0374 - acc: 0.6238 - val_loss: 7.0679 - val_acc: 0.5078\n",
      "Epoch 52/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6600/6680 [============================>.] - ETA: 3s - loss: 4.0295 - acc: 0.750 - ETA: 5s - loss: 6.0444 - acc: 0.625 - ETA: 12s - loss: 5.9111 - acc: 0.63 - ETA: 10s - loss: 5.9644 - acc: 0.63 - ETA: 14s - loss: 6.0083 - acc: 0.62 - ETA: 14s - loss: 5.9777 - acc: 0.62 - ETA: 14s - loss: 5.6419 - acc: 0.65 - ETA: 11s - loss: 5.6865 - acc: 0.64 - ETA: 9s - loss: 5.6786 - acc: 0.6477 - ETA: 8s - loss: 5.7038 - acc: 0.646 - ETA: 8s - loss: 5.6149 - acc: 0.651 - ETA: 7s - loss: 5.6193 - acc: 0.651 - ETA: 6s - loss: 5.6024 - acc: 0.652 - ETA: 6s - loss: 5.7117 - acc: 0.645 - ETA: 5s - loss: 5.7997 - acc: 0.640 - ETA: 5s - loss: 5.7015 - acc: 0.645 - ETA: 4s - loss: 5.7227 - acc: 0.644 - ETA: 4s - loss: 5.6084 - acc: 0.651 - ETA: 4s - loss: 5.6670 - acc: 0.647 - ETA: 4s - loss: 5.6335 - acc: 0.650 - ETA: 4s - loss: 5.6241 - acc: 0.650 - ETA: 4s - loss: 5.6438 - acc: 0.649 - ETA: 3s - loss: 5.6524 - acc: 0.648 - ETA: 3s - loss: 5.6519 - acc: 0.649 - ETA: 3s - loss: 5.6119 - acc: 0.651 - ETA: 3s - loss: 5.6485 - acc: 0.649 - ETA: 3s - loss: 5.6910 - acc: 0.646 - ETA: 3s - loss: 5.6548 - acc: 0.648 - ETA: 3s - loss: 5.6996 - acc: 0.646 - ETA: 2s - loss: 5.7098 - acc: 0.645 - ETA: 2s - loss: 5.7502 - acc: 0.642 - ETA: 2s - loss: 5.7745 - acc: 0.641 - ETA: 2s - loss: 5.7475 - acc: 0.643 - ETA: 2s - loss: 5.7656 - acc: 0.641 - ETA: 2s - loss: 5.7720 - acc: 0.641 - ETA: 2s - loss: 5.8026 - acc: 0.639 - ETA: 2s - loss: 5.8123 - acc: 0.639 - ETA: 2s - loss: 5.7978 - acc: 0.640 - ETA: 2s - loss: 5.8164 - acc: 0.638 - ETA: 2s - loss: 5.8205 - acc: 0.638 - ETA: 1s - loss: 5.8897 - acc: 0.634 - ETA: 1s - loss: 5.9030 - acc: 0.633 - ETA: 1s - loss: 5.9251 - acc: 0.632 - ETA: 1s - loss: 5.9422 - acc: 0.631 - ETA: 1s - loss: 5.9388 - acc: 0.631 - ETA: 1s - loss: 5.9152 - acc: 0.632 - ETA: 1s - loss: 5.9238 - acc: 0.632 - ETA: 1s - loss: 5.9501 - acc: 0.630 - ETA: 1s - loss: 5.9561 - acc: 0.630 - ETA: 1s - loss: 5.9771 - acc: 0.629 - ETA: 1s - loss: 6.0073 - acc: 0.627 - ETA: 1s - loss: 5.9775 - acc: 0.628 - ETA: 1s - loss: 6.0032 - acc: 0.627 - ETA: 0s - loss: 5.9865 - acc: 0.628 - ETA: 0s - loss: 6.0203 - acc: 0.626 - ETA: 0s - loss: 5.9870 - acc: 0.628 - ETA: 0s - loss: 5.9749 - acc: 0.628 - ETA: 0s - loss: 5.9644 - acc: 0.629 - ETA: 0s - loss: 5.9662 - acc: 0.629 - ETA: 0s - loss: 5.9406 - acc: 0.631 - ETA: 0s - loss: 5.9505 - acc: 0.630 - ETA: 0s - loss: 5.9480 - acc: 0.630 - ETA: 0s - loss: 5.9688 - acc: 0.629 - ETA: 0s - loss: 5.9846 - acc: 0.628 - ETA: 0s - loss: 5.9962 - acc: 0.627 - ETA: 0s - loss: 6.0059 - acc: 0.627 - ETA: 0s - loss: 6.0056 - acc: 0.626 - ETA: 0s - loss: 6.0223 - acc: 0.625 - ETA: 0s - loss: 6.0312 - acc: 0.6253Epoch 00051: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 6.0362 - acc: 0.6250 - val_loss: 7.0524 - val_acc: 0.5138\n",
      "Epoch 53/100\n",
      "6640/6680 [============================>.] - ETA: 4s - loss: 5.6414 - acc: 0.650 - ETA: 3s - loss: 6.4501 - acc: 0.600 - ETA: 3s - loss: 6.7845 - acc: 0.579 - ETA: 3s - loss: 7.0645 - acc: 0.561 - ETA: 3s - loss: 6.5946 - acc: 0.590 - ETA: 3s - loss: 6.4766 - acc: 0.598 - ETA: 3s - loss: 6.3081 - acc: 0.607 - ETA: 2s - loss: 6.4099 - acc: 0.600 - ETA: 2s - loss: 6.3076 - acc: 0.606 - ETA: 2s - loss: 6.4188 - acc: 0.600 - ETA: 2s - loss: 6.3623 - acc: 0.603 - ETA: 2s - loss: 6.3022 - acc: 0.607 - ETA: 2s - loss: 6.3154 - acc: 0.606 - ETA: 2s - loss: 6.2257 - acc: 0.612 - ETA: 2s - loss: 6.2296 - acc: 0.612 - ETA: 2s - loss: 6.2724 - acc: 0.609 - ETA: 2s - loss: 6.2571 - acc: 0.610 - ETA: 2s - loss: 6.2673 - acc: 0.610 - ETA: 2s - loss: 6.3246 - acc: 0.606 - ETA: 2s - loss: 6.2535 - acc: 0.611 - ETA: 2s - loss: 6.2252 - acc: 0.613 - ETA: 2s - loss: 6.2136 - acc: 0.613 - ETA: 2s - loss: 6.1445 - acc: 0.617 - ETA: 2s - loss: 6.1307 - acc: 0.618 - ETA: 2s - loss: 6.0934 - acc: 0.620 - ETA: 2s - loss: 6.0917 - acc: 0.620 - ETA: 1s - loss: 6.1000 - acc: 0.619 - ETA: 1s - loss: 6.0512 - acc: 0.622 - ETA: 1s - loss: 6.0562 - acc: 0.622 - ETA: 1s - loss: 6.0944 - acc: 0.620 - ETA: 1s - loss: 6.0704 - acc: 0.621 - ETA: 1s - loss: 6.0407 - acc: 0.623 - ETA: 1s - loss: 6.0478 - acc: 0.623 - ETA: 1s - loss: 6.0319 - acc: 0.624 - ETA: 1s - loss: 6.0301 - acc: 0.624 - ETA: 1s - loss: 6.0412 - acc: 0.623 - ETA: 1s - loss: 6.0475 - acc: 0.623 - ETA: 1s - loss: 6.0617 - acc: 0.622 - ETA: 1s - loss: 6.0792 - acc: 0.621 - ETA: 1s - loss: 6.0997 - acc: 0.620 - ETA: 1s - loss: 6.1078 - acc: 0.620 - ETA: 1s - loss: 6.0824 - acc: 0.621 - ETA: 1s - loss: 6.0833 - acc: 0.621 - ETA: 1s - loss: 6.0897 - acc: 0.621 - ETA: 1s - loss: 6.0697 - acc: 0.622 - ETA: 0s - loss: 6.0826 - acc: 0.621 - ETA: 0s - loss: 6.0786 - acc: 0.621 - ETA: 0s - loss: 6.0683 - acc: 0.622 - ETA: 0s - loss: 6.0631 - acc: 0.622 - ETA: 0s - loss: 6.0736 - acc: 0.622 - ETA: 0s - loss: 6.0715 - acc: 0.622 - ETA: 0s - loss: 6.0455 - acc: 0.623 - ETA: 0s - loss: 6.0676 - acc: 0.622 - ETA: 0s - loss: 6.0570 - acc: 0.623 - ETA: 0s - loss: 6.0611 - acc: 0.623 - ETA: 0s - loss: 6.0734 - acc: 0.622 - ETA: 0s - loss: 6.0743 - acc: 0.622 - ETA: 0s - loss: 6.0637 - acc: 0.622 - ETA: 0s - loss: 6.0555 - acc: 0.623 - ETA: 0s - loss: 6.0631 - acc: 0.622 - ETA: 0s - loss: 6.0590 - acc: 0.622 - ETA: 0s - loss: 6.0475 - acc: 0.623 - ETA: 0s - loss: 6.0301 - acc: 0.624 - ETA: 0s - loss: 6.0461 - acc: 0.6238Epoch 00052: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 6.0341 - acc: 0.6246 - val_loss: 7.0483 - val_acc: 0.5174\n",
      "Epoch 54/100\n",
      "6600/6680 [============================>.] - ETA: 6s - loss: 5.8377 - acc: 0.600 - ETA: 3s - loss: 6.0775 - acc: 0.616 - ETA: 3s - loss: 5.9976 - acc: 0.620 - ETA: 3s - loss: 5.9876 - acc: 0.623 - ETA: 3s - loss: 5.8357 - acc: 0.634 - ETA: 3s - loss: 5.8987 - acc: 0.630 - ETA: 3s - loss: 5.9512 - acc: 0.628 - ETA: 3s - loss: 6.0836 - acc: 0.620 - ETA: 3s - loss: 6.0693 - acc: 0.621 - ETA: 2s - loss: 6.0998 - acc: 0.619 - ETA: 3s - loss: 6.1267 - acc: 0.618 - ETA: 2s - loss: 5.9995 - acc: 0.626 - ETA: 2s - loss: 5.9782 - acc: 0.627 - ETA: 2s - loss: 6.0307 - acc: 0.624 - ETA: 2s - loss: 6.0261 - acc: 0.625 - ETA: 2s - loss: 6.0222 - acc: 0.625 - ETA: 2s - loss: 5.9669 - acc: 0.628 - ETA: 2s - loss: 6.0115 - acc: 0.626 - ETA: 2s - loss: 5.9800 - acc: 0.628 - ETA: 2s - loss: 5.9552 - acc: 0.629 - ETA: 2s - loss: 5.9480 - acc: 0.630 - ETA: 2s - loss: 5.9487 - acc: 0.630 - ETA: 2s - loss: 5.9856 - acc: 0.627 - ETA: 2s - loss: 5.9742 - acc: 0.628 - ETA: 2s - loss: 5.9480 - acc: 0.630 - ETA: 2s - loss: 5.9486 - acc: 0.630 - ETA: 2s - loss: 5.8995 - acc: 0.633 - ETA: 2s - loss: 5.9246 - acc: 0.631 - ETA: 1s - loss: 5.8911 - acc: 0.633 - ETA: 1s - loss: 5.8919 - acc: 0.633 - ETA: 1s - loss: 5.9197 - acc: 0.632 - ETA: 1s - loss: 5.9390 - acc: 0.630 - ETA: 1s - loss: 5.9588 - acc: 0.629 - ETA: 1s - loss: 5.9750 - acc: 0.628 - ETA: 1s - loss: 5.9886 - acc: 0.627 - ETA: 1s - loss: 5.9817 - acc: 0.628 - ETA: 1s - loss: 5.9794 - acc: 0.628 - ETA: 1s - loss: 6.0038 - acc: 0.626 - ETA: 1s - loss: 5.9907 - acc: 0.627 - ETA: 1s - loss: 6.0001 - acc: 0.626 - ETA: 1s - loss: 6.0069 - acc: 0.626 - ETA: 1s - loss: 6.0340 - acc: 0.624 - ETA: 1s - loss: 6.0251 - acc: 0.625 - ETA: 1s - loss: 6.0380 - acc: 0.624 - ETA: 1s - loss: 6.0433 - acc: 0.624 - ETA: 0s - loss: 6.0535 - acc: 0.623 - ETA: 0s - loss: 6.0370 - acc: 0.624 - ETA: 0s - loss: 6.0372 - acc: 0.624 - ETA: 0s - loss: 6.0389 - acc: 0.624 - ETA: 0s - loss: 6.0189 - acc: 0.625 - ETA: 0s - loss: 6.0270 - acc: 0.625 - ETA: 0s - loss: 6.0273 - acc: 0.625 - ETA: 0s - loss: 6.0364 - acc: 0.624 - ETA: 0s - loss: 6.0552 - acc: 0.623 - ETA: 0s - loss: 6.0578 - acc: 0.623 - ETA: 0s - loss: 6.0353 - acc: 0.624 - ETA: 0s - loss: 6.0313 - acc: 0.625 - ETA: 0s - loss: 6.0316 - acc: 0.625 - ETA: 0s - loss: 6.0476 - acc: 0.624 - ETA: 0s - loss: 6.0424 - acc: 0.624 - ETA: 0s - loss: 6.0363 - acc: 0.624 - ETA: 0s - loss: 6.0464 - acc: 0.624 - ETA: 0s - loss: 6.0414 - acc: 0.6244Epoch 00053: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 6.0318 - acc: 0.6250 - val_loss: 7.0479 - val_acc: 0.5006\n",
      "Epoch 55/100\n",
      "6580/6680 [============================>.] - ETA: 2s - loss: 4.0295 - acc: 0.750 - ETA: 3s - loss: 5.8313 - acc: 0.633 - ETA: 3s - loss: 5.9689 - acc: 0.622 - ETA: 3s - loss: 5.5215 - acc: 0.652 - ETA: 3s - loss: 5.5854 - acc: 0.650 - ETA: 3s - loss: 5.6554 - acc: 0.646 - ETA: 3s - loss: 5.7288 - acc: 0.642 - ETA: 3s - loss: 5.9130 - acc: 0.631 - ETA: 3s - loss: 6.0150 - acc: 0.625 - ETA: 3s - loss: 5.9650 - acc: 0.628 - ETA: 3s - loss: 5.9174 - acc: 0.631 - ETA: 2s - loss: 5.9935 - acc: 0.626 - ETA: 2s - loss: 5.9335 - acc: 0.630 - ETA: 2s - loss: 5.9117 - acc: 0.632 - ETA: 2s - loss: 5.9115 - acc: 0.632 - ETA: 2s - loss: 5.9730 - acc: 0.628 - ETA: 2s - loss: 5.8927 - acc: 0.633 - ETA: 2s - loss: 5.8761 - acc: 0.634 - ETA: 2s - loss: 5.8832 - acc: 0.634 - ETA: 2s - loss: 5.9517 - acc: 0.630 - ETA: 2s - loss: 6.0406 - acc: 0.624 - ETA: 2s - loss: 6.0734 - acc: 0.622 - ETA: 2s - loss: 6.0687 - acc: 0.622 - ETA: 2s - loss: 6.0111 - acc: 0.626 - ETA: 2s - loss: 5.9811 - acc: 0.628 - ETA: 2s - loss: 5.9682 - acc: 0.629 - ETA: 2s - loss: 5.9975 - acc: 0.627 - ETA: 1s - loss: 6.0051 - acc: 0.626 - ETA: 1s - loss: 5.9874 - acc: 0.628 - ETA: 1s - loss: 5.9761 - acc: 0.628 - ETA: 1s - loss: 5.9584 - acc: 0.629 - ETA: 1s - loss: 5.9585 - acc: 0.629 - ETA: 1s - loss: 5.9444 - acc: 0.630 - ETA: 1s - loss: 5.9310 - acc: 0.631 - ETA: 1s - loss: 5.9572 - acc: 0.630 - ETA: 1s - loss: 5.9340 - acc: 0.631 - ETA: 1s - loss: 5.9601 - acc: 0.629 - ETA: 1s - loss: 5.9604 - acc: 0.629 - ETA: 1s - loss: 5.9776 - acc: 0.628 - ETA: 1s - loss: 6.0123 - acc: 0.626 - ETA: 1s - loss: 6.0321 - acc: 0.625 - ETA: 1s - loss: 6.0361 - acc: 0.624 - ETA: 1s - loss: 6.0273 - acc: 0.625 - ETA: 1s - loss: 6.0242 - acc: 0.625 - ETA: 1s - loss: 6.0590 - acc: 0.623 - ETA: 0s - loss: 6.0570 - acc: 0.623 - ETA: 0s - loss: 6.0716 - acc: 0.622 - ETA: 0s - loss: 6.0793 - acc: 0.622 - ETA: 0s - loss: 6.0692 - acc: 0.622 - ETA: 0s - loss: 6.0828 - acc: 0.621 - ETA: 0s - loss: 6.0653 - acc: 0.622 - ETA: 0s - loss: 6.0740 - acc: 0.622 - ETA: 0s - loss: 6.0432 - acc: 0.624 - ETA: 0s - loss: 6.0447 - acc: 0.623 - ETA: 0s - loss: 6.0109 - acc: 0.626 - ETA: 0s - loss: 6.0198 - acc: 0.625 - ETA: 0s - loss: 6.0135 - acc: 0.625 - ETA: 0s - loss: 6.0247 - acc: 0.625 - ETA: 0s - loss: 6.0342 - acc: 0.624 - ETA: 0s - loss: 6.0176 - acc: 0.625 - ETA: 0s - loss: 6.0156 - acc: 0.625 - ETA: 0s - loss: 6.0148 - acc: 0.625 - ETA: 0s - loss: 6.0116 - acc: 0.6261Epoch 00054: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 6.0302 - acc: 0.6250 - val_loss: 7.0473 - val_acc: 0.5042\n",
      "Epoch 56/100\n",
      "6600/6680 [============================>.] - ETA: 2s - loss: 4.8354 - acc: 0.700 - ETA: 3s - loss: 6.0443 - acc: 0.625 - ETA: 3s - loss: 6.1542 - acc: 0.618 - ETA: 3s - loss: 6.2576 - acc: 0.611 - ETA: 3s - loss: 6.0809 - acc: 0.622 - ETA: 3s - loss: 5.8801 - acc: 0.635 - ETA: 3s - loss: 6.2763 - acc: 0.610 - ETA: 3s - loss: 6.2631 - acc: 0.610 - ETA: 2s - loss: 6.1924 - acc: 0.614 - ETA: 2s - loss: 6.1585 - acc: 0.617 - ETA: 2s - loss: 6.1262 - acc: 0.619 - ETA: 2s - loss: 6.1313 - acc: 0.618 - ETA: 2s - loss: 6.1675 - acc: 0.616 - ETA: 2s - loss: 6.0739 - acc: 0.622 - ETA: 3s - loss: 6.0289 - acc: 0.625 - ETA: 3s - loss: 6.0193 - acc: 0.626 - ETA: 3s - loss: 5.9995 - acc: 0.627 - ETA: 3s - loss: 5.9766 - acc: 0.628 - ETA: 3s - loss: 5.9553 - acc: 0.630 - ETA: 3s - loss: 5.9110 - acc: 0.632 - ETA: 4s - loss: 5.9199 - acc: 0.632 - ETA: 4s - loss: 5.8566 - acc: 0.636 - ETA: 4s - loss: 5.8629 - acc: 0.635 - ETA: 4s - loss: 5.8941 - acc: 0.633 - ETA: 4s - loss: 5.9136 - acc: 0.632 - ETA: 4s - loss: 5.9082 - acc: 0.633 - ETA: 5s - loss: 5.9188 - acc: 0.632 - ETA: 5s - loss: 5.9212 - acc: 0.632 - ETA: 4s - loss: 5.9257 - acc: 0.631 - ETA: 4s - loss: 5.9444 - acc: 0.630 - ETA: 4s - loss: 5.9134 - acc: 0.632 - ETA: 4s - loss: 5.8707 - acc: 0.635 - ETA: 4s - loss: 5.8438 - acc: 0.637 - ETA: 4s - loss: 5.8501 - acc: 0.636 - ETA: 4s - loss: 5.8968 - acc: 0.633 - ETA: 3s - loss: 5.9109 - acc: 0.633 - ETA: 3s - loss: 5.9509 - acc: 0.630 - ETA: 3s - loss: 5.9500 - acc: 0.630 - ETA: 3s - loss: 5.9417 - acc: 0.631 - ETA: 3s - loss: 5.9571 - acc: 0.630 - ETA: 3s - loss: 5.9854 - acc: 0.628 - ETA: 3s - loss: 5.9806 - acc: 0.628 - ETA: 3s - loss: 5.9655 - acc: 0.629 - ETA: 3s - loss: 5.9864 - acc: 0.628 - ETA: 2s - loss: 5.9765 - acc: 0.628 - ETA: 2s - loss: 5.9554 - acc: 0.630 - ETA: 2s - loss: 5.9204 - acc: 0.632 - ETA: 2s - loss: 5.9360 - acc: 0.631 - ETA: 2s - loss: 5.9829 - acc: 0.628 - ETA: 2s - loss: 5.9865 - acc: 0.628 - ETA: 2s - loss: 5.9796 - acc: 0.628 - ETA: 2s - loss: 5.9949 - acc: 0.627 - ETA: 2s - loss: 6.0362 - acc: 0.625 - ETA: 2s - loss: 6.0457 - acc: 0.624 - ETA: 1s - loss: 6.0730 - acc: 0.622 - ETA: 1s - loss: 6.0421 - acc: 0.624 - ETA: 1s - loss: 6.0419 - acc: 0.624 - ETA: 1s - loss: 6.0180 - acc: 0.626 - ETA: 1s - loss: 6.0135 - acc: 0.626 - ETA: 1s - loss: 6.0115 - acc: 0.626 - ETA: 1s - loss: 5.9797 - acc: 0.628 - ETA: 1s - loss: 5.9552 - acc: 0.630 - ETA: 1s - loss: 5.9315 - acc: 0.631 - ETA: 1s - loss: 5.9364 - acc: 0.631 - ETA: 1s - loss: 5.9308 - acc: 0.631 - ETA: 1s - loss: 5.9235 - acc: 0.632 - ETA: 0s - loss: 5.9253 - acc: 0.631 - ETA: 0s - loss: 5.9560 - acc: 0.630 - ETA: 0s - loss: 5.9658 - acc: 0.629 - ETA: 0s - loss: 5.9855 - acc: 0.628 - ETA: 0s - loss: 5.9961 - acc: 0.627 - ETA: 0s - loss: 5.9874 - acc: 0.628 - ETA: 0s - loss: 5.9844 - acc: 0.628 - ETA: 0s - loss: 6.0103 - acc: 0.626 - ETA: 0s - loss: 6.0237 - acc: 0.625 - ETA: 0s - loss: 6.0079 - acc: 0.626 - ETA: 0s - loss: 6.0267 - acc: 0.625 - ETA: 0s - loss: 6.0240 - acc: 0.625 - ETA: 0s - loss: 6.0279 - acc: 0.6255Epoch 00055: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 6.0287 - acc: 0.6253 - val_loss: 7.0959 - val_acc: 0.5066\n",
      "Epoch 57/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6660/6680 [============================>.] - ETA: 3s - loss: 6.4472 - acc: 0.600 - ETA: 4s - loss: 6.2605 - acc: 0.610 - ETA: 4s - loss: 6.5226 - acc: 0.594 - ETA: 4s - loss: 6.4162 - acc: 0.600 - ETA: 4s - loss: 6.5183 - acc: 0.594 - ETA: 4s - loss: 6.4558 - acc: 0.597 - ETA: 3s - loss: 6.4599 - acc: 0.596 - ETA: 3s - loss: 6.2680 - acc: 0.607 - ETA: 3s - loss: 6.2570 - acc: 0.606 - ETA: 3s - loss: 6.2154 - acc: 0.609 - ETA: 3s - loss: 6.1944 - acc: 0.610 - ETA: 3s - loss: 6.1923 - acc: 0.611 - ETA: 3s - loss: 6.1865 - acc: 0.610 - ETA: 3s - loss: 6.0953 - acc: 0.616 - ETA: 3s - loss: 6.0921 - acc: 0.616 - ETA: 3s - loss: 6.0240 - acc: 0.620 - ETA: 3s - loss: 5.9911 - acc: 0.622 - ETA: 3s - loss: 6.0043 - acc: 0.622 - ETA: 3s - loss: 6.0164 - acc: 0.621 - ETA: 3s - loss: 5.9672 - acc: 0.624 - ETA: 3s - loss: 6.0342 - acc: 0.620 - ETA: 2s - loss: 6.0173 - acc: 0.622 - ETA: 2s - loss: 6.0018 - acc: 0.623 - ETA: 2s - loss: 6.0274 - acc: 0.621 - ETA: 2s - loss: 6.1081 - acc: 0.617 - ETA: 2s - loss: 6.1161 - acc: 0.616 - ETA: 2s - loss: 6.0824 - acc: 0.618 - ETA: 2s - loss: 6.1012 - acc: 0.617 - ETA: 2s - loss: 6.1117 - acc: 0.617 - ETA: 2s - loss: 6.1520 - acc: 0.614 - ETA: 2s - loss: 6.1549 - acc: 0.614 - ETA: 2s - loss: 6.1285 - acc: 0.616 - ETA: 2s - loss: 6.0938 - acc: 0.618 - ETA: 2s - loss: 6.0755 - acc: 0.619 - ETA: 2s - loss: 6.0281 - acc: 0.623 - ETA: 2s - loss: 6.0116 - acc: 0.624 - ETA: 2s - loss: 6.0039 - acc: 0.623 - ETA: 2s - loss: 6.0371 - acc: 0.621 - ETA: 2s - loss: 6.0522 - acc: 0.620 - ETA: 1s - loss: 6.0496 - acc: 0.620 - ETA: 1s - loss: 6.0427 - acc: 0.620 - ETA: 1s - loss: 6.0545 - acc: 0.620 - ETA: 1s - loss: 6.0393 - acc: 0.621 - ETA: 1s - loss: 6.0218 - acc: 0.622 - ETA: 1s - loss: 6.0125 - acc: 0.622 - ETA: 1s - loss: 6.0331 - acc: 0.621 - ETA: 1s - loss: 6.0333 - acc: 0.621 - ETA: 1s - loss: 6.0315 - acc: 0.621 - ETA: 1s - loss: 6.0189 - acc: 0.622 - ETA: 1s - loss: 6.0213 - acc: 0.622 - ETA: 1s - loss: 6.0216 - acc: 0.622 - ETA: 1s - loss: 6.0246 - acc: 0.621 - ETA: 1s - loss: 6.0179 - acc: 0.622 - ETA: 1s - loss: 6.0219 - acc: 0.622 - ETA: 1s - loss: 6.0171 - acc: 0.622 - ETA: 1s - loss: 6.0052 - acc: 0.623 - ETA: 1s - loss: 6.0071 - acc: 0.622 - ETA: 1s - loss: 5.9906 - acc: 0.623 - ETA: 1s - loss: 6.0126 - acc: 0.622 - ETA: 1s - loss: 6.0114 - acc: 0.622 - ETA: 0s - loss: 6.0133 - acc: 0.622 - ETA: 0s - loss: 6.0172 - acc: 0.622 - ETA: 0s - loss: 6.0196 - acc: 0.621 - ETA: 0s - loss: 6.0052 - acc: 0.622 - ETA: 0s - loss: 6.0132 - acc: 0.622 - ETA: 0s - loss: 6.0428 - acc: 0.620 - ETA: 0s - loss: 6.0113 - acc: 0.622 - ETA: 0s - loss: 5.9927 - acc: 0.623 - ETA: 0s - loss: 6.0065 - acc: 0.622 - ETA: 0s - loss: 5.9861 - acc: 0.623 - ETA: 0s - loss: 5.9895 - acc: 0.623 - ETA: 0s - loss: 5.9888 - acc: 0.623 - ETA: 0s - loss: 5.9862 - acc: 0.623 - ETA: 0s - loss: 5.9850 - acc: 0.623 - ETA: 0s - loss: 5.9793 - acc: 0.623 - ETA: 0s - loss: 5.9838 - acc: 0.6234Epoch 00056: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 5.9803 - acc: 0.6237 - val_loss: 7.3016 - val_acc: 0.4850\n",
      "Epoch 58/100\n",
      "6660/6680 [============================>.] - ETA: 6s - loss: 6.0429 - acc: 0.600 - ETA: 3s - loss: 7.7242 - acc: 0.516 - ETA: 3s - loss: 6.9634 - acc: 0.562 - ETA: 3s - loss: 6.8809 - acc: 0.569 - ETA: 3s - loss: 6.5418 - acc: 0.591 - ETA: 3s - loss: 6.5297 - acc: 0.590 - ETA: 3s - loss: 6.3753 - acc: 0.600 - ETA: 3s - loss: 6.3685 - acc: 0.600 - ETA: 3s - loss: 6.4155 - acc: 0.595 - ETA: 3s - loss: 6.3742 - acc: 0.597 - ETA: 2s - loss: 6.2680 - acc: 0.604 - ETA: 2s - loss: 6.3247 - acc: 0.600 - ETA: 2s - loss: 6.3237 - acc: 0.600 - ETA: 2s - loss: 6.2618 - acc: 0.605 - ETA: 2s - loss: 6.1888 - acc: 0.609 - ETA: 2s - loss: 6.1781 - acc: 0.610 - ETA: 2s - loss: 6.1204 - acc: 0.613 - ETA: 3s - loss: 6.1363 - acc: 0.612 - ETA: 3s - loss: 6.1210 - acc: 0.613 - ETA: 3s - loss: 6.0891 - acc: 0.615 - ETA: 2s - loss: 6.1049 - acc: 0.614 - ETA: 2s - loss: 6.0514 - acc: 0.618 - ETA: 2s - loss: 6.1115 - acc: 0.615 - ETA: 2s - loss: 6.1555 - acc: 0.612 - ETA: 2s - loss: 6.1825 - acc: 0.610 - ETA: 2s - loss: 6.2114 - acc: 0.609 - ETA: 2s - loss: 6.1776 - acc: 0.611 - ETA: 2s - loss: 6.2075 - acc: 0.609 - ETA: 2s - loss: 6.1856 - acc: 0.611 - ETA: 2s - loss: 6.1812 - acc: 0.611 - ETA: 2s - loss: 6.1412 - acc: 0.614 - ETA: 2s - loss: 6.0955 - acc: 0.616 - ETA: 2s - loss: 6.0705 - acc: 0.618 - ETA: 2s - loss: 6.0983 - acc: 0.617 - ETA: 2s - loss: 6.0670 - acc: 0.618 - ETA: 2s - loss: 6.0385 - acc: 0.620 - ETA: 2s - loss: 6.0098 - acc: 0.622 - ETA: 2s - loss: 6.0375 - acc: 0.620 - ETA: 2s - loss: 6.0358 - acc: 0.620 - ETA: 2s - loss: 6.0218 - acc: 0.621 - ETA: 2s - loss: 6.0108 - acc: 0.622 - ETA: 1s - loss: 6.0077 - acc: 0.622 - ETA: 1s - loss: 5.9978 - acc: 0.623 - ETA: 1s - loss: 5.9612 - acc: 0.625 - ETA: 1s - loss: 5.9330 - acc: 0.627 - ETA: 1s - loss: 5.9458 - acc: 0.626 - ETA: 1s - loss: 5.9450 - acc: 0.626 - ETA: 1s - loss: 5.9698 - acc: 0.625 - ETA: 1s - loss: 5.9478 - acc: 0.626 - ETA: 1s - loss: 5.9296 - acc: 0.627 - ETA: 1s - loss: 5.9235 - acc: 0.628 - ETA: 1s - loss: 5.9115 - acc: 0.628 - ETA: 1s - loss: 5.9029 - acc: 0.629 - ETA: 1s - loss: 5.9277 - acc: 0.627 - ETA: 1s - loss: 5.9503 - acc: 0.626 - ETA: 1s - loss: 5.9554 - acc: 0.625 - ETA: 1s - loss: 5.9445 - acc: 0.626 - ETA: 1s - loss: 5.9192 - acc: 0.627 - ETA: 0s - loss: 5.9211 - acc: 0.627 - ETA: 0s - loss: 5.9309 - acc: 0.626 - ETA: 0s - loss: 5.9322 - acc: 0.626 - ETA: 0s - loss: 5.9448 - acc: 0.625 - ETA: 0s - loss: 5.9326 - acc: 0.626 - ETA: 0s - loss: 5.9395 - acc: 0.626 - ETA: 0s - loss: 5.9354 - acc: 0.626 - ETA: 0s - loss: 5.9359 - acc: 0.626 - ETA: 0s - loss: 5.9266 - acc: 0.627 - ETA: 0s - loss: 5.9238 - acc: 0.627 - ETA: 0s - loss: 5.9218 - acc: 0.627 - ETA: 0s - loss: 5.9370 - acc: 0.626 - ETA: 0s - loss: 5.9239 - acc: 0.627 - ETA: 0s - loss: 5.9336 - acc: 0.627 - ETA: 0s - loss: 5.9283 - acc: 0.6276Epoch 00057: val_loss improved from 7.04041 to 7.02980, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 5.9251 - acc: 0.6278 - val_loss: 7.0298 - val_acc: 0.5030\n",
      "Epoch 59/100\n",
      "6620/6680 [============================>.] - ETA: 5s - loss: 6.4472 - acc: 0.600 - ETA: 3s - loss: 6.5818 - acc: 0.591 - ETA: 3s - loss: 6.6713 - acc: 0.581 - ETA: 3s - loss: 6.4522 - acc: 0.596 - ETA: 3s - loss: 6.4915 - acc: 0.595 - ETA: 3s - loss: 6.5165 - acc: 0.592 - ETA: 3s - loss: 6.1929 - acc: 0.611 - ETA: 3s - loss: 6.3178 - acc: 0.604 - ETA: 3s - loss: 6.0485 - acc: 0.621 - ETA: 4s - loss: 6.1145 - acc: 0.616 - ETA: 4s - loss: 6.1842 - acc: 0.612 - ETA: 4s - loss: 6.1577 - acc: 0.614 - ETA: 4s - loss: 6.0892 - acc: 0.618 - ETA: 4s - loss: 5.9847 - acc: 0.625 - ETA: 4s - loss: 5.9708 - acc: 0.626 - ETA: 3s - loss: 6.0394 - acc: 0.622 - ETA: 3s - loss: 6.0343 - acc: 0.622 - ETA: 3s - loss: 6.0417 - acc: 0.621 - ETA: 3s - loss: 6.0093 - acc: 0.623 - ETA: 3s - loss: 5.9978 - acc: 0.624 - ETA: 3s - loss: 6.0158 - acc: 0.623 - ETA: 3s - loss: 5.9461 - acc: 0.627 - ETA: 3s - loss: 5.9095 - acc: 0.629 - ETA: 2s - loss: 5.9459 - acc: 0.627 - ETA: 2s - loss: 5.8684 - acc: 0.631 - ETA: 2s - loss: 5.8261 - acc: 0.634 - ETA: 2s - loss: 5.8188 - acc: 0.634 - ETA: 2s - loss: 5.8265 - acc: 0.634 - ETA: 2s - loss: 5.7898 - acc: 0.636 - ETA: 2s - loss: 5.7903 - acc: 0.636 - ETA: 2s - loss: 5.8454 - acc: 0.632 - ETA: 2s - loss: 5.9055 - acc: 0.629 - ETA: 2s - loss: 5.9000 - acc: 0.629 - ETA: 2s - loss: 5.9170 - acc: 0.628 - ETA: 1s - loss: 5.9231 - acc: 0.627 - ETA: 1s - loss: 5.8939 - acc: 0.629 - ETA: 1s - loss: 5.8989 - acc: 0.629 - ETA: 1s - loss: 5.8920 - acc: 0.629 - ETA: 1s - loss: 5.8741 - acc: 0.631 - ETA: 1s - loss: 5.8640 - acc: 0.631 - ETA: 1s - loss: 5.8986 - acc: 0.629 - ETA: 1s - loss: 5.9154 - acc: 0.628 - ETA: 1s - loss: 5.8975 - acc: 0.629 - ETA: 1s - loss: 5.9027 - acc: 0.629 - ETA: 1s - loss: 5.9210 - acc: 0.628 - ETA: 1s - loss: 5.8997 - acc: 0.629 - ETA: 1s - loss: 5.8735 - acc: 0.631 - ETA: 1s - loss: 5.8839 - acc: 0.631 - ETA: 1s - loss: 5.8888 - acc: 0.630 - ETA: 0s - loss: 5.8775 - acc: 0.631 - ETA: 0s - loss: 5.8693 - acc: 0.632 - ETA: 0s - loss: 5.8747 - acc: 0.631 - ETA: 0s - loss: 5.8937 - acc: 0.630 - ETA: 0s - loss: 5.8831 - acc: 0.631 - ETA: 0s - loss: 5.8808 - acc: 0.631 - ETA: 0s - loss: 5.8794 - acc: 0.631 - ETA: 0s - loss: 5.8696 - acc: 0.632 - ETA: 0s - loss: 5.8887 - acc: 0.630 - ETA: 0s - loss: 5.9108 - acc: 0.629 - ETA: 0s - loss: 5.9295 - acc: 0.628 - ETA: 0s - loss: 5.9297 - acc: 0.628 - ETA: 0s - loss: 5.9174 - acc: 0.629 - ETA: 0s - loss: 5.9012 - acc: 0.630 - ETA: 0s - loss: 5.9021 - acc: 0.630 - ETA: 0s - loss: 5.9073 - acc: 0.6298Epoch 00058: val_loss improved from 7.02980 to 7.01386, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 5.9098 - acc: 0.6296 - val_loss: 7.0139 - val_acc: 0.5066\n",
      "Epoch 60/100\n",
      "6600/6680 [============================>.] - ETA: 3s - loss: 4.8354 - acc: 0.700 - ETA: 3s - loss: 6.0396 - acc: 0.614 - ETA: 3s - loss: 5.8947 - acc: 0.627 - ETA: 3s - loss: 5.7148 - acc: 0.640 - ETA: 3s - loss: 5.8510 - acc: 0.633 - ETA: 3s - loss: 5.6247 - acc: 0.648 - ETA: 3s - loss: 5.4454 - acc: 0.659 - ETA: 3s - loss: 5.5435 - acc: 0.654 - ETA: 3s - loss: 5.4402 - acc: 0.660 - ETA: 3s - loss: 5.4150 - acc: 0.662 - ETA: 2s - loss: 5.4731 - acc: 0.658 - ETA: 2s - loss: 5.5542 - acc: 0.653 - ETA: 2s - loss: 5.5739 - acc: 0.652 - ETA: 2s - loss: 5.6864 - acc: 0.644 - ETA: 2s - loss: 5.7024 - acc: 0.643 - ETA: 2s - loss: 5.7822 - acc: 0.639 - ETA: 2s - loss: 5.8265 - acc: 0.636 - ETA: 2s - loss: 5.7866 - acc: 0.638 - ETA: 2s - loss: 5.7827 - acc: 0.638 - ETA: 2s - loss: 5.8084 - acc: 0.636 - ETA: 2s - loss: 5.8629 - acc: 0.633 - ETA: 2s - loss: 5.8630 - acc: 0.633 - ETA: 2s - loss: 5.8533 - acc: 0.633 - ETA: 2s - loss: 5.8294 - acc: 0.635 - ETA: 2s - loss: 5.8425 - acc: 0.634 - ETA: 2s - loss: 5.8360 - acc: 0.634 - ETA: 2s - loss: 5.9049 - acc: 0.630 - ETA: 2s - loss: 5.8841 - acc: 0.631 - ETA: 1s - loss: 5.8598 - acc: 0.632 - ETA: 1s - loss: 5.8103 - acc: 0.635 - ETA: 1s - loss: 5.8345 - acc: 0.634 - ETA: 1s - loss: 5.8349 - acc: 0.634 - ETA: 1s - loss: 5.8105 - acc: 0.635 - ETA: 1s - loss: 5.8557 - acc: 0.633 - ETA: 1s - loss: 5.8632 - acc: 0.632 - ETA: 1s - loss: 5.9010 - acc: 0.630 - ETA: 1s - loss: 5.8688 - acc: 0.632 - ETA: 1s - loss: 5.8460 - acc: 0.633 - ETA: 1s - loss: 5.8182 - acc: 0.635 - ETA: 1s - loss: 5.7951 - acc: 0.636 - ETA: 1s - loss: 5.7899 - acc: 0.636 - ETA: 1s - loss: 5.7827 - acc: 0.637 - ETA: 1s - loss: 5.7980 - acc: 0.636 - ETA: 1s - loss: 5.7729 - acc: 0.637 - ETA: 1s - loss: 5.7948 - acc: 0.636 - ETA: 1s - loss: 5.8500 - acc: 0.633 - ETA: 1s - loss: 5.8794 - acc: 0.631 - ETA: 0s - loss: 5.8407 - acc: 0.634 - ETA: 0s - loss: 5.8625 - acc: 0.632 - ETA: 0s - loss: 5.8516 - acc: 0.633 - ETA: 0s - loss: 5.8539 - acc: 0.633 - ETA: 0s - loss: 5.8611 - acc: 0.632 - ETA: 0s - loss: 5.8574 - acc: 0.632 - ETA: 0s - loss: 5.8595 - acc: 0.632 - ETA: 0s - loss: 5.8752 - acc: 0.631 - ETA: 0s - loss: 5.8823 - acc: 0.631 - ETA: 0s - loss: 5.8831 - acc: 0.631 - ETA: 0s - loss: 5.8764 - acc: 0.631 - ETA: 0s - loss: 5.8742 - acc: 0.631 - ETA: 0s - loss: 5.8757 - acc: 0.631 - ETA: 0s - loss: 5.8848 - acc: 0.631 - ETA: 0s - loss: 5.8962 - acc: 0.630 - ETA: 0s - loss: 5.9064 - acc: 0.629 - ETA: 0s - loss: 5.8972 - acc: 0.6305Epoch 00059: val_loss improved from 7.01386 to 6.96304, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 5.9014 - acc: 0.6302 - val_loss: 6.9630 - val_acc: 0.5090\n",
      "Epoch 61/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6640/6680 [============================>.] - ETA: 2s - loss: 6.4472 - acc: 0.600 - ETA: 3s - loss: 5.9100 - acc: 0.633 - ETA: 3s - loss: 5.9372 - acc: 0.631 - ETA: 3s - loss: 5.5499 - acc: 0.655 - ETA: 3s - loss: 6.0473 - acc: 0.625 - ETA: 3s - loss: 6.2111 - acc: 0.614 - ETA: 3s - loss: 6.2229 - acc: 0.614 - ETA: 3s - loss: 6.0578 - acc: 0.624 - ETA: 3s - loss: 5.9124 - acc: 0.633 - ETA: 3s - loss: 5.6611 - acc: 0.648 - ETA: 3s - loss: 5.6774 - acc: 0.647 - ETA: 4s - loss: 5.6932 - acc: 0.646 - ETA: 4s - loss: 5.7057 - acc: 0.646 - ETA: 4s - loss: 5.6303 - acc: 0.650 - ETA: 3s - loss: 5.6842 - acc: 0.647 - ETA: 3s - loss: 5.6445 - acc: 0.650 - ETA: 3s - loss: 5.5976 - acc: 0.652 - ETA: 3s - loss: 5.5912 - acc: 0.652 - ETA: 3s - loss: 5.5857 - acc: 0.652 - ETA: 3s - loss: 5.6473 - acc: 0.648 - ETA: 3s - loss: 5.7026 - acc: 0.645 - ETA: 3s - loss: 5.8139 - acc: 0.638 - ETA: 3s - loss: 5.8465 - acc: 0.636 - ETA: 3s - loss: 5.8145 - acc: 0.638 - ETA: 3s - loss: 5.8215 - acc: 0.637 - ETA: 3s - loss: 5.8076 - acc: 0.638 - ETA: 3s - loss: 5.7916 - acc: 0.639 - ETA: 3s - loss: 5.7462 - acc: 0.642 - ETA: 3s - loss: 5.7163 - acc: 0.644 - ETA: 3s - loss: 5.6729 - acc: 0.647 - ETA: 3s - loss: 5.6989 - acc: 0.645 - ETA: 3s - loss: 5.7044 - acc: 0.644 - ETA: 2s - loss: 5.7327 - acc: 0.642 - ETA: 2s - loss: 5.7288 - acc: 0.642 - ETA: 2s - loss: 5.7527 - acc: 0.641 - ETA: 2s - loss: 5.7649 - acc: 0.640 - ETA: 2s - loss: 5.7969 - acc: 0.638 - ETA: 2s - loss: 5.7950 - acc: 0.639 - ETA: 2s - loss: 5.7981 - acc: 0.638 - ETA: 2s - loss: 5.8310 - acc: 0.636 - ETA: 2s - loss: 5.8260 - acc: 0.636 - ETA: 2s - loss: 5.8393 - acc: 0.636 - ETA: 2s - loss: 5.8292 - acc: 0.636 - ETA: 2s - loss: 5.8109 - acc: 0.638 - ETA: 2s - loss: 5.8192 - acc: 0.637 - ETA: 2s - loss: 5.8365 - acc: 0.636 - ETA: 2s - loss: 5.8550 - acc: 0.635 - ETA: 1s - loss: 5.8299 - acc: 0.636 - ETA: 1s - loss: 5.8177 - acc: 0.637 - ETA: 1s - loss: 5.8278 - acc: 0.637 - ETA: 1s - loss: 5.8679 - acc: 0.634 - ETA: 1s - loss: 5.8796 - acc: 0.633 - ETA: 1s - loss: 5.8640 - acc: 0.634 - ETA: 1s - loss: 5.8810 - acc: 0.633 - ETA: 1s - loss: 5.8986 - acc: 0.632 - ETA: 1s - loss: 5.8932 - acc: 0.632 - ETA: 1s - loss: 5.8947 - acc: 0.632 - ETA: 1s - loss: 5.9025 - acc: 0.632 - ETA: 1s - loss: 5.8883 - acc: 0.633 - ETA: 0s - loss: 5.8927 - acc: 0.632 - ETA: 0s - loss: 5.9030 - acc: 0.632 - ETA: 0s - loss: 5.9002 - acc: 0.632 - ETA: 0s - loss: 5.9192 - acc: 0.631 - ETA: 0s - loss: 5.9322 - acc: 0.630 - ETA: 0s - loss: 5.9189 - acc: 0.631 - ETA: 0s - loss: 5.9170 - acc: 0.631 - ETA: 0s - loss: 5.9017 - acc: 0.632 - ETA: 0s - loss: 5.8948 - acc: 0.632 - ETA: 0s - loss: 5.8960 - acc: 0.632 - ETA: 0s - loss: 5.8951 - acc: 0.632 - ETA: 0s - loss: 5.9037 - acc: 0.632 - ETA: 0s - loss: 5.8869 - acc: 0.633 - ETA: 0s - loss: 5.8954 - acc: 0.6325Epoch 00060: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 5.8915 - acc: 0.6328 - val_loss: 7.0350 - val_acc: 0.5114\n",
      "Epoch 62/100\n",
      "6600/6680 [============================>.] - ETA: 3s - loss: 7.2531 - acc: 0.550 - ETA: 3s - loss: 7.3683 - acc: 0.542 - ETA: 3s - loss: 6.6488 - acc: 0.587 - ETA: 3s - loss: 6.1628 - acc: 0.617 - ETA: 3s - loss: 5.7879 - acc: 0.640 - ETA: 3s - loss: 5.9398 - acc: 0.631 - ETA: 3s - loss: 6.0948 - acc: 0.621 - ETA: 3s - loss: 6.2295 - acc: 0.613 - ETA: 3s - loss: 6.2363 - acc: 0.613 - ETA: 2s - loss: 6.1619 - acc: 0.617 - ETA: 2s - loss: 6.2041 - acc: 0.615 - ETA: 2s - loss: 6.1564 - acc: 0.617 - ETA: 2s - loss: 6.1028 - acc: 0.620 - ETA: 2s - loss: 6.1233 - acc: 0.619 - ETA: 2s - loss: 6.0451 - acc: 0.624 - ETA: 2s - loss: 6.0294 - acc: 0.624 - ETA: 2s - loss: 6.1226 - acc: 0.618 - ETA: 2s - loss: 6.0312 - acc: 0.624 - ETA: 2s - loss: 6.0064 - acc: 0.626 - ETA: 2s - loss: 5.9041 - acc: 0.632 - ETA: 2s - loss: 5.9914 - acc: 0.627 - ETA: 2s - loss: 6.0278 - acc: 0.624 - ETA: 2s - loss: 5.9731 - acc: 0.628 - ETA: 2s - loss: 5.9660 - acc: 0.628 - ETA: 2s - loss: 5.9215 - acc: 0.631 - ETA: 2s - loss: 5.9271 - acc: 0.630 - ETA: 2s - loss: 5.9324 - acc: 0.630 - ETA: 1s - loss: 5.9392 - acc: 0.630 - ETA: 1s - loss: 5.9598 - acc: 0.628 - ETA: 1s - loss: 5.9338 - acc: 0.630 - ETA: 1s - loss: 5.9601 - acc: 0.628 - ETA: 1s - loss: 5.9422 - acc: 0.629 - ETA: 1s - loss: 5.9316 - acc: 0.630 - ETA: 1s - loss: 5.9374 - acc: 0.629 - ETA: 1s - loss: 5.9650 - acc: 0.628 - ETA: 1s - loss: 6.0022 - acc: 0.625 - ETA: 1s - loss: 5.9970 - acc: 0.626 - ETA: 1s - loss: 5.9867 - acc: 0.626 - ETA: 1s - loss: 5.9844 - acc: 0.626 - ETA: 1s - loss: 5.9656 - acc: 0.627 - ETA: 1s - loss: 5.9693 - acc: 0.627 - ETA: 1s - loss: 5.9507 - acc: 0.628 - ETA: 1s - loss: 5.9479 - acc: 0.628 - ETA: 1s - loss: 5.9305 - acc: 0.630 - ETA: 1s - loss: 5.9036 - acc: 0.631 - ETA: 0s - loss: 5.8822 - acc: 0.633 - ETA: 0s - loss: 5.8914 - acc: 0.632 - ETA: 0s - loss: 5.8732 - acc: 0.633 - ETA: 0s - loss: 5.8750 - acc: 0.632 - ETA: 0s - loss: 5.8395 - acc: 0.635 - ETA: 0s - loss: 5.8328 - acc: 0.635 - ETA: 0s - loss: 5.8345 - acc: 0.635 - ETA: 0s - loss: 5.8514 - acc: 0.634 - ETA: 0s - loss: 5.8821 - acc: 0.632 - ETA: 0s - loss: 5.8996 - acc: 0.631 - ETA: 0s - loss: 5.8924 - acc: 0.632 - ETA: 0s - loss: 5.8955 - acc: 0.632 - ETA: 0s - loss: 5.8958 - acc: 0.632 - ETA: 0s - loss: 5.8865 - acc: 0.632 - ETA: 0s - loss: 5.8878 - acc: 0.632 - ETA: 0s - loss: 5.8789 - acc: 0.633 - ETA: 0s - loss: 5.9067 - acc: 0.631 - ETA: 0s - loss: 5.8929 - acc: 0.6324Epoch 00061: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 5.8899 - acc: 0.6326 - val_loss: 7.0502 - val_acc: 0.4910\n",
      "Epoch 63/100\n",
      "6660/6680 [============================>.] - ETA: 2s - loss: 4.8365 - acc: 0.700 - ETA: 3s - loss: 5.7911 - acc: 0.633 - ETA: 3s - loss: 6.2380 - acc: 0.609 - ETA: 3s - loss: 6.2174 - acc: 0.611 - ETA: 3s - loss: 6.1653 - acc: 0.611 - ETA: 3s - loss: 6.0683 - acc: 0.618 - ETA: 3s - loss: 6.0519 - acc: 0.620 - ETA: 3s - loss: 6.1271 - acc: 0.616 - ETA: 2s - loss: 5.9281 - acc: 0.629 - ETA: 2s - loss: 5.8986 - acc: 0.631 - ETA: 2s - loss: 5.9504 - acc: 0.628 - ETA: 2s - loss: 6.0071 - acc: 0.625 - ETA: 2s - loss: 5.9141 - acc: 0.631 - ETA: 2s - loss: 5.7756 - acc: 0.639 - ETA: 2s - loss: 5.8106 - acc: 0.637 - ETA: 2s - loss: 5.8669 - acc: 0.634 - ETA: 2s - loss: 5.8339 - acc: 0.636 - ETA: 2s - loss: 5.7497 - acc: 0.641 - ETA: 2s - loss: 5.7888 - acc: 0.639 - ETA: 2s - loss: 5.7979 - acc: 0.638 - ETA: 2s - loss: 5.7823 - acc: 0.639 - ETA: 2s - loss: 5.7908 - acc: 0.639 - ETA: 2s - loss: 5.7543 - acc: 0.641 - ETA: 2s - loss: 5.7351 - acc: 0.642 - ETA: 2s - loss: 5.7313 - acc: 0.643 - ETA: 2s - loss: 5.7334 - acc: 0.643 - ETA: 2s - loss: 5.7352 - acc: 0.643 - ETA: 2s - loss: 5.7662 - acc: 0.641 - ETA: 1s - loss: 5.7785 - acc: 0.640 - ETA: 1s - loss: 5.7466 - acc: 0.642 - ETA: 1s - loss: 5.8008 - acc: 0.638 - ETA: 1s - loss: 5.8257 - acc: 0.637 - ETA: 1s - loss: 5.8443 - acc: 0.635 - ETA: 1s - loss: 5.8315 - acc: 0.636 - ETA: 1s - loss: 5.8808 - acc: 0.633 - ETA: 1s - loss: 5.9052 - acc: 0.631 - ETA: 1s - loss: 5.8766 - acc: 0.633 - ETA: 1s - loss: 5.9147 - acc: 0.631 - ETA: 1s - loss: 5.9612 - acc: 0.628 - ETA: 1s - loss: 5.9118 - acc: 0.631 - ETA: 1s - loss: 5.9247 - acc: 0.630 - ETA: 1s - loss: 5.9221 - acc: 0.630 - ETA: 1s - loss: 5.9119 - acc: 0.631 - ETA: 1s - loss: 5.9143 - acc: 0.631 - ETA: 1s - loss: 5.8977 - acc: 0.632 - ETA: 1s - loss: 5.8853 - acc: 0.633 - ETA: 1s - loss: 5.8894 - acc: 0.632 - ETA: 0s - loss: 5.8976 - acc: 0.632 - ETA: 0s - loss: 5.8925 - acc: 0.632 - ETA: 0s - loss: 5.8684 - acc: 0.634 - ETA: 0s - loss: 5.8484 - acc: 0.635 - ETA: 0s - loss: 5.8629 - acc: 0.634 - ETA: 0s - loss: 5.8560 - acc: 0.634 - ETA: 0s - loss: 5.8462 - acc: 0.635 - ETA: 0s - loss: 5.8721 - acc: 0.633 - ETA: 0s - loss: 5.8831 - acc: 0.633 - ETA: 0s - loss: 5.8565 - acc: 0.634 - ETA: 0s - loss: 5.8550 - acc: 0.634 - ETA: 0s - loss: 5.8622 - acc: 0.634 - ETA: 0s - loss: 5.8747 - acc: 0.633 - ETA: 0s - loss: 5.8788 - acc: 0.633 - ETA: 0s - loss: 5.8780 - acc: 0.633 - ETA: 0s - loss: 5.8719 - acc: 0.633 - ETA: 0s - loss: 5.8782 - acc: 0.633 - ETA: 0s - loss: 5.8942 - acc: 0.632 - ETA: 0s - loss: 5.8936 - acc: 0.6323Epoch 00062: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 5.8905 - acc: 0.6325 - val_loss: 7.0385 - val_acc: 0.5066\n",
      "Epoch 64/100\n",
      "6660/6680 [============================>.] - ETA: 3s - loss: 7.2531 - acc: 0.550 - ETA: 3s - loss: 6.7159 - acc: 0.583 - ETA: 3s - loss: 6.6890 - acc: 0.585 - ETA: 3s - loss: 6.6845 - acc: 0.580 - ETA: 3s - loss: 6.5497 - acc: 0.589 - ETA: 3s - loss: 6.3567 - acc: 0.602 - ETA: 3s - loss: 6.2579 - acc: 0.608 - ETA: 3s - loss: 6.1401 - acc: 0.616 - ETA: 3s - loss: 6.0532 - acc: 0.622 - ETA: 3s - loss: 6.1178 - acc: 0.618 - ETA: 3s - loss: 6.1859 - acc: 0.614 - ETA: 3s - loss: 6.1802 - acc: 0.615 - ETA: 3s - loss: 6.1477 - acc: 0.617 - ETA: 3s - loss: 6.3250 - acc: 0.606 - ETA: 2s - loss: 6.2392 - acc: 0.611 - ETA: 2s - loss: 6.1099 - acc: 0.619 - ETA: 2s - loss: 6.0902 - acc: 0.621 - ETA: 2s - loss: 6.1700 - acc: 0.616 - ETA: 2s - loss: 6.1312 - acc: 0.618 - ETA: 2s - loss: 6.0789 - acc: 0.622 - ETA: 2s - loss: 6.0849 - acc: 0.621 - ETA: 2s - loss: 6.0637 - acc: 0.623 - ETA: 2s - loss: 5.9925 - acc: 0.627 - ETA: 2s - loss: 5.9701 - acc: 0.628 - ETA: 2s - loss: 5.9495 - acc: 0.630 - ETA: 2s - loss: 5.9436 - acc: 0.630 - ETA: 2s - loss: 5.9631 - acc: 0.629 - ETA: 2s - loss: 5.8593 - acc: 0.635 - ETA: 2s - loss: 5.8614 - acc: 0.635 - ETA: 2s - loss: 5.8470 - acc: 0.636 - ETA: 1s - loss: 5.8826 - acc: 0.634 - ETA: 1s - loss: 5.8820 - acc: 0.634 - ETA: 1s - loss: 5.8854 - acc: 0.633 - ETA: 1s - loss: 5.8927 - acc: 0.633 - ETA: 1s - loss: 5.8995 - acc: 0.633 - ETA: 1s - loss: 5.9150 - acc: 0.632 - ETA: 1s - loss: 5.9253 - acc: 0.631 - ETA: 1s - loss: 5.9393 - acc: 0.630 - ETA: 1s - loss: 5.9568 - acc: 0.629 - ETA: 1s - loss: 5.9782 - acc: 0.628 - ETA: 1s - loss: 5.9433 - acc: 0.630 - ETA: 1s - loss: 5.9634 - acc: 0.629 - ETA: 1s - loss: 5.9596 - acc: 0.629 - ETA: 1s - loss: 5.9597 - acc: 0.629 - ETA: 1s - loss: 5.9708 - acc: 0.628 - ETA: 1s - loss: 5.9514 - acc: 0.629 - ETA: 1s - loss: 5.9481 - acc: 0.629 - ETA: 1s - loss: 5.9222 - acc: 0.631 - ETA: 1s - loss: 5.9130 - acc: 0.632 - ETA: 0s - loss: 5.9174 - acc: 0.631 - ETA: 0s - loss: 5.9259 - acc: 0.631 - ETA: 0s - loss: 5.9436 - acc: 0.630 - ETA: 0s - loss: 5.9228 - acc: 0.631 - ETA: 0s - loss: 5.9308 - acc: 0.630 - ETA: 0s - loss: 5.9215 - acc: 0.631 - ETA: 0s - loss: 5.9143 - acc: 0.631 - ETA: 0s - loss: 5.9291 - acc: 0.630 - ETA: 0s - loss: 5.9454 - acc: 0.629 - ETA: 0s - loss: 5.9403 - acc: 0.630 - ETA: 0s - loss: 5.9218 - acc: 0.631 - ETA: 0s - loss: 5.9235 - acc: 0.631 - ETA: 0s - loss: 5.9215 - acc: 0.631 - ETA: 0s - loss: 5.9095 - acc: 0.632 - ETA: 0s - loss: 5.9024 - acc: 0.632 - ETA: 0s - loss: 5.9255 - acc: 0.631 - ETA: 0s - loss: 5.9270 - acc: 0.631 - ETA: 0s - loss: 5.9130 - acc: 0.631 - ETA: 0s - loss: 5.9198 - acc: 0.631 - ETA: 0s - loss: 5.9188 - acc: 0.631 - ETA: 0s - loss: 5.9030 - acc: 0.632 - ETA: 0s - loss: 5.8973 - acc: 0.632 - ETA: 0s - loss: 5.8910 - acc: 0.6333Epoch 00063: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 5.8879 - acc: 0.6335 - val_loss: 7.0033 - val_acc: 0.5138\n",
      "Epoch 65/100\n",
      "6640/6680 [============================>.] - ETA: 4s - loss: 4.8354 - acc: 0.700 - ETA: 3s - loss: 5.6416 - acc: 0.650 - ETA: 3s - loss: 5.8833 - acc: 0.635 - ETA: 3s - loss: 5.6414 - acc: 0.650 - ETA: 3s - loss: 5.5608 - acc: 0.655 - ETA: 3s - loss: 5.6414 - acc: 0.650 - ETA: 3s - loss: 5.8479 - acc: 0.635 - ETA: 3s - loss: 5.9884 - acc: 0.627 - ETA: 3s - loss: 5.9727 - acc: 0.628 - ETA: 3s - loss: 5.9011 - acc: 0.632 - ETA: 3s - loss: 5.8651 - acc: 0.635 - ETA: 3s - loss: 5.8968 - acc: 0.633 - ETA: 3s - loss: 5.8674 - acc: 0.634 - ETA: 3s - loss: 5.9913 - acc: 0.626 - ETA: 3s - loss: 5.9948 - acc: 0.626 - ETA: 4s - loss: 5.9890 - acc: 0.627 - ETA: 4s - loss: 5.9979 - acc: 0.626 - ETA: 4s - loss: 5.9137 - acc: 0.631 - ETA: 4s - loss: 5.9098 - acc: 0.632 - ETA: 4s - loss: 5.9287 - acc: 0.631 - ETA: 5s - loss: 5.9136 - acc: 0.631 - ETA: 5s - loss: 5.8919 - acc: 0.633 - ETA: 4s - loss: 5.9303 - acc: 0.631 - ETA: 4s - loss: 5.9066 - acc: 0.632 - ETA: 4s - loss: 5.8858 - acc: 0.633 - ETA: 4s - loss: 5.9191 - acc: 0.631 - ETA: 4s - loss: 5.9463 - acc: 0.629 - ETA: 4s - loss: 5.9472 - acc: 0.629 - ETA: 3s - loss: 5.9705 - acc: 0.628 - ETA: 3s - loss: 5.9805 - acc: 0.627 - ETA: 3s - loss: 5.9867 - acc: 0.627 - ETA: 3s - loss: 5.9325 - acc: 0.631 - ETA: 3s - loss: 5.9593 - acc: 0.629 - ETA: 3s - loss: 5.9472 - acc: 0.630 - ETA: 3s - loss: 5.9537 - acc: 0.629 - ETA: 3s - loss: 5.9523 - acc: 0.629 - ETA: 2s - loss: 5.9472 - acc: 0.630 - ETA: 2s - loss: 5.9584 - acc: 0.629 - ETA: 2s - loss: 5.9347 - acc: 0.631 - ETA: 2s - loss: 5.9273 - acc: 0.631 - ETA: 2s - loss: 5.8857 - acc: 0.634 - ETA: 2s - loss: 5.8496 - acc: 0.636 - ETA: 2s - loss: 5.8483 - acc: 0.636 - ETA: 2s - loss: 5.8342 - acc: 0.637 - ETA: 2s - loss: 5.8113 - acc: 0.638 - ETA: 2s - loss: 5.7846 - acc: 0.640 - ETA: 2s - loss: 5.8017 - acc: 0.639 - ETA: 1s - loss: 5.7936 - acc: 0.639 - ETA: 1s - loss: 5.8215 - acc: 0.638 - ETA: 1s - loss: 5.8025 - acc: 0.639 - ETA: 1s - loss: 5.8139 - acc: 0.638 - ETA: 1s - loss: 5.8136 - acc: 0.638 - ETA: 1s - loss: 5.8206 - acc: 0.638 - ETA: 1s - loss: 5.8025 - acc: 0.639 - ETA: 1s - loss: 5.7881 - acc: 0.640 - ETA: 1s - loss: 5.8052 - acc: 0.639 - ETA: 1s - loss: 5.8324 - acc: 0.637 - ETA: 1s - loss: 5.8513 - acc: 0.636 - ETA: 1s - loss: 5.8471 - acc: 0.636 - ETA: 1s - loss: 5.8650 - acc: 0.635 - ETA: 0s - loss: 5.8332 - acc: 0.637 - ETA: 0s - loss: 5.8296 - acc: 0.637 - ETA: 0s - loss: 5.8498 - acc: 0.636 - ETA: 0s - loss: 5.8750 - acc: 0.634 - ETA: 0s - loss: 5.8823 - acc: 0.634 - ETA: 0s - loss: 5.8894 - acc: 0.634 - ETA: 0s - loss: 5.8833 - acc: 0.634 - ETA: 0s - loss: 5.8846 - acc: 0.634 - ETA: 0s - loss: 5.8886 - acc: 0.634 - ETA: 0s - loss: 5.8767 - acc: 0.634 - ETA: 0s - loss: 5.8781 - acc: 0.634 - ETA: 0s - loss: 5.8623 - acc: 0.635 - ETA: 0s - loss: 5.8689 - acc: 0.635 - ETA: 0s - loss: 5.8605 - acc: 0.635 - ETA: 0s - loss: 5.8832 - acc: 0.6345Epoch 00064: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 5.8866 - acc: 0.6343 - val_loss: 7.0242 - val_acc: 0.5018\n",
      "Epoch 66/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6580/6680 [============================>.] - ETA: 4s - loss: 6.4472 - acc: 0.600 - ETA: 3s - loss: 5.6417 - acc: 0.650 - ETA: 3s - loss: 5.5685 - acc: 0.654 - ETA: 3s - loss: 5.9984 - acc: 0.625 - ETA: 3s - loss: 5.8975 - acc: 0.630 - ETA: 3s - loss: 5.7173 - acc: 0.642 - ETA: 3s - loss: 5.6240 - acc: 0.648 - ETA: 3s - loss: 5.5574 - acc: 0.652 - ETA: 3s - loss: 5.6285 - acc: 0.648 - ETA: 3s - loss: 5.7396 - acc: 0.642 - ETA: 3s - loss: 5.7460 - acc: 0.641 - ETA: 3s - loss: 5.7811 - acc: 0.639 - ETA: 3s - loss: 5.6602 - acc: 0.647 - ETA: 3s - loss: 5.7468 - acc: 0.642 - ETA: 3s - loss: 5.7644 - acc: 0.641 - ETA: 2s - loss: 5.8663 - acc: 0.634 - ETA: 2s - loss: 5.9384 - acc: 0.630 - ETA: 2s - loss: 5.9793 - acc: 0.628 - ETA: 2s - loss: 6.0433 - acc: 0.624 - ETA: 2s - loss: 6.0214 - acc: 0.625 - ETA: 2s - loss: 6.0308 - acc: 0.625 - ETA: 2s - loss: 6.0756 - acc: 0.622 - ETA: 2s - loss: 6.0628 - acc: 0.623 - ETA: 2s - loss: 6.0108 - acc: 0.626 - ETA: 2s - loss: 5.9320 - acc: 0.631 - ETA: 2s - loss: 5.9642 - acc: 0.629 - ETA: 2s - loss: 5.9514 - acc: 0.630 - ETA: 2s - loss: 5.9667 - acc: 0.629 - ETA: 2s - loss: 6.0322 - acc: 0.625 - ETA: 2s - loss: 6.0356 - acc: 0.625 - ETA: 2s - loss: 6.0777 - acc: 0.622 - ETA: 2s - loss: 6.0471 - acc: 0.624 - ETA: 1s - loss: 6.0573 - acc: 0.623 - ETA: 1s - loss: 6.0694 - acc: 0.623 - ETA: 1s - loss: 6.0565 - acc: 0.623 - ETA: 1s - loss: 6.0444 - acc: 0.624 - ETA: 1s - loss: 5.9715 - acc: 0.629 - ETA: 1s - loss: 5.9802 - acc: 0.628 - ETA: 1s - loss: 5.9865 - acc: 0.628 - ETA: 1s - loss: 5.9818 - acc: 0.628 - ETA: 1s - loss: 5.9856 - acc: 0.628 - ETA: 1s - loss: 5.9970 - acc: 0.627 - ETA: 1s - loss: 5.9813 - acc: 0.628 - ETA: 1s - loss: 5.9407 - acc: 0.630 - ETA: 1s - loss: 5.9140 - acc: 0.632 - ETA: 1s - loss: 5.9031 - acc: 0.633 - ETA: 1s - loss: 5.9114 - acc: 0.632 - ETA: 1s - loss: 5.8850 - acc: 0.634 - ETA: 1s - loss: 5.8420 - acc: 0.636 - ETA: 0s - loss: 5.8543 - acc: 0.636 - ETA: 0s - loss: 5.8654 - acc: 0.635 - ETA: 0s - loss: 5.8633 - acc: 0.635 - ETA: 0s - loss: 5.8529 - acc: 0.636 - ETA: 0s - loss: 5.8671 - acc: 0.635 - ETA: 0s - loss: 5.8719 - acc: 0.635 - ETA: 0s - loss: 5.8794 - acc: 0.634 - ETA: 0s - loss: 5.8780 - acc: 0.634 - ETA: 0s - loss: 5.8795 - acc: 0.634 - ETA: 0s - loss: 5.8699 - acc: 0.635 - ETA: 0s - loss: 5.8843 - acc: 0.634 - ETA: 0s - loss: 5.8757 - acc: 0.634 - ETA: 0s - loss: 5.8974 - acc: 0.633 - ETA: 0s - loss: 5.8950 - acc: 0.633 - ETA: 0s - loss: 5.8860 - acc: 0.634 - ETA: 0s - loss: 5.8797 - acc: 0.634 - ETA: 0s - loss: 5.8974 - acc: 0.6336Epoch 00065: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 5.8871 - acc: 0.6341 - val_loss: 7.0482 - val_acc: 0.5054\n",
      "Epoch 67/100\n",
      "6620/6680 [============================>.] - ETA: 4s - loss: 6.4496 - acc: 0.600 - ETA: 3s - loss: 6.9849 - acc: 0.566 - ETA: 3s - loss: 5.5072 - acc: 0.658 - ETA: 3s - loss: 5.7310 - acc: 0.644 - ETA: 3s - loss: 5.6414 - acc: 0.650 - ETA: 3s - loss: 5.6702 - acc: 0.648 - ETA: 3s - loss: 5.7147 - acc: 0.645 - ETA: 3s - loss: 5.8374 - acc: 0.637 - ETA: 3s - loss: 5.8969 - acc: 0.634 - ETA: 3s - loss: 5.8986 - acc: 0.634 - ETA: 3s - loss: 5.8437 - acc: 0.636 - ETA: 2s - loss: 5.9340 - acc: 0.631 - ETA: 2s - loss: 5.8940 - acc: 0.633 - ETA: 2s - loss: 5.8406 - acc: 0.637 - ETA: 2s - loss: 5.9361 - acc: 0.631 - ETA: 2s - loss: 5.9379 - acc: 0.631 - ETA: 2s - loss: 6.0118 - acc: 0.626 - ETA: 2s - loss: 6.0181 - acc: 0.626 - ETA: 2s - loss: 6.0492 - acc: 0.624 - ETA: 2s - loss: 6.0530 - acc: 0.624 - ETA: 2s - loss: 6.0180 - acc: 0.626 - ETA: 2s - loss: 6.0156 - acc: 0.626 - ETA: 2s - loss: 6.0171 - acc: 0.626 - ETA: 2s - loss: 5.9683 - acc: 0.629 - ETA: 2s - loss: 6.0256 - acc: 0.625 - ETA: 2s - loss: 6.0048 - acc: 0.627 - ETA: 2s - loss: 5.9618 - acc: 0.629 - ETA: 2s - loss: 6.0010 - acc: 0.627 - ETA: 1s - loss: 6.0286 - acc: 0.625 - ETA: 1s - loss: 6.0425 - acc: 0.624 - ETA: 1s - loss: 6.0141 - acc: 0.626 - ETA: 1s - loss: 5.9925 - acc: 0.627 - ETA: 1s - loss: 5.9558 - acc: 0.629 - ETA: 1s - loss: 5.9326 - acc: 0.631 - ETA: 1s - loss: 5.9137 - acc: 0.632 - ETA: 1s - loss: 5.9327 - acc: 0.631 - ETA: 1s - loss: 5.9121 - acc: 0.632 - ETA: 1s - loss: 5.9233 - acc: 0.632 - ETA: 1s - loss: 5.9490 - acc: 0.630 - ETA: 1s - loss: 5.9693 - acc: 0.629 - ETA: 1s - loss: 5.9614 - acc: 0.629 - ETA: 1s - loss: 5.9691 - acc: 0.629 - ETA: 1s - loss: 5.9655 - acc: 0.629 - ETA: 1s - loss: 5.9633 - acc: 0.629 - ETA: 1s - loss: 5.9419 - acc: 0.630 - ETA: 1s - loss: 5.9668 - acc: 0.629 - ETA: 1s - loss: 5.9667 - acc: 0.629 - ETA: 0s - loss: 5.9834 - acc: 0.628 - ETA: 0s - loss: 5.9718 - acc: 0.628 - ETA: 0s - loss: 5.9684 - acc: 0.629 - ETA: 0s - loss: 5.9903 - acc: 0.627 - ETA: 0s - loss: 5.9775 - acc: 0.628 - ETA: 0s - loss: 5.9621 - acc: 0.629 - ETA: 0s - loss: 5.9325 - acc: 0.631 - ETA: 0s - loss: 5.9273 - acc: 0.631 - ETA: 0s - loss: 5.9336 - acc: 0.631 - ETA: 0s - loss: 5.9239 - acc: 0.631 - ETA: 0s - loss: 5.9191 - acc: 0.632 - ETA: 0s - loss: 5.9153 - acc: 0.632 - ETA: 0s - loss: 5.9107 - acc: 0.632 - ETA: 0s - loss: 5.9116 - acc: 0.632 - ETA: 0s - loss: 5.9099 - acc: 0.632 - ETA: 0s - loss: 5.9133 - acc: 0.632 - ETA: 0s - loss: 5.9006 - acc: 0.633 - ETA: 0s - loss: 5.9049 - acc: 0.633 - ETA: 0s - loss: 5.8868 - acc: 0.634 - ETA: 0s - loss: 5.8806 - acc: 0.6346Epoch 00066: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 5.8857 - acc: 0.6343 - val_loss: 7.0580 - val_acc: 0.5030\n",
      "Epoch 68/100\n",
      "6640/6680 [============================>.] - ETA: 2s - loss: 4.8354 - acc: 0.700 - ETA: 3s - loss: 4.9697 - acc: 0.691 - ETA: 3s - loss: 5.5681 - acc: 0.654 - ETA: 3s - loss: 6.2576 - acc: 0.611 - ETA: 3s - loss: 6.0975 - acc: 0.621 - ETA: 3s - loss: 6.2751 - acc: 0.610 - ETA: 3s - loss: 6.1632 - acc: 0.617 - ETA: 2s - loss: 6.1584 - acc: 0.617 - ETA: 2s - loss: 6.1180 - acc: 0.620 - ETA: 2s - loss: 6.0111 - acc: 0.627 - ETA: 2s - loss: 5.9002 - acc: 0.634 - ETA: 2s - loss: 5.9565 - acc: 0.629 - ETA: 2s - loss: 5.9150 - acc: 0.632 - ETA: 2s - loss: 5.8011 - acc: 0.639 - ETA: 2s - loss: 5.7270 - acc: 0.644 - ETA: 2s - loss: 5.7600 - acc: 0.642 - ETA: 2s - loss: 5.7703 - acc: 0.641 - ETA: 2s - loss: 5.7718 - acc: 0.641 - ETA: 2s - loss: 5.7570 - acc: 0.642 - ETA: 2s - loss: 5.7828 - acc: 0.640 - ETA: 2s - loss: 5.7690 - acc: 0.641 - ETA: 2s - loss: 5.7286 - acc: 0.644 - ETA: 2s - loss: 5.7111 - acc: 0.645 - ETA: 2s - loss: 5.7470 - acc: 0.643 - ETA: 2s - loss: 5.7491 - acc: 0.643 - ETA: 2s - loss: 5.7633 - acc: 0.642 - ETA: 2s - loss: 5.7764 - acc: 0.641 - ETA: 1s - loss: 5.7716 - acc: 0.641 - ETA: 1s - loss: 5.7690 - acc: 0.641 - ETA: 1s - loss: 5.7904 - acc: 0.640 - ETA: 1s - loss: 5.7908 - acc: 0.640 - ETA: 1s - loss: 5.8459 - acc: 0.637 - ETA: 1s - loss: 5.8591 - acc: 0.636 - ETA: 1s - loss: 5.8622 - acc: 0.636 - ETA: 1s - loss: 5.8774 - acc: 0.635 - ETA: 1s - loss: 5.8754 - acc: 0.635 - ETA: 1s - loss: 5.8168 - acc: 0.638 - ETA: 1s - loss: 5.8520 - acc: 0.636 - ETA: 1s - loss: 5.8387 - acc: 0.637 - ETA: 1s - loss: 5.8339 - acc: 0.637 - ETA: 1s - loss: 5.8552 - acc: 0.636 - ETA: 1s - loss: 5.8502 - acc: 0.636 - ETA: 1s - loss: 5.8592 - acc: 0.636 - ETA: 1s - loss: 5.8864 - acc: 0.634 - ETA: 1s - loss: 5.8846 - acc: 0.634 - ETA: 1s - loss: 5.8863 - acc: 0.634 - ETA: 0s - loss: 5.8779 - acc: 0.635 - ETA: 0s - loss: 5.8796 - acc: 0.635 - ETA: 0s - loss: 5.8749 - acc: 0.635 - ETA: 0s - loss: 5.8735 - acc: 0.635 - ETA: 0s - loss: 5.8783 - acc: 0.635 - ETA: 0s - loss: 5.8790 - acc: 0.635 - ETA: 0s - loss: 5.8628 - acc: 0.636 - ETA: 0s - loss: 5.8936 - acc: 0.634 - ETA: 0s - loss: 5.8968 - acc: 0.634 - ETA: 0s - loss: 5.8868 - acc: 0.634 - ETA: 0s - loss: 5.8991 - acc: 0.633 - ETA: 0s - loss: 5.8778 - acc: 0.635 - ETA: 0s - loss: 5.8740 - acc: 0.635 - ETA: 0s - loss: 5.8902 - acc: 0.634 - ETA: 0s - loss: 5.8990 - acc: 0.633 - ETA: 0s - loss: 5.9025 - acc: 0.633 - ETA: 0s - loss: 5.9002 - acc: 0.633 - ETA: 0s - loss: 5.8818 - acc: 0.6349Epoch 00067: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 5.8852 - acc: 0.6347 - val_loss: 7.0362 - val_acc: 0.5066\n",
      "Epoch 69/100\n",
      "6660/6680 [============================>.] - ETA: 3s - loss: 5.6413 - acc: 0.650 - ETA: 3s - loss: 5.7757 - acc: 0.641 - ETA: 3s - loss: 5.3483 - acc: 0.668 - ETA: 3s - loss: 5.5910 - acc: 0.653 - ETA: 3s - loss: 5.5262 - acc: 0.657 - ETA: 3s - loss: 5.6419 - acc: 0.650 - ETA: 3s - loss: 5.7718 - acc: 0.641 - ETA: 3s - loss: 5.7979 - acc: 0.640 - ETA: 3s - loss: 5.8967 - acc: 0.634 - ETA: 3s - loss: 5.8515 - acc: 0.637 - ETA: 2s - loss: 5.8893 - acc: 0.634 - ETA: 2s - loss: 5.8915 - acc: 0.634 - ETA: 2s - loss: 6.0569 - acc: 0.624 - ETA: 2s - loss: 6.0385 - acc: 0.625 - ETA: 2s - loss: 6.0661 - acc: 0.623 - ETA: 2s - loss: 6.0902 - acc: 0.622 - ETA: 2s - loss: 6.1059 - acc: 0.621 - ETA: 2s - loss: 6.0533 - acc: 0.624 - ETA: 2s - loss: 6.0275 - acc: 0.626 - ETA: 2s - loss: 6.0206 - acc: 0.626 - ETA: 2s - loss: 5.9953 - acc: 0.628 - ETA: 2s - loss: 5.9552 - acc: 0.630 - ETA: 2s - loss: 6.0003 - acc: 0.627 - ETA: 2s - loss: 5.9858 - acc: 0.628 - ETA: 2s - loss: 5.9725 - acc: 0.629 - ETA: 2s - loss: 5.9936 - acc: 0.628 - ETA: 1s - loss: 5.9729 - acc: 0.629 - ETA: 1s - loss: 5.9394 - acc: 0.631 - ETA: 1s - loss: 5.9829 - acc: 0.628 - ETA: 1s - loss: 5.9720 - acc: 0.629 - ETA: 1s - loss: 5.9896 - acc: 0.628 - ETA: 1s - loss: 5.9936 - acc: 0.628 - ETA: 1s - loss: 5.9665 - acc: 0.629 - ETA: 1s - loss: 5.9894 - acc: 0.628 - ETA: 1s - loss: 5.9486 - acc: 0.630 - ETA: 1s - loss: 5.9404 - acc: 0.631 - ETA: 1s - loss: 5.8974 - acc: 0.634 - ETA: 1s - loss: 5.9237 - acc: 0.632 - ETA: 1s - loss: 5.9749 - acc: 0.629 - ETA: 1s - loss: 5.9499 - acc: 0.630 - ETA: 1s - loss: 5.9125 - acc: 0.633 - ETA: 1s - loss: 5.8905 - acc: 0.634 - ETA: 1s - loss: 5.9195 - acc: 0.632 - ETA: 1s - loss: 5.9147 - acc: 0.633 - ETA: 1s - loss: 5.8668 - acc: 0.636 - ETA: 0s - loss: 5.8779 - acc: 0.635 - ETA: 0s - loss: 5.8666 - acc: 0.636 - ETA: 0s - loss: 5.8685 - acc: 0.635 - ETA: 0s - loss: 5.8882 - acc: 0.634 - ETA: 0s - loss: 5.8927 - acc: 0.634 - ETA: 0s - loss: 5.8941 - acc: 0.634 - ETA: 0s - loss: 5.8924 - acc: 0.634 - ETA: 0s - loss: 5.8995 - acc: 0.634 - ETA: 0s - loss: 5.8892 - acc: 0.634 - ETA: 0s - loss: 5.8785 - acc: 0.635 - ETA: 0s - loss: 5.8546 - acc: 0.636 - ETA: 0s - loss: 5.8564 - acc: 0.636 - ETA: 0s - loss: 5.8502 - acc: 0.637 - ETA: 0s - loss: 5.8514 - acc: 0.637 - ETA: 0s - loss: 5.8404 - acc: 0.637 - ETA: 0s - loss: 5.8524 - acc: 0.636 - ETA: 0s - loss: 5.8633 - acc: 0.636 - ETA: 0s - loss: 5.8834 - acc: 0.6350Epoch 00068: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 5.8851 - acc: 0.6349 - val_loss: 7.0980 - val_acc: 0.5030\n",
      "Epoch 70/100\n",
      "6600/6680 [============================>.] - ETA: 2s - loss: 6.4475 - acc: 0.600 - ETA: 3s - loss: 5.7757 - acc: 0.641 - ETA: 3s - loss: 5.4948 - acc: 0.659 - ETA: 3s - loss: 5.7362 - acc: 0.644 - ETA: 3s - loss: 5.7879 - acc: 0.640 - ETA: 3s - loss: 5.8583 - acc: 0.636 - ETA: 3s - loss: 5.9793 - acc: 0.629 - ETA: 3s - loss: 6.0552 - acc: 0.624 - ETA: 3s - loss: 6.0166 - acc: 0.626 - ETA: 2s - loss: 6.0035 - acc: 0.627 - ETA: 2s - loss: 6.1399 - acc: 0.619 - ETA: 2s - loss: 6.0849 - acc: 0.622 - ETA: 2s - loss: 6.0888 - acc: 0.621 - ETA: 2s - loss: 6.0223 - acc: 0.625 - ETA: 2s - loss: 6.0291 - acc: 0.625 - ETA: 2s - loss: 6.0351 - acc: 0.625 - ETA: 2s - loss: 6.0688 - acc: 0.622 - ETA: 2s - loss: 6.0719 - acc: 0.622 - ETA: 2s - loss: 6.0323 - acc: 0.625 - ETA: 2s - loss: 6.0772 - acc: 0.622 - ETA: 2s - loss: 6.0949 - acc: 0.621 - ETA: 2s - loss: 6.0856 - acc: 0.622 - ETA: 2s - loss: 6.0591 - acc: 0.623 - ETA: 2s - loss: 6.0415 - acc: 0.624 - ETA: 2s - loss: 6.0932 - acc: 0.621 - ETA: 2s - loss: 6.0787 - acc: 0.622 - ETA: 2s - loss: 6.0301 - acc: 0.625 - ETA: 1s - loss: 6.0164 - acc: 0.626 - ETA: 1s - loss: 5.9654 - acc: 0.629 - ETA: 1s - loss: 5.9918 - acc: 0.628 - ETA: 1s - loss: 6.0012 - acc: 0.627 - ETA: 1s - loss: 5.9607 - acc: 0.629 - ETA: 1s - loss: 5.9349 - acc: 0.631 - ETA: 1s - loss: 5.9590 - acc: 0.629 - ETA: 1s - loss: 5.9999 - acc: 0.627 - ETA: 1s - loss: 5.9970 - acc: 0.627 - ETA: 1s - loss: 5.9620 - acc: 0.629 - ETA: 1s - loss: 5.9537 - acc: 0.630 - ETA: 1s - loss: 5.9256 - acc: 0.631 - ETA: 1s - loss: 5.9399 - acc: 0.631 - ETA: 1s - loss: 5.9249 - acc: 0.632 - ETA: 1s - loss: 5.9094 - acc: 0.632 - ETA: 1s - loss: 5.9033 - acc: 0.633 - ETA: 1s - loss: 5.8939 - acc: 0.633 - ETA: 1s - loss: 5.8954 - acc: 0.633 - ETA: 1s - loss: 5.8969 - acc: 0.633 - ETA: 0s - loss: 5.8983 - acc: 0.633 - ETA: 0s - loss: 5.9118 - acc: 0.632 - ETA: 0s - loss: 5.9117 - acc: 0.632 - ETA: 0s - loss: 5.8917 - acc: 0.634 - ETA: 0s - loss: 5.8785 - acc: 0.634 - ETA: 0s - loss: 5.8496 - acc: 0.636 - ETA: 0s - loss: 5.8360 - acc: 0.637 - ETA: 0s - loss: 5.8237 - acc: 0.638 - ETA: 0s - loss: 5.8484 - acc: 0.636 - ETA: 0s - loss: 5.8251 - acc: 0.638 - ETA: 0s - loss: 5.8309 - acc: 0.637 - ETA: 0s - loss: 5.8386 - acc: 0.637 - ETA: 0s - loss: 5.8192 - acc: 0.638 - ETA: 0s - loss: 5.8167 - acc: 0.638 - ETA: 0s - loss: 5.8451 - acc: 0.637 - ETA: 0s - loss: 5.8419 - acc: 0.637 - ETA: 0s - loss: 5.8640 - acc: 0.635 - ETA: 0s - loss: 5.8673 - acc: 0.635 - ETA: 0s - loss: 5.8688 - acc: 0.6356Epoch 00069: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 5.8830 - acc: 0.6347 - val_loss: 7.0583 - val_acc: 0.5078\n",
      "Epoch 71/100\n",
      "6620/6680 [============================>.] - ETA: 5s - loss: 5.6413 - acc: 0.650 - ETA: 3s - loss: 6.3321 - acc: 0.607 - ETA: 3s - loss: 6.1373 - acc: 0.619 - ETA: 3s - loss: 6.2246 - acc: 0.613 - ETA: 3s - loss: 6.0087 - acc: 0.627 - ETA: 3s - loss: 6.0601 - acc: 0.624 - ETA: 3s - loss: 6.0954 - acc: 0.621 - ETA: 3s - loss: 6.1212 - acc: 0.620 - ETA: 3s - loss: 6.0355 - acc: 0.625 - ETA: 3s - loss: 6.1119 - acc: 0.620 - ETA: 3s - loss: 6.1222 - acc: 0.620 - ETA: 2s - loss: 6.0308 - acc: 0.625 - ETA: 2s - loss: 6.0383 - acc: 0.625 - ETA: 2s - loss: 5.9072 - acc: 0.633 - ETA: 2s - loss: 5.9331 - acc: 0.631 - ETA: 2s - loss: 5.9213 - acc: 0.632 - ETA: 2s - loss: 5.9490 - acc: 0.630 - ETA: 2s - loss: 5.8782 - acc: 0.635 - ETA: 2s - loss: 5.8718 - acc: 0.635 - ETA: 2s - loss: 5.8604 - acc: 0.636 - ETA: 2s - loss: 5.8578 - acc: 0.636 - ETA: 2s - loss: 5.8894 - acc: 0.634 - ETA: 2s - loss: 5.8857 - acc: 0.634 - ETA: 2s - loss: 5.8492 - acc: 0.636 - ETA: 2s - loss: 5.8601 - acc: 0.636 - ETA: 2s - loss: 5.8884 - acc: 0.634 - ETA: 2s - loss: 5.9186 - acc: 0.632 - ETA: 1s - loss: 5.9449 - acc: 0.631 - ETA: 1s - loss: 5.8962 - acc: 0.634 - ETA: 1s - loss: 5.8651 - acc: 0.635 - ETA: 1s - loss: 5.8427 - acc: 0.637 - ETA: 1s - loss: 5.8118 - acc: 0.639 - ETA: 1s - loss: 5.8211 - acc: 0.638 - ETA: 1s - loss: 5.8532 - acc: 0.636 - ETA: 1s - loss: 5.8608 - acc: 0.636 - ETA: 1s - loss: 5.8549 - acc: 0.636 - ETA: 1s - loss: 5.8406 - acc: 0.637 - ETA: 1s - loss: 5.8438 - acc: 0.637 - ETA: 1s - loss: 5.8509 - acc: 0.636 - ETA: 1s - loss: 5.8497 - acc: 0.636 - ETA: 1s - loss: 5.8524 - acc: 0.636 - ETA: 1s - loss: 5.8729 - acc: 0.635 - ETA: 1s - loss: 5.8639 - acc: 0.636 - ETA: 1s - loss: 5.8733 - acc: 0.635 - ETA: 1s - loss: 5.8788 - acc: 0.635 - ETA: 1s - loss: 5.8565 - acc: 0.636 - ETA: 0s - loss: 5.8411 - acc: 0.637 - ETA: 0s - loss: 5.8732 - acc: 0.635 - ETA: 0s - loss: 5.9051 - acc: 0.633 - ETA: 0s - loss: 5.9083 - acc: 0.633 - ETA: 0s - loss: 5.9053 - acc: 0.633 - ETA: 0s - loss: 5.9155 - acc: 0.632 - ETA: 0s - loss: 5.8976 - acc: 0.634 - ETA: 0s - loss: 5.8900 - acc: 0.634 - ETA: 0s - loss: 5.8942 - acc: 0.634 - ETA: 0s - loss: 5.8919 - acc: 0.634 - ETA: 0s - loss: 5.9014 - acc: 0.633 - ETA: 0s - loss: 5.8961 - acc: 0.633 - ETA: 0s - loss: 5.9202 - acc: 0.632 - ETA: 0s - loss: 5.9278 - acc: 0.632 - ETA: 0s - loss: 5.9216 - acc: 0.632 - ETA: 0s - loss: 5.9323 - acc: 0.631 - ETA: 0s - loss: 5.9303 - acc: 0.631 - ETA: 0s - loss: 5.8928 - acc: 0.6341Epoch 00070: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 5.8833 - acc: 0.6347 - val_loss: 7.1524 - val_acc: 0.4994\n",
      "Epoch 72/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6560/6680 [============================>.] - ETA: 2s - loss: 4.8354 - acc: 0.700 - ETA: 3s - loss: 6.3130 - acc: 0.608 - ETA: 3s - loss: 6.5205 - acc: 0.595 - ETA: 3s - loss: 5.9384 - acc: 0.629 - ETA: 3s - loss: 6.1889 - acc: 0.614 - ETA: 3s - loss: 6.0847 - acc: 0.621 - ETA: 3s - loss: 6.0771 - acc: 0.621 - ETA: 3s - loss: 6.2578 - acc: 0.610 - ETA: 3s - loss: 6.1268 - acc: 0.619 - ETA: 2s - loss: 6.1501 - acc: 0.617 - ETA: 2s - loss: 6.1174 - acc: 0.619 - ETA: 2s - loss: 6.1042 - acc: 0.620 - ETA: 2s - loss: 6.0223 - acc: 0.625 - ETA: 2s - loss: 6.0480 - acc: 0.624 - ETA: 2s - loss: 6.0533 - acc: 0.624 - ETA: 2s - loss: 6.0372 - acc: 0.625 - ETA: 2s - loss: 5.9454 - acc: 0.630 - ETA: 2s - loss: 6.0014 - acc: 0.627 - ETA: 2s - loss: 6.0041 - acc: 0.627 - ETA: 2s - loss: 5.9288 - acc: 0.631 - ETA: 2s - loss: 5.8995 - acc: 0.633 - ETA: 2s - loss: 5.9220 - acc: 0.632 - ETA: 2s - loss: 5.9589 - acc: 0.630 - ETA: 2s - loss: 5.9994 - acc: 0.627 - ETA: 2s - loss: 5.9695 - acc: 0.629 - ETA: 2s - loss: 6.0001 - acc: 0.627 - ETA: 2s - loss: 5.9810 - acc: 0.628 - ETA: 2s - loss: 6.0089 - acc: 0.627 - ETA: 1s - loss: 5.9743 - acc: 0.629 - ETA: 1s - loss: 5.9899 - acc: 0.628 - ETA: 1s - loss: 6.0228 - acc: 0.626 - ETA: 1s - loss: 6.0383 - acc: 0.625 - ETA: 1s - loss: 6.0652 - acc: 0.623 - ETA: 1s - loss: 6.0905 - acc: 0.621 - ETA: 1s - loss: 6.0981 - acc: 0.621 - ETA: 1s - loss: 6.0855 - acc: 0.622 - ETA: 1s - loss: 6.0628 - acc: 0.623 - ETA: 1s - loss: 6.0560 - acc: 0.624 - ETA: 1s - loss: 6.0334 - acc: 0.625 - ETA: 1s - loss: 6.0237 - acc: 0.626 - ETA: 1s - loss: 6.0339 - acc: 0.625 - ETA: 1s - loss: 6.0247 - acc: 0.626 - ETA: 1s - loss: 6.0031 - acc: 0.627 - ETA: 1s - loss: 5.9827 - acc: 0.628 - ETA: 1s - loss: 5.9788 - acc: 0.628 - ETA: 1s - loss: 5.9922 - acc: 0.628 - ETA: 0s - loss: 5.9916 - acc: 0.628 - ETA: 0s - loss: 5.9911 - acc: 0.628 - ETA: 0s - loss: 5.9712 - acc: 0.629 - ETA: 0s - loss: 5.9615 - acc: 0.630 - ETA: 0s - loss: 5.9492 - acc: 0.630 - ETA: 0s - loss: 5.9495 - acc: 0.630 - ETA: 0s - loss: 5.9426 - acc: 0.631 - ETA: 0s - loss: 5.9284 - acc: 0.632 - ETA: 0s - loss: 5.9290 - acc: 0.632 - ETA: 0s - loss: 5.9578 - acc: 0.630 - ETA: 0s - loss: 5.9626 - acc: 0.629 - ETA: 0s - loss: 5.9696 - acc: 0.629 - ETA: 0s - loss: 5.9536 - acc: 0.630 - ETA: 0s - loss: 5.9354 - acc: 0.631 - ETA: 0s - loss: 5.9179 - acc: 0.632 - ETA: 0s - loss: 5.9008 - acc: 0.633 - ETA: 0s - loss: 5.8993 - acc: 0.633 - ETA: 0s - loss: 5.8929 - acc: 0.6341Epoch 00071: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 5.8836 - acc: 0.6347 - val_loss: 7.0747 - val_acc: 0.5054\n",
      "Epoch 73/100\n",
      "6580/6680 [============================>.] - ETA: 4s - loss: 8.8650 - acc: 0.450 - ETA: 3s - loss: 6.4472 - acc: 0.600 - ETA: 3s - loss: 5.8428 - acc: 0.637 - ETA: 3s - loss: 6.2576 - acc: 0.611 - ETA: 3s - loss: 5.9710 - acc: 0.629 - ETA: 3s - loss: 5.7354 - acc: 0.642 - ETA: 3s - loss: 5.8230 - acc: 0.636 - ETA: 3s - loss: 5.8203 - acc: 0.636 - ETA: 2s - loss: 5.9791 - acc: 0.627 - ETA: 2s - loss: 5.8838 - acc: 0.633 - ETA: 2s - loss: 5.9065 - acc: 0.632 - ETA: 2s - loss: 5.9948 - acc: 0.626 - ETA: 2s - loss: 6.0246 - acc: 0.625 - ETA: 2s - loss: 6.0202 - acc: 0.625 - ETA: 2s - loss: 6.0055 - acc: 0.626 - ETA: 2s - loss: 6.0437 - acc: 0.624 - ETA: 2s - loss: 6.0389 - acc: 0.624 - ETA: 2s - loss: 6.0575 - acc: 0.623 - ETA: 2s - loss: 6.0223 - acc: 0.625 - ETA: 2s - loss: 6.0356 - acc: 0.624 - ETA: 2s - loss: 6.0477 - acc: 0.624 - ETA: 2s - loss: 5.9962 - acc: 0.627 - ETA: 2s - loss: 6.0158 - acc: 0.626 - ETA: 2s - loss: 6.0002 - acc: 0.627 - ETA: 2s - loss: 5.9896 - acc: 0.627 - ETA: 2s - loss: 6.0043 - acc: 0.626 - ETA: 2s - loss: 6.0204 - acc: 0.625 - ETA: 2s - loss: 6.0497 - acc: 0.624 - ETA: 1s - loss: 6.0357 - acc: 0.625 - ETA: 1s - loss: 5.9983 - acc: 0.627 - ETA: 1s - loss: 5.9868 - acc: 0.628 - ETA: 1s - loss: 5.9760 - acc: 0.628 - ETA: 1s - loss: 5.9561 - acc: 0.630 - ETA: 1s - loss: 5.8994 - acc: 0.633 - ETA: 1s - loss: 5.9059 - acc: 0.633 - ETA: 1s - loss: 5.8774 - acc: 0.634 - ETA: 1s - loss: 5.8723 - acc: 0.635 - ETA: 1s - loss: 5.8790 - acc: 0.634 - ETA: 1s - loss: 5.9021 - acc: 0.633 - ETA: 1s - loss: 5.8914 - acc: 0.634 - ETA: 1s - loss: 5.8999 - acc: 0.633 - ETA: 1s - loss: 5.8860 - acc: 0.634 - ETA: 1s - loss: 5.8631 - acc: 0.635 - ETA: 1s - loss: 5.8764 - acc: 0.634 - ETA: 1s - loss: 5.8856 - acc: 0.634 - ETA: 1s - loss: 5.8662 - acc: 0.635 - ETA: 1s - loss: 5.8752 - acc: 0.635 - ETA: 1s - loss: 5.8703 - acc: 0.635 - ETA: 0s - loss: 5.8458 - acc: 0.636 - ETA: 0s - loss: 5.8634 - acc: 0.635 - ETA: 0s - loss: 5.8456 - acc: 0.636 - ETA: 0s - loss: 5.8510 - acc: 0.636 - ETA: 0s - loss: 5.8743 - acc: 0.635 - ETA: 0s - loss: 5.8462 - acc: 0.636 - ETA: 0s - loss: 5.8483 - acc: 0.636 - ETA: 0s - loss: 5.8811 - acc: 0.634 - ETA: 0s - loss: 5.8761 - acc: 0.635 - ETA: 0s - loss: 5.8638 - acc: 0.635 - ETA: 0s - loss: 5.8790 - acc: 0.634 - ETA: 0s - loss: 5.8990 - acc: 0.633 - ETA: 0s - loss: 5.9079 - acc: 0.633 - ETA: 0s - loss: 5.9088 - acc: 0.633 - ETA: 0s - loss: 5.9038 - acc: 0.633 - ETA: 0s - loss: 5.9072 - acc: 0.633 - ETA: 0s - loss: 5.8909 - acc: 0.6342Epoch 00072: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 5.8824 - acc: 0.6347 - val_loss: 7.0841 - val_acc: 0.5090\n",
      "Epoch 74/100\n",
      "6620/6680 [============================>.] - ETA: 5s - loss: 5.6413 - acc: 0.650 - ETA: 3s - loss: 5.8716 - acc: 0.635 - ETA: 3s - loss: 5.9771 - acc: 0.629 - ETA: 3s - loss: 6.2102 - acc: 0.614 - ETA: 3s - loss: 6.1542 - acc: 0.618 - ETA: 3s - loss: 6.0891 - acc: 0.622 - ETA: 3s - loss: 6.0623 - acc: 0.623 - ETA: 3s - loss: 6.1143 - acc: 0.620 - ETA: 3s - loss: 6.1233 - acc: 0.619 - ETA: 3s - loss: 6.0395 - acc: 0.625 - ETA: 2s - loss: 6.0171 - acc: 0.626 - ETA: 2s - loss: 5.9380 - acc: 0.631 - ETA: 2s - loss: 5.9022 - acc: 0.633 - ETA: 2s - loss: 5.8599 - acc: 0.636 - ETA: 2s - loss: 5.9105 - acc: 0.633 - ETA: 2s - loss: 5.8299 - acc: 0.638 - ETA: 2s - loss: 5.8093 - acc: 0.639 - ETA: 2s - loss: 5.8537 - acc: 0.636 - ETA: 2s - loss: 5.8340 - acc: 0.637 - ETA: 2s - loss: 5.8784 - acc: 0.635 - ETA: 2s - loss: 5.8848 - acc: 0.634 - ETA: 2s - loss: 5.8957 - acc: 0.634 - ETA: 2s - loss: 5.8895 - acc: 0.634 - ETA: 2s - loss: 5.8859 - acc: 0.634 - ETA: 2s - loss: 5.8570 - acc: 0.636 - ETA: 2s - loss: 5.8594 - acc: 0.636 - ETA: 2s - loss: 5.8632 - acc: 0.636 - ETA: 1s - loss: 5.8724 - acc: 0.635 - ETA: 1s - loss: 5.8591 - acc: 0.636 - ETA: 1s - loss: 5.8148 - acc: 0.639 - ETA: 1s - loss: 5.7939 - acc: 0.640 - ETA: 1s - loss: 5.8340 - acc: 0.638 - ETA: 1s - loss: 5.8942 - acc: 0.634 - ETA: 1s - loss: 5.9522 - acc: 0.630 - ETA: 1s - loss: 5.9932 - acc: 0.628 - ETA: 1s - loss: 6.0256 - acc: 0.626 - ETA: 1s - loss: 6.0111 - acc: 0.627 - ETA: 1s - loss: 6.0118 - acc: 0.626 - ETA: 1s - loss: 6.0025 - acc: 0.627 - ETA: 1s - loss: 5.9936 - acc: 0.628 - ETA: 1s - loss: 5.9619 - acc: 0.630 - ETA: 1s - loss: 5.9695 - acc: 0.629 - ETA: 1s - loss: 5.9458 - acc: 0.631 - ETA: 1s - loss: 5.9641 - acc: 0.629 - ETA: 1s - loss: 5.9465 - acc: 0.631 - ETA: 1s - loss: 5.9833 - acc: 0.628 - ETA: 0s - loss: 5.9581 - acc: 0.630 - ETA: 0s - loss: 5.9484 - acc: 0.630 - ETA: 0s - loss: 5.9487 - acc: 0.630 - ETA: 0s - loss: 5.9332 - acc: 0.631 - ETA: 0s - loss: 5.9349 - acc: 0.631 - ETA: 0s - loss: 5.9446 - acc: 0.631 - ETA: 0s - loss: 5.9260 - acc: 0.632 - ETA: 0s - loss: 5.9267 - acc: 0.632 - ETA: 0s - loss: 5.9399 - acc: 0.631 - ETA: 0s - loss: 5.8979 - acc: 0.634 - ETA: 0s - loss: 5.8907 - acc: 0.634 - ETA: 0s - loss: 5.8892 - acc: 0.634 - ETA: 0s - loss: 5.8931 - acc: 0.634 - ETA: 0s - loss: 5.8837 - acc: 0.634 - ETA: 0s - loss: 5.8668 - acc: 0.636 - ETA: 0s - loss: 5.8658 - acc: 0.636 - ETA: 0s - loss: 5.8774 - acc: 0.635 - ETA: 0s - loss: 5.8845 - acc: 0.634 - ETA: 0s - loss: 5.8853 - acc: 0.6346Epoch 00073: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 5.8831 - acc: 0.6347 - val_loss: 7.0740 - val_acc: 0.5090\n",
      "Epoch 75/100\n",
      "6580/6680 [============================>.] - ETA: 3s - loss: 8.8650 - acc: 0.450 - ETA: 3s - loss: 6.8502 - acc: 0.575 - ETA: 3s - loss: 6.5205 - acc: 0.595 - ETA: 3s - loss: 6.1954 - acc: 0.615 - ETA: 3s - loss: 5.9483 - acc: 0.631 - ETA: 3s - loss: 6.1249 - acc: 0.620 - ETA: 3s - loss: 6.0443 - acc: 0.625 - ETA: 3s - loss: 6.1709 - acc: 0.617 - ETA: 3s - loss: 6.1047 - acc: 0.621 - ETA: 3s - loss: 6.0626 - acc: 0.623 - ETA: 3s - loss: 6.0854 - acc: 0.622 - ETA: 2s - loss: 6.1249 - acc: 0.620 - ETA: 2s - loss: 6.1249 - acc: 0.620 - ETA: 2s - loss: 6.1001 - acc: 0.621 - ETA: 2s - loss: 5.9819 - acc: 0.628 - ETA: 2s - loss: 5.9030 - acc: 0.633 - ETA: 2s - loss: 5.8674 - acc: 0.636 - ETA: 2s - loss: 5.8451 - acc: 0.637 - ETA: 2s - loss: 5.8580 - acc: 0.636 - ETA: 2s - loss: 5.8963 - acc: 0.634 - ETA: 2s - loss: 5.8995 - acc: 0.634 - ETA: 2s - loss: 5.8779 - acc: 0.635 - ETA: 2s - loss: 5.8958 - acc: 0.634 - ETA: 2s - loss: 5.9122 - acc: 0.633 - ETA: 2s - loss: 5.8948 - acc: 0.634 - ETA: 2s - loss: 5.8787 - acc: 0.635 - ETA: 2s - loss: 5.8819 - acc: 0.635 - ETA: 2s - loss: 5.9196 - acc: 0.632 - ETA: 1s - loss: 5.9081 - acc: 0.633 - ETA: 1s - loss: 5.8885 - acc: 0.634 - ETA: 1s - loss: 5.8962 - acc: 0.634 - ETA: 1s - loss: 5.8517 - acc: 0.637 - ETA: 1s - loss: 5.8890 - acc: 0.634 - ETA: 1s - loss: 5.8630 - acc: 0.636 - ETA: 1s - loss: 5.8737 - acc: 0.635 - ETA: 1s - loss: 5.8717 - acc: 0.635 - ETA: 1s - loss: 5.8871 - acc: 0.634 - ETA: 1s - loss: 5.8878 - acc: 0.634 - ETA: 1s - loss: 5.9128 - acc: 0.633 - ETA: 1s - loss: 5.8903 - acc: 0.634 - ETA: 1s - loss: 5.8844 - acc: 0.634 - ETA: 1s - loss: 5.8523 - acc: 0.636 - ETA: 1s - loss: 5.8549 - acc: 0.636 - ETA: 1s - loss: 5.8491 - acc: 0.637 - ETA: 1s - loss: 5.8445 - acc: 0.637 - ETA: 1s - loss: 5.8393 - acc: 0.637 - ETA: 0s - loss: 5.8320 - acc: 0.637 - ETA: 0s - loss: 5.8149 - acc: 0.638 - ETA: 0s - loss: 5.8373 - acc: 0.637 - ETA: 0s - loss: 5.8548 - acc: 0.636 - ETA: 0s - loss: 5.8654 - acc: 0.635 - ETA: 0s - loss: 5.8430 - acc: 0.637 - ETA: 0s - loss: 5.8541 - acc: 0.636 - ETA: 0s - loss: 5.8524 - acc: 0.636 - ETA: 0s - loss: 5.8458 - acc: 0.637 - ETA: 0s - loss: 5.8444 - acc: 0.637 - ETA: 0s - loss: 5.8491 - acc: 0.636 - ETA: 0s - loss: 5.8457 - acc: 0.637 - ETA: 0s - loss: 5.8469 - acc: 0.637 - ETA: 0s - loss: 5.8611 - acc: 0.636 - ETA: 0s - loss: 5.8628 - acc: 0.636 - ETA: 0s - loss: 5.8744 - acc: 0.635 - ETA: 0s - loss: 5.8733 - acc: 0.635 - ETA: 0s - loss: 5.8827 - acc: 0.6348Epoch 00074: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 5.8815 - acc: 0.6349 - val_loss: 7.1658 - val_acc: 0.5102\n",
      "Epoch 76/100\n",
      "6620/6680 [============================>.] - ETA: 4s - loss: 2.4279 - acc: 0.850 - ETA: 3s - loss: 5.2974 - acc: 0.671 - ETA: 3s - loss: 5.2082 - acc: 0.676 - ETA: 3s - loss: 5.5971 - acc: 0.652 - ETA: 3s - loss: 5.5685 - acc: 0.654 - ETA: 3s - loss: 5.5841 - acc: 0.653 - ETA: 2s - loss: 5.8550 - acc: 0.636 - ETA: 2s - loss: 5.8482 - acc: 0.637 - ETA: 2s - loss: 5.8923 - acc: 0.634 - ETA: 2s - loss: 5.8628 - acc: 0.636 - ETA: 2s - loss: 5.7970 - acc: 0.640 - ETA: 2s - loss: 5.8105 - acc: 0.639 - ETA: 2s - loss: 5.8099 - acc: 0.639 - ETA: 2s - loss: 5.7534 - acc: 0.643 - ETA: 2s - loss: 5.7880 - acc: 0.640 - ETA: 2s - loss: 5.7496 - acc: 0.643 - ETA: 2s - loss: 5.7156 - acc: 0.645 - ETA: 2s - loss: 5.7028 - acc: 0.646 - ETA: 2s - loss: 5.7319 - acc: 0.644 - ETA: 2s - loss: 5.6962 - acc: 0.646 - ETA: 2s - loss: 5.7235 - acc: 0.644 - ETA: 2s - loss: 5.7121 - acc: 0.645 - ETA: 2s - loss: 5.7430 - acc: 0.643 - ETA: 2s - loss: 5.7519 - acc: 0.643 - ETA: 2s - loss: 5.7530 - acc: 0.643 - ETA: 2s - loss: 5.7608 - acc: 0.642 - ETA: 1s - loss: 5.7853 - acc: 0.641 - ETA: 1s - loss: 5.7804 - acc: 0.641 - ETA: 1s - loss: 5.8069 - acc: 0.639 - ETA: 1s - loss: 5.8119 - acc: 0.639 - ETA: 1s - loss: 5.8076 - acc: 0.639 - ETA: 1s - loss: 5.7831 - acc: 0.641 - ETA: 1s - loss: 5.7315 - acc: 0.644 - ETA: 1s - loss: 5.7427 - acc: 0.643 - ETA: 1s - loss: 5.7315 - acc: 0.644 - ETA: 1s - loss: 5.7300 - acc: 0.644 - ETA: 1s - loss: 5.7021 - acc: 0.646 - ETA: 1s - loss: 5.7054 - acc: 0.646 - ETA: 1s - loss: 5.7333 - acc: 0.644 - ETA: 1s - loss: 5.7635 - acc: 0.642 - ETA: 1s - loss: 5.7327 - acc: 0.644 - ETA: 1s - loss: 5.7731 - acc: 0.641 - ETA: 1s - loss: 5.7657 - acc: 0.642 - ETA: 1s - loss: 5.7628 - acc: 0.642 - ETA: 1s - loss: 5.7745 - acc: 0.641 - ETA: 1s - loss: 5.7927 - acc: 0.640 - ETA: 1s - loss: 5.7654 - acc: 0.642 - ETA: 1s - loss: 5.7735 - acc: 0.641 - ETA: 0s - loss: 5.7774 - acc: 0.641 - ETA: 0s - loss: 5.7876 - acc: 0.640 - ETA: 0s - loss: 5.8109 - acc: 0.639 - ETA: 0s - loss: 5.8264 - acc: 0.638 - ETA: 0s - loss: 5.8291 - acc: 0.638 - ETA: 0s - loss: 5.8255 - acc: 0.638 - ETA: 0s - loss: 5.8074 - acc: 0.639 - ETA: 0s - loss: 5.8038 - acc: 0.639 - ETA: 0s - loss: 5.8129 - acc: 0.639 - ETA: 0s - loss: 5.8240 - acc: 0.638 - ETA: 0s - loss: 5.8505 - acc: 0.637 - ETA: 0s - loss: 5.8307 - acc: 0.638 - ETA: 0s - loss: 5.8382 - acc: 0.637 - ETA: 0s - loss: 5.8403 - acc: 0.637 - ETA: 0s - loss: 5.8416 - acc: 0.637 - ETA: 0s - loss: 5.8511 - acc: 0.637 - ETA: 0s - loss: 5.8671 - acc: 0.636 - ETA: 0s - loss: 5.8752 - acc: 0.6355Epoch 00075: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 5.8827 - acc: 0.6350 - val_loss: 7.1349 - val_acc: 0.5078\n",
      "Epoch 77/100\n",
      "6580/6680 [============================>.] - ETA: 3s - loss: 7.2531 - acc: 0.550 - ETA: 3s - loss: 5.7565 - acc: 0.642 - ETA: 3s - loss: 6.0443 - acc: 0.625 - ETA: 3s - loss: 5.6413 - acc: 0.650 - ETA: 3s - loss: 5.6413 - acc: 0.650 - ETA: 3s - loss: 6.1019 - acc: 0.621 - ETA: 3s - loss: 6.1786 - acc: 0.616 - ETA: 3s - loss: 6.1927 - acc: 0.615 - ETA: 2s - loss: 6.0809 - acc: 0.622 - ETA: 2s - loss: 5.9637 - acc: 0.630 - ETA: 2s - loss: 5.9198 - acc: 0.632 - ETA: 2s - loss: 5.8263 - acc: 0.638 - ETA: 2s - loss: 5.8489 - acc: 0.637 - ETA: 2s - loss: 5.8230 - acc: 0.638 - ETA: 2s - loss: 5.8240 - acc: 0.638 - ETA: 2s - loss: 5.8731 - acc: 0.635 - ETA: 2s - loss: 5.8120 - acc: 0.639 - ETA: 2s - loss: 5.8025 - acc: 0.640 - ETA: 2s - loss: 5.8110 - acc: 0.639 - ETA: 2s - loss: 5.8267 - acc: 0.638 - ETA: 2s - loss: 5.8179 - acc: 0.639 - ETA: 2s - loss: 5.8737 - acc: 0.635 - ETA: 2s - loss: 5.8220 - acc: 0.638 - ETA: 2s - loss: 5.7746 - acc: 0.641 - ETA: 2s - loss: 5.7821 - acc: 0.641 - ETA: 2s - loss: 5.7635 - acc: 0.642 - ETA: 2s - loss: 5.7767 - acc: 0.641 - ETA: 1s - loss: 5.7322 - acc: 0.644 - ETA: 1s - loss: 5.7503 - acc: 0.643 - ETA: 1s - loss: 5.7520 - acc: 0.643 - ETA: 1s - loss: 5.7638 - acc: 0.642 - ETA: 1s - loss: 5.7502 - acc: 0.643 - ETA: 1s - loss: 5.7476 - acc: 0.643 - ETA: 1s - loss: 5.7263 - acc: 0.644 - ETA: 1s - loss: 5.7143 - acc: 0.645 - ETA: 1s - loss: 5.7339 - acc: 0.644 - ETA: 1s - loss: 5.7357 - acc: 0.644 - ETA: 1s - loss: 5.7333 - acc: 0.644 - ETA: 1s - loss: 5.7386 - acc: 0.644 - ETA: 1s - loss: 5.7402 - acc: 0.643 - ETA: 1s - loss: 5.7339 - acc: 0.644 - ETA: 1s - loss: 5.7501 - acc: 0.643 - ETA: 1s - loss: 5.7806 - acc: 0.641 - ETA: 1s - loss: 5.7811 - acc: 0.641 - ETA: 1s - loss: 5.7705 - acc: 0.642 - ETA: 1s - loss: 5.7746 - acc: 0.641 - ETA: 0s - loss: 5.7451 - acc: 0.643 - ETA: 0s - loss: 5.7692 - acc: 0.642 - ETA: 0s - loss: 5.7762 - acc: 0.641 - ETA: 0s - loss: 5.7925 - acc: 0.640 - ETA: 0s - loss: 5.8112 - acc: 0.639 - ETA: 0s - loss: 5.8141 - acc: 0.639 - ETA: 0s - loss: 5.8464 - acc: 0.637 - ETA: 0s - loss: 5.8427 - acc: 0.637 - ETA: 0s - loss: 5.8534 - acc: 0.636 - ETA: 0s - loss: 5.8377 - acc: 0.637 - ETA: 0s - loss: 5.8565 - acc: 0.636 - ETA: 0s - loss: 5.8440 - acc: 0.637 - ETA: 0s - loss: 5.8374 - acc: 0.637 - ETA: 0s - loss: 5.8420 - acc: 0.637 - ETA: 0s - loss: 5.8440 - acc: 0.637 - ETA: 0s - loss: 5.8585 - acc: 0.636 - ETA: 0s - loss: 5.8526 - acc: 0.636 - ETA: 0s - loss: 5.8690 - acc: 0.6359Epoch 00076: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 5.8825 - acc: 0.6350 - val_loss: 7.1401 - val_acc: 0.5114\n",
      "Epoch 78/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6620/6680 [============================>.] - ETA: 3s - loss: 4.8354 - acc: 0.700 - ETA: 3s - loss: 6.0443 - acc: 0.625 - ETA: 3s - loss: 5.4948 - acc: 0.659 - ETA: 3s - loss: 5.7836 - acc: 0.641 - ETA: 3s - loss: 5.8245 - acc: 0.638 - ETA: 3s - loss: 5.8503 - acc: 0.637 - ETA: 3s - loss: 5.8176 - acc: 0.639 - ETA: 3s - loss: 5.8156 - acc: 0.639 - ETA: 3s - loss: 5.6797 - acc: 0.647 - ETA: 3s - loss: 5.7253 - acc: 0.644 - ETA: 2s - loss: 5.7757 - acc: 0.641 - ETA: 2s - loss: 5.7803 - acc: 0.641 - ETA: 2s - loss: 5.8460 - acc: 0.637 - ETA: 2s - loss: 5.9100 - acc: 0.633 - ETA: 2s - loss: 5.8885 - acc: 0.634 - ETA: 2s - loss: 5.9299 - acc: 0.632 - ETA: 2s - loss: 5.9225 - acc: 0.632 - ETA: 2s - loss: 5.9272 - acc: 0.631 - ETA: 2s - loss: 5.9159 - acc: 0.632 - ETA: 2s - loss: 5.8699 - acc: 0.635 - ETA: 2s - loss: 5.9665 - acc: 0.629 - ETA: 2s - loss: 5.9884 - acc: 0.627 - ETA: 2s - loss: 5.9242 - acc: 0.631 - ETA: 2s - loss: 5.9460 - acc: 0.630 - ETA: 2s - loss: 5.9725 - acc: 0.628 - ETA: 2s - loss: 5.9722 - acc: 0.628 - ETA: 2s - loss: 5.9339 - acc: 0.631 - ETA: 1s - loss: 5.9612 - acc: 0.629 - ETA: 1s - loss: 5.9613 - acc: 0.629 - ETA: 1s - loss: 5.9943 - acc: 0.627 - ETA: 1s - loss: 5.9985 - acc: 0.627 - ETA: 1s - loss: 6.0002 - acc: 0.627 - ETA: 1s - loss: 5.9367 - acc: 0.631 - ETA: 1s - loss: 5.9608 - acc: 0.629 - ETA: 1s - loss: 5.9861 - acc: 0.627 - ETA: 1s - loss: 5.9943 - acc: 0.627 - ETA: 1s - loss: 5.9849 - acc: 0.628 - ETA: 1s - loss: 5.9578 - acc: 0.629 - ETA: 1s - loss: 5.9393 - acc: 0.630 - ETA: 1s - loss: 5.9636 - acc: 0.629 - ETA: 1s - loss: 5.9713 - acc: 0.628 - ETA: 1s - loss: 5.9846 - acc: 0.628 - ETA: 1s - loss: 5.9878 - acc: 0.628 - ETA: 1s - loss: 6.0071 - acc: 0.626 - ETA: 1s - loss: 6.0046 - acc: 0.626 - ETA: 1s - loss: 5.9799 - acc: 0.628 - ETA: 0s - loss: 5.9630 - acc: 0.629 - ETA: 0s - loss: 5.9499 - acc: 0.630 - ETA: 0s - loss: 5.9374 - acc: 0.631 - ETA: 0s - loss: 5.9222 - acc: 0.632 - ETA: 0s - loss: 5.9107 - acc: 0.632 - ETA: 0s - loss: 5.8927 - acc: 0.634 - ETA: 0s - loss: 5.9028 - acc: 0.633 - ETA: 0s - loss: 5.9155 - acc: 0.632 - ETA: 0s - loss: 5.8907 - acc: 0.634 - ETA: 0s - loss: 5.9088 - acc: 0.633 - ETA: 0s - loss: 5.9244 - acc: 0.632 - ETA: 0s - loss: 5.9206 - acc: 0.632 - ETA: 0s - loss: 5.8999 - acc: 0.633 - ETA: 0s - loss: 5.8957 - acc: 0.633 - ETA: 0s - loss: 5.8942 - acc: 0.633 - ETA: 0s - loss: 5.8876 - acc: 0.634 - ETA: 0s - loss: 5.8964 - acc: 0.633 - ETA: 0s - loss: 5.8900 - acc: 0.634 - ETA: 0s - loss: 5.8789 - acc: 0.6349Epoch 00077: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 5.8816 - acc: 0.6347 - val_loss: 7.1530 - val_acc: 0.5102\n",
      "Epoch 79/100\n",
      "6560/6680 [============================>.] - ETA: 3s - loss: 7.2531 - acc: 0.550 - ETA: 3s - loss: 6.1786 - acc: 0.616 - ETA: 3s - loss: 5.4215 - acc: 0.663 - ETA: 3s - loss: 5.6887 - acc: 0.647 - ETA: 3s - loss: 5.5681 - acc: 0.654 - ETA: 3s - loss: 5.7309 - acc: 0.644 - ETA: 3s - loss: 5.6673 - acc: 0.648 - ETA: 3s - loss: 5.6190 - acc: 0.651 - ETA: 3s - loss: 5.6413 - acc: 0.650 - ETA: 3s - loss: 5.6055 - acc: 0.652 - ETA: 3s - loss: 5.6413 - acc: 0.650 - ETA: 2s - loss: 5.5982 - acc: 0.652 - ETA: 2s - loss: 5.6017 - acc: 0.652 - ETA: 2s - loss: 5.6293 - acc: 0.650 - ETA: 2s - loss: 5.5751 - acc: 0.654 - ETA: 2s - loss: 5.5793 - acc: 0.653 - ETA: 2s - loss: 5.6315 - acc: 0.650 - ETA: 2s - loss: 5.5681 - acc: 0.654 - ETA: 2s - loss: 5.5547 - acc: 0.655 - ETA: 2s - loss: 5.5920 - acc: 0.653 - ETA: 2s - loss: 5.5787 - acc: 0.653 - ETA: 2s - loss: 5.5891 - acc: 0.653 - ETA: 2s - loss: 5.5700 - acc: 0.654 - ETA: 2s - loss: 5.6140 - acc: 0.651 - ETA: 2s - loss: 5.6544 - acc: 0.649 - ETA: 2s - loss: 5.6726 - acc: 0.648 - ETA: 2s - loss: 5.7383 - acc: 0.644 - ETA: 2s - loss: 5.7698 - acc: 0.642 - ETA: 2s - loss: 5.7146 - acc: 0.645 - ETA: 1s - loss: 5.7387 - acc: 0.644 - ETA: 1s - loss: 5.7198 - acc: 0.645 - ETA: 1s - loss: 5.7123 - acc: 0.645 - ETA: 1s - loss: 5.7244 - acc: 0.644 - ETA: 1s - loss: 5.7077 - acc: 0.645 - ETA: 1s - loss: 5.7104 - acc: 0.645 - ETA: 1s - loss: 5.7264 - acc: 0.644 - ETA: 1s - loss: 5.7633 - acc: 0.642 - ETA: 1s - loss: 5.7834 - acc: 0.641 - ETA: 1s - loss: 5.7627 - acc: 0.642 - ETA: 1s - loss: 5.7517 - acc: 0.643 - ETA: 1s - loss: 5.6788 - acc: 0.647 - ETA: 1s - loss: 5.6930 - acc: 0.646 - ETA: 1s - loss: 5.7140 - acc: 0.645 - ETA: 1s - loss: 5.6982 - acc: 0.646 - ETA: 1s - loss: 5.7179 - acc: 0.645 - ETA: 1s - loss: 5.6920 - acc: 0.646 - ETA: 0s - loss: 5.7076 - acc: 0.645 - ETA: 0s - loss: 5.7093 - acc: 0.645 - ETA: 0s - loss: 5.7402 - acc: 0.643 - ETA: 0s - loss: 5.7351 - acc: 0.644 - ETA: 0s - loss: 5.7457 - acc: 0.643 - ETA: 0s - loss: 5.7403 - acc: 0.643 - ETA: 0s - loss: 5.7504 - acc: 0.643 - ETA: 0s - loss: 5.7776 - acc: 0.641 - ETA: 0s - loss: 5.8147 - acc: 0.639 - ETA: 0s - loss: 5.8123 - acc: 0.639 - ETA: 0s - loss: 5.8038 - acc: 0.639 - ETA: 0s - loss: 5.8277 - acc: 0.638 - ETA: 0s - loss: 5.8426 - acc: 0.637 - ETA: 0s - loss: 5.8393 - acc: 0.637 - ETA: 0s - loss: 5.8439 - acc: 0.637 - ETA: 0s - loss: 5.8610 - acc: 0.636 - ETA: 0s - loss: 5.8451 - acc: 0.637 - ETA: 0s - loss: 5.8444 - acc: 0.6373Epoch 00078: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 5.8818 - acc: 0.6350 - val_loss: 7.1412 - val_acc: 0.5042\n",
      "Epoch 80/100\n",
      "6660/6680 [============================>.] - ETA: 2s - loss: 4.8354 - acc: 0.700 - ETA: 3s - loss: 5.7565 - acc: 0.642 - ETA: 3s - loss: 6.1786 - acc: 0.616 - ETA: 3s - loss: 6.2234 - acc: 0.613 - ETA: 3s - loss: 6.0618 - acc: 0.623 - ETA: 3s - loss: 6.1697 - acc: 0.617 - ETA: 2s - loss: 6.1482 - acc: 0.618 - ETA: 2s - loss: 6.1050 - acc: 0.621 - ETA: 2s - loss: 6.1788 - acc: 0.616 - ETA: 2s - loss: 6.1895 - acc: 0.616 - ETA: 2s - loss: 6.2683 - acc: 0.611 - ETA: 2s - loss: 6.2384 - acc: 0.612 - ETA: 2s - loss: 6.1782 - acc: 0.616 - ETA: 2s - loss: 6.0498 - acc: 0.624 - ETA: 2s - loss: 6.1311 - acc: 0.619 - ETA: 2s - loss: 6.1103 - acc: 0.620 - ETA: 2s - loss: 6.1208 - acc: 0.620 - ETA: 2s - loss: 6.0938 - acc: 0.621 - ETA: 2s - loss: 6.0355 - acc: 0.625 - ETA: 2s - loss: 5.9635 - acc: 0.630 - ETA: 2s - loss: 5.9833 - acc: 0.628 - ETA: 2s - loss: 5.9533 - acc: 0.630 - ETA: 2s - loss: 5.9373 - acc: 0.631 - ETA: 2s - loss: 5.9622 - acc: 0.630 - ETA: 2s - loss: 5.9056 - acc: 0.633 - ETA: 2s - loss: 5.8998 - acc: 0.634 - ETA: 1s - loss: 5.8887 - acc: 0.634 - ETA: 1s - loss: 5.8579 - acc: 0.636 - ETA: 1s - loss: 5.8239 - acc: 0.638 - ETA: 1s - loss: 5.8427 - acc: 0.637 - ETA: 1s - loss: 5.8919 - acc: 0.634 - ETA: 1s - loss: 5.9331 - acc: 0.631 - ETA: 1s - loss: 5.9245 - acc: 0.632 - ETA: 1s - loss: 5.9072 - acc: 0.633 - ETA: 1s - loss: 5.8850 - acc: 0.634 - ETA: 1s - loss: 5.8972 - acc: 0.634 - ETA: 1s - loss: 5.8735 - acc: 0.635 - ETA: 1s - loss: 5.8428 - acc: 0.637 - ETA: 1s - loss: 5.8528 - acc: 0.636 - ETA: 1s - loss: 5.8477 - acc: 0.637 - ETA: 1s - loss: 5.8466 - acc: 0.637 - ETA: 1s - loss: 5.8446 - acc: 0.637 - ETA: 1s - loss: 5.8364 - acc: 0.637 - ETA: 1s - loss: 5.8250 - acc: 0.638 - ETA: 1s - loss: 5.8341 - acc: 0.638 - ETA: 0s - loss: 5.8402 - acc: 0.637 - ETA: 0s - loss: 5.8162 - acc: 0.639 - ETA: 0s - loss: 5.8485 - acc: 0.637 - ETA: 0s - loss: 5.8475 - acc: 0.637 - ETA: 0s - loss: 5.8498 - acc: 0.637 - ETA: 0s - loss: 5.8428 - acc: 0.637 - ETA: 0s - loss: 5.8263 - acc: 0.638 - ETA: 0s - loss: 5.8317 - acc: 0.638 - ETA: 0s - loss: 5.8514 - acc: 0.636 - ETA: 0s - loss: 5.8470 - acc: 0.637 - ETA: 0s - loss: 5.8435 - acc: 0.637 - ETA: 0s - loss: 5.8346 - acc: 0.638 - ETA: 0s - loss: 5.8414 - acc: 0.637 - ETA: 0s - loss: 5.8355 - acc: 0.637 - ETA: 0s - loss: 5.8454 - acc: 0.637 - ETA: 0s - loss: 5.8421 - acc: 0.637 - ETA: 0s - loss: 5.8509 - acc: 0.637 - ETA: 0s - loss: 5.8741 - acc: 0.635 - ETA: 0s - loss: 5.8803 - acc: 0.6351Epoch 00079: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 5.8820 - acc: 0.6350 - val_loss: 7.1676 - val_acc: 0.5030\n",
      "Epoch 81/100\n",
      "6580/6680 [============================>.] - ETA: 3s - loss: 6.4472 - acc: 0.600 - ETA: 3s - loss: 6.2170 - acc: 0.614 - ETA: 3s - loss: 6.5144 - acc: 0.595 - ETA: 3s - loss: 6.1786 - acc: 0.616 - ETA: 3s - loss: 6.2720 - acc: 0.610 - ETA: 2s - loss: 6.0860 - acc: 0.622 - ETA: 2s - loss: 5.8784 - acc: 0.635 - ETA: 2s - loss: 5.9306 - acc: 0.632 - ETA: 3s - loss: 5.9893 - acc: 0.628 - ETA: 2s - loss: 5.9209 - acc: 0.632 - ETA: 2s - loss: 5.8801 - acc: 0.635 - ETA: 2s - loss: 5.8735 - acc: 0.635 - ETA: 2s - loss: 5.8844 - acc: 0.634 - ETA: 2s - loss: 5.9376 - acc: 0.631 - ETA: 2s - loss: 5.9789 - acc: 0.629 - ETA: 2s - loss: 6.0290 - acc: 0.625 - ETA: 2s - loss: 5.9963 - acc: 0.628 - ETA: 2s - loss: 6.0579 - acc: 0.624 - ETA: 2s - loss: 6.0660 - acc: 0.623 - ETA: 2s - loss: 6.1101 - acc: 0.620 - ETA: 2s - loss: 6.0482 - acc: 0.624 - ETA: 2s - loss: 6.0184 - acc: 0.626 - ETA: 2s - loss: 5.9847 - acc: 0.628 - ETA: 2s - loss: 5.9771 - acc: 0.629 - ETA: 2s - loss: 5.9900 - acc: 0.628 - ETA: 2s - loss: 5.9619 - acc: 0.629 - ETA: 2s - loss: 5.9561 - acc: 0.630 - ETA: 2s - loss: 5.9450 - acc: 0.631 - ETA: 1s - loss: 5.9382 - acc: 0.631 - ETA: 1s - loss: 5.9548 - acc: 0.630 - ETA: 1s - loss: 5.9347 - acc: 0.631 - ETA: 1s - loss: 5.9405 - acc: 0.631 - ETA: 1s - loss: 5.9604 - acc: 0.630 - ETA: 1s - loss: 5.9249 - acc: 0.632 - ETA: 1s - loss: 5.9351 - acc: 0.631 - ETA: 1s - loss: 5.9536 - acc: 0.630 - ETA: 1s - loss: 5.9581 - acc: 0.630 - ETA: 1s - loss: 5.9751 - acc: 0.629 - ETA: 1s - loss: 5.9487 - acc: 0.630 - ETA: 1s - loss: 5.9120 - acc: 0.633 - ETA: 1s - loss: 5.8901 - acc: 0.634 - ETA: 1s - loss: 5.8944 - acc: 0.634 - ETA: 1s - loss: 5.8810 - acc: 0.635 - ETA: 1s - loss: 5.8839 - acc: 0.634 - ETA: 1s - loss: 5.8846 - acc: 0.634 - ETA: 1s - loss: 5.8521 - acc: 0.636 - ETA: 0s - loss: 5.8544 - acc: 0.636 - ETA: 0s - loss: 5.8303 - acc: 0.638 - ETA: 0s - loss: 5.8772 - acc: 0.635 - ETA: 0s - loss: 5.8820 - acc: 0.635 - ETA: 0s - loss: 5.8589 - acc: 0.636 - ETA: 0s - loss: 5.8691 - acc: 0.635 - ETA: 0s - loss: 5.8738 - acc: 0.635 - ETA: 0s - loss: 5.8821 - acc: 0.635 - ETA: 0s - loss: 5.8835 - acc: 0.634 - ETA: 0s - loss: 5.8916 - acc: 0.634 - ETA: 0s - loss: 5.8984 - acc: 0.633 - ETA: 0s - loss: 5.8913 - acc: 0.634 - ETA: 0s - loss: 5.8804 - acc: 0.634 - ETA: 0s - loss: 5.8731 - acc: 0.635 - ETA: 0s - loss: 5.8797 - acc: 0.634 - ETA: 0s - loss: 5.8676 - acc: 0.635 - ETA: 0s - loss: 5.8684 - acc: 0.635 - ETA: 0s - loss: 5.8698 - acc: 0.6356Epoch 00080: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 5.8774 - acc: 0.6350 - val_loss: 7.2013 - val_acc: 0.5030\n",
      "Epoch 82/100\n",
      "6660/6680 [============================>.] - ETA: 3s - loss: 1.6118 - acc: 0.900 - ETA: 3s - loss: 5.7757 - acc: 0.641 - ETA: 3s - loss: 5.2933 - acc: 0.668 - ETA: 3s - loss: 5.3549 - acc: 0.662 - ETA: 3s - loss: 5.5000 - acc: 0.654 - ETA: 3s - loss: 5.5314 - acc: 0.653 - ETA: 3s - loss: 5.7786 - acc: 0.635 - ETA: 3s - loss: 5.7994 - acc: 0.635 - ETA: 2s - loss: 5.7961 - acc: 0.636 - ETA: 2s - loss: 5.8336 - acc: 0.634 - ETA: 2s - loss: 5.8459 - acc: 0.634 - ETA: 2s - loss: 5.8468 - acc: 0.633 - ETA: 2s - loss: 5.7814 - acc: 0.637 - ETA: 2s - loss: 5.8179 - acc: 0.635 - ETA: 2s - loss: 5.8060 - acc: 0.636 - ETA: 2s - loss: 5.7752 - acc: 0.638 - ETA: 2s - loss: 5.7768 - acc: 0.638 - ETA: 2s - loss: 5.7241 - acc: 0.642 - ETA: 2s - loss: 5.7698 - acc: 0.639 - ETA: 2s - loss: 5.7392 - acc: 0.641 - ETA: 2s - loss: 5.7863 - acc: 0.638 - ETA: 2s - loss: 5.7291 - acc: 0.641 - ETA: 2s - loss: 5.7828 - acc: 0.638 - ETA: 2s - loss: 5.7819 - acc: 0.638 - ETA: 2s - loss: 5.7688 - acc: 0.639 - ETA: 2s - loss: 5.7578 - acc: 0.640 - ETA: 2s - loss: 5.7713 - acc: 0.639 - ETA: 2s - loss: 5.7747 - acc: 0.639 - ETA: 1s - loss: 5.8021 - acc: 0.637 - ETA: 1s - loss: 5.8180 - acc: 0.636 - ETA: 1s - loss: 5.7713 - acc: 0.639 - ETA: 1s - loss: 5.7488 - acc: 0.640 - ETA: 1s - loss: 5.7602 - acc: 0.640 - ETA: 1s - loss: 5.7716 - acc: 0.639 - ETA: 1s - loss: 5.7773 - acc: 0.639 - ETA: 1s - loss: 5.7704 - acc: 0.639 - ETA: 1s - loss: 5.8093 - acc: 0.637 - ETA: 1s - loss: 5.7833 - acc: 0.638 - ETA: 1s - loss: 5.7716 - acc: 0.639 - ETA: 1s - loss: 5.7843 - acc: 0.638 - ETA: 1s - loss: 5.8111 - acc: 0.637 - ETA: 1s - loss: 5.8112 - acc: 0.637 - ETA: 1s - loss: 5.8066 - acc: 0.637 - ETA: 1s - loss: 5.7795 - acc: 0.638 - ETA: 1s - loss: 5.7863 - acc: 0.638 - ETA: 0s - loss: 5.7827 - acc: 0.638 - ETA: 0s - loss: 5.7599 - acc: 0.640 - ETA: 0s - loss: 5.7978 - acc: 0.637 - ETA: 0s - loss: 5.8005 - acc: 0.637 - ETA: 0s - loss: 5.7974 - acc: 0.638 - ETA: 0s - loss: 5.7792 - acc: 0.639 - ETA: 0s - loss: 5.7587 - acc: 0.640 - ETA: 0s - loss: 5.7477 - acc: 0.641 - ETA: 0s - loss: 5.7487 - acc: 0.641 - ETA: 0s - loss: 5.7719 - acc: 0.639 - ETA: 0s - loss: 5.7869 - acc: 0.638 - ETA: 0s - loss: 5.7904 - acc: 0.638 - ETA: 0s - loss: 5.7635 - acc: 0.640 - ETA: 0s - loss: 5.7696 - acc: 0.640 - ETA: 0s - loss: 5.7837 - acc: 0.639 - ETA: 0s - loss: 5.7960 - acc: 0.638 - ETA: 0s - loss: 5.8063 - acc: 0.637 - ETA: 0s - loss: 5.8233 - acc: 0.636 - ETA: 0s - loss: 5.8206 - acc: 0.636 - ETA: 0s - loss: 5.8348 - acc: 0.6360Epoch 00081: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 5.8415 - acc: 0.6356 - val_loss: 7.2429 - val_acc: 0.4982\n",
      "Epoch 83/100\n",
      "6620/6680 [============================>.] - ETA: 3s - loss: 5.6413 - acc: 0.650 - ETA: 3s - loss: 6.3337 - acc: 0.607 - ETA: 3s - loss: 5.9117 - acc: 0.633 - ETA: 3s - loss: 5.9281 - acc: 0.632 - ETA: 3s - loss: 6.1560 - acc: 0.618 - ETA: 3s - loss: 6.0607 - acc: 0.624 - ETA: 3s - loss: 5.9546 - acc: 0.630 - ETA: 3s - loss: 5.9559 - acc: 0.630 - ETA: 3s - loss: 5.9043 - acc: 0.633 - ETA: 3s - loss: 5.8751 - acc: 0.635 - ETA: 3s - loss: 5.7573 - acc: 0.642 - ETA: 3s - loss: 5.7029 - acc: 0.646 - ETA: 3s - loss: 5.8366 - acc: 0.637 - ETA: 2s - loss: 5.8057 - acc: 0.639 - ETA: 2s - loss: 5.8435 - acc: 0.637 - ETA: 2s - loss: 5.8166 - acc: 0.638 - ETA: 2s - loss: 5.8533 - acc: 0.636 - ETA: 2s - loss: 5.8597 - acc: 0.636 - ETA: 2s - loss: 5.8411 - acc: 0.637 - ETA: 2s - loss: 5.8221 - acc: 0.638 - ETA: 2s - loss: 5.7325 - acc: 0.644 - ETA: 2s - loss: 5.7281 - acc: 0.644 - ETA: 2s - loss: 5.7464 - acc: 0.643 - ETA: 2s - loss: 5.7419 - acc: 0.643 - ETA: 2s - loss: 5.7713 - acc: 0.641 - ETA: 2s - loss: 5.7725 - acc: 0.641 - ETA: 2s - loss: 5.7737 - acc: 0.641 - ETA: 2s - loss: 5.8011 - acc: 0.639 - ETA: 2s - loss: 5.7552 - acc: 0.642 - ETA: 2s - loss: 5.7672 - acc: 0.641 - ETA: 1s - loss: 5.7897 - acc: 0.640 - ETA: 1s - loss: 5.8160 - acc: 0.638 - ETA: 1s - loss: 5.7910 - acc: 0.640 - ETA: 1s - loss: 5.7868 - acc: 0.640 - ETA: 1s - loss: 5.7919 - acc: 0.639 - ETA: 1s - loss: 5.7877 - acc: 0.640 - ETA: 1s - loss: 5.7785 - acc: 0.640 - ETA: 1s - loss: 5.7878 - acc: 0.640 - ETA: 1s - loss: 5.7798 - acc: 0.640 - ETA: 1s - loss: 5.7553 - acc: 0.642 - ETA: 1s - loss: 5.7334 - acc: 0.643 - ETA: 1s - loss: 5.7352 - acc: 0.643 - ETA: 1s - loss: 5.7346 - acc: 0.643 - ETA: 1s - loss: 5.7251 - acc: 0.643 - ETA: 1s - loss: 5.7304 - acc: 0.643 - ETA: 1s - loss: 5.7390 - acc: 0.643 - ETA: 1s - loss: 5.7611 - acc: 0.641 - ETA: 1s - loss: 5.7557 - acc: 0.642 - ETA: 0s - loss: 5.7693 - acc: 0.640 - ETA: 0s - loss: 5.7663 - acc: 0.641 - ETA: 0s - loss: 5.7672 - acc: 0.640 - ETA: 0s - loss: 5.7679 - acc: 0.640 - ETA: 0s - loss: 5.7808 - acc: 0.640 - ETA: 0s - loss: 5.7992 - acc: 0.639 - ETA: 0s - loss: 5.7963 - acc: 0.639 - ETA: 0s - loss: 5.8282 - acc: 0.637 - ETA: 0s - loss: 5.8391 - acc: 0.636 - ETA: 0s - loss: 5.8100 - acc: 0.638 - ETA: 0s - loss: 5.8262 - acc: 0.637 - ETA: 0s - loss: 5.8065 - acc: 0.638 - ETA: 0s - loss: 5.8180 - acc: 0.637 - ETA: 0s - loss: 5.8126 - acc: 0.638 - ETA: 0s - loss: 5.8150 - acc: 0.638 - ETA: 0s - loss: 5.8241 - acc: 0.637 - ETA: 0s - loss: 5.8286 - acc: 0.637 - ETA: 0s - loss: 5.8380 - acc: 0.6366Epoch 00082: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 5.8193 - acc: 0.6377 - val_loss: 7.3225 - val_acc: 0.4970\n",
      "Epoch 84/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6560/6680 [============================>.] - ETA: 3s - loss: 5.6413 - acc: 0.650 - ETA: 3s - loss: 5.1042 - acc: 0.683 - ETA: 3s - loss: 5.3484 - acc: 0.668 - ETA: 3s - loss: 5.6918 - acc: 0.646 - ETA: 3s - loss: 5.8382 - acc: 0.635 - ETA: 3s - loss: 5.7384 - acc: 0.642 - ETA: 3s - loss: 5.7227 - acc: 0.643 - ETA: 3s - loss: 5.5323 - acc: 0.655 - ETA: 3s - loss: 5.5095 - acc: 0.657 - ETA: 3s - loss: 5.5750 - acc: 0.653 - ETA: 2s - loss: 5.6589 - acc: 0.648 - ETA: 3s - loss: 5.6864 - acc: 0.646 - ETA: 2s - loss: 5.7355 - acc: 0.643 - ETA: 2s - loss: 5.7872 - acc: 0.640 - ETA: 2s - loss: 5.8667 - acc: 0.635 - ETA: 2s - loss: 5.9630 - acc: 0.629 - ETA: 2s - loss: 5.9242 - acc: 0.631 - ETA: 2s - loss: 5.8715 - acc: 0.635 - ETA: 2s - loss: 5.8504 - acc: 0.636 - ETA: 2s - loss: 5.7904 - acc: 0.640 - ETA: 2s - loss: 5.8223 - acc: 0.638 - ETA: 2s - loss: 5.8198 - acc: 0.638 - ETA: 2s - loss: 5.8331 - acc: 0.637 - ETA: 2s - loss: 5.8386 - acc: 0.637 - ETA: 2s - loss: 5.8567 - acc: 0.636 - ETA: 2s - loss: 5.8920 - acc: 0.634 - ETA: 2s - loss: 5.8582 - acc: 0.635 - ETA: 2s - loss: 5.8925 - acc: 0.633 - ETA: 2s - loss: 5.8838 - acc: 0.634 - ETA: 1s - loss: 5.8976 - acc: 0.633 - ETA: 1s - loss: 5.8945 - acc: 0.633 - ETA: 1s - loss: 5.8612 - acc: 0.635 - ETA: 1s - loss: 5.8950 - acc: 0.633 - ETA: 1s - loss: 5.8875 - acc: 0.633 - ETA: 1s - loss: 5.8897 - acc: 0.633 - ETA: 1s - loss: 5.8873 - acc: 0.633 - ETA: 1s - loss: 5.8880 - acc: 0.633 - ETA: 1s - loss: 5.8730 - acc: 0.634 - ETA: 1s - loss: 5.8906 - acc: 0.633 - ETA: 1s - loss: 5.8473 - acc: 0.636 - ETA: 1s - loss: 5.8073 - acc: 0.638 - ETA: 1s - loss: 5.8026 - acc: 0.639 - ETA: 1s - loss: 5.8193 - acc: 0.638 - ETA: 1s - loss: 5.8306 - acc: 0.637 - ETA: 1s - loss: 5.8411 - acc: 0.636 - ETA: 1s - loss: 5.8473 - acc: 0.636 - ETA: 1s - loss: 5.8463 - acc: 0.636 - ETA: 0s - loss: 5.8554 - acc: 0.635 - ETA: 0s - loss: 5.8381 - acc: 0.636 - ETA: 0s - loss: 5.8279 - acc: 0.637 - ETA: 0s - loss: 5.8305 - acc: 0.637 - ETA: 0s - loss: 5.8323 - acc: 0.637 - ETA: 0s - loss: 5.8329 - acc: 0.637 - ETA: 0s - loss: 5.8060 - acc: 0.638 - ETA: 0s - loss: 5.8347 - acc: 0.637 - ETA: 0s - loss: 5.8504 - acc: 0.636 - ETA: 0s - loss: 5.8412 - acc: 0.636 - ETA: 0s - loss: 5.8430 - acc: 0.636 - ETA: 0s - loss: 5.8290 - acc: 0.637 - ETA: 0s - loss: 5.8330 - acc: 0.637 - ETA: 0s - loss: 5.8429 - acc: 0.636 - ETA: 0s - loss: 5.8492 - acc: 0.636 - ETA: 0s - loss: 5.8261 - acc: 0.637 - ETA: 0s - loss: 5.8183 - acc: 0.6380Epoch 00083: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 5.8151 - acc: 0.6382 - val_loss: 7.1440 - val_acc: 0.5054\n",
      "Epoch 85/100\n",
      "6600/6680 [============================>.] - ETA: 3s - loss: 5.6413 - acc: 0.650 - ETA: 3s - loss: 5.4111 - acc: 0.664 - ETA: 3s - loss: 5.2694 - acc: 0.673 - ETA: 3s - loss: 5.4622 - acc: 0.661 - ETA: 3s - loss: 5.6063 - acc: 0.652 - ETA: 3s - loss: 5.5858 - acc: 0.653 - ETA: 3s - loss: 5.6653 - acc: 0.648 - ETA: 3s - loss: 5.6622 - acc: 0.648 - ETA: 2s - loss: 5.7669 - acc: 0.642 - ETA: 2s - loss: 5.7061 - acc: 0.646 - ETA: 2s - loss: 5.6704 - acc: 0.648 - ETA: 2s - loss: 5.7341 - acc: 0.644 - ETA: 2s - loss: 5.7881 - acc: 0.640 - ETA: 2s - loss: 5.7437 - acc: 0.643 - ETA: 2s - loss: 5.7476 - acc: 0.643 - ETA: 2s - loss: 5.7020 - acc: 0.646 - ETA: 2s - loss: 5.6889 - acc: 0.647 - ETA: 2s - loss: 5.6863 - acc: 0.647 - ETA: 2s - loss: 5.6755 - acc: 0.647 - ETA: 2s - loss: 5.6899 - acc: 0.647 - ETA: 2s - loss: 5.7029 - acc: 0.646 - ETA: 2s - loss: 5.6778 - acc: 0.647 - ETA: 2s - loss: 5.6207 - acc: 0.651 - ETA: 2s - loss: 5.6349 - acc: 0.650 - ETA: 2s - loss: 5.6031 - acc: 0.652 - ETA: 2s - loss: 5.5923 - acc: 0.653 - ETA: 2s - loss: 5.5822 - acc: 0.653 - ETA: 2s - loss: 5.6129 - acc: 0.651 - ETA: 1s - loss: 5.6689 - acc: 0.648 - ETA: 1s - loss: 5.6468 - acc: 0.649 - ETA: 1s - loss: 5.6158 - acc: 0.651 - ETA: 1s - loss: 5.6266 - acc: 0.650 - ETA: 1s - loss: 5.6223 - acc: 0.651 - ETA: 1s - loss: 5.6228 - acc: 0.651 - ETA: 1s - loss: 5.6098 - acc: 0.652 - ETA: 1s - loss: 5.6503 - acc: 0.649 - ETA: 1s - loss: 5.6629 - acc: 0.648 - ETA: 1s - loss: 5.6668 - acc: 0.648 - ETA: 1s - loss: 5.6863 - acc: 0.647 - ETA: 1s - loss: 5.6655 - acc: 0.648 - ETA: 1s - loss: 5.6650 - acc: 0.648 - ETA: 1s - loss: 5.6907 - acc: 0.646 - ETA: 1s - loss: 5.6822 - acc: 0.647 - ETA: 1s - loss: 5.6918 - acc: 0.646 - ETA: 1s - loss: 5.6942 - acc: 0.646 - ETA: 1s - loss: 5.7105 - acc: 0.645 - ETA: 0s - loss: 5.7426 - acc: 0.643 - ETA: 0s - loss: 5.7274 - acc: 0.644 - ETA: 0s - loss: 5.7350 - acc: 0.643 - ETA: 0s - loss: 5.7489 - acc: 0.643 - ETA: 0s - loss: 5.7561 - acc: 0.642 - ETA: 0s - loss: 5.7600 - acc: 0.642 - ETA: 0s - loss: 5.7519 - acc: 0.642 - ETA: 0s - loss: 5.7647 - acc: 0.641 - ETA: 0s - loss: 5.7621 - acc: 0.642 - ETA: 0s - loss: 5.7628 - acc: 0.642 - ETA: 0s - loss: 5.7607 - acc: 0.642 - ETA: 0s - loss: 5.7394 - acc: 0.643 - ETA: 0s - loss: 5.7484 - acc: 0.642 - ETA: 0s - loss: 5.7571 - acc: 0.642 - ETA: 0s - loss: 5.7656 - acc: 0.641 - ETA: 0s - loss: 5.7759 - acc: 0.641 - ETA: 0s - loss: 5.7713 - acc: 0.641 - ETA: 0s - loss: 5.7814 - acc: 0.6408Epoch 00084: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 5.8111 - acc: 0.6389 - val_loss: 7.2087 - val_acc: 0.5066\n",
      "Epoch 86/100\n",
      "6600/6680 [============================>.] - ETA: 3s - loss: 6.4472 - acc: 0.600 - ETA: 4s - loss: 6.4472 - acc: 0.600 - ETA: 4s - loss: 6.5368 - acc: 0.594 - ETA: 4s - loss: 6.2745 - acc: 0.610 - ETA: 4s - loss: 5.8652 - acc: 0.636 - ETA: 4s - loss: 5.7815 - acc: 0.641 - ETA: 3s - loss: 5.6413 - acc: 0.650 - ETA: 3s - loss: 5.6650 - acc: 0.648 - ETA: 3s - loss: 5.8480 - acc: 0.637 - ETA: 3s - loss: 5.9100 - acc: 0.633 - ETA: 3s - loss: 5.8831 - acc: 0.635 - ETA: 3s - loss: 5.8801 - acc: 0.635 - ETA: 3s - loss: 5.9503 - acc: 0.630 - ETA: 3s - loss: 5.9513 - acc: 0.630 - ETA: 2s - loss: 5.9181 - acc: 0.632 - ETA: 2s - loss: 5.9598 - acc: 0.630 - ETA: 2s - loss: 5.9464 - acc: 0.631 - ETA: 2s - loss: 5.8640 - acc: 0.636 - ETA: 2s - loss: 5.9570 - acc: 0.630 - ETA: 2s - loss: 5.9490 - acc: 0.630 - ETA: 2s - loss: 5.9025 - acc: 0.633 - ETA: 2s - loss: 5.8796 - acc: 0.635 - ETA: 2s - loss: 5.8468 - acc: 0.637 - ETA: 2s - loss: 5.8467 - acc: 0.637 - ETA: 2s - loss: 5.8635 - acc: 0.636 - ETA: 2s - loss: 5.8529 - acc: 0.636 - ETA: 2s - loss: 5.8079 - acc: 0.639 - ETA: 2s - loss: 5.8018 - acc: 0.640 - ETA: 2s - loss: 5.7790 - acc: 0.641 - ETA: 2s - loss: 5.7632 - acc: 0.642 - ETA: 2s - loss: 5.7707 - acc: 0.642 - ETA: 1s - loss: 5.7353 - acc: 0.644 - ETA: 1s - loss: 5.7576 - acc: 0.642 - ETA: 1s - loss: 5.7679 - acc: 0.642 - ETA: 1s - loss: 5.7642 - acc: 0.642 - ETA: 1s - loss: 5.7783 - acc: 0.641 - ETA: 1s - loss: 5.7878 - acc: 0.640 - ETA: 1s - loss: 5.7760 - acc: 0.641 - ETA: 1s - loss: 5.7767 - acc: 0.641 - ETA: 1s - loss: 5.7644 - acc: 0.642 - ETA: 1s - loss: 5.7653 - acc: 0.642 - ETA: 1s - loss: 5.7507 - acc: 0.643 - ETA: 1s - loss: 5.7519 - acc: 0.643 - ETA: 1s - loss: 5.7424 - acc: 0.643 - ETA: 1s - loss: 5.7037 - acc: 0.646 - ETA: 1s - loss: 5.6880 - acc: 0.647 - ETA: 1s - loss: 5.6972 - acc: 0.646 - ETA: 1s - loss: 5.7403 - acc: 0.643 - ETA: 1s - loss: 5.7515 - acc: 0.643 - ETA: 0s - loss: 5.7586 - acc: 0.642 - ETA: 0s - loss: 5.7722 - acc: 0.641 - ETA: 0s - loss: 5.8009 - acc: 0.640 - ETA: 0s - loss: 5.7912 - acc: 0.640 - ETA: 0s - loss: 5.7854 - acc: 0.641 - ETA: 0s - loss: 5.7960 - acc: 0.640 - ETA: 0s - loss: 5.7845 - acc: 0.640 - ETA: 0s - loss: 5.7801 - acc: 0.641 - ETA: 0s - loss: 5.7979 - acc: 0.640 - ETA: 0s - loss: 5.8147 - acc: 0.639 - ETA: 0s - loss: 5.8309 - acc: 0.638 - ETA: 0s - loss: 5.8338 - acc: 0.637 - ETA: 0s - loss: 5.8306 - acc: 0.638 - ETA: 0s - loss: 5.8250 - acc: 0.638 - ETA: 0s - loss: 5.8220 - acc: 0.638 - ETA: 0s - loss: 5.8463 - acc: 0.637 - ETA: 0s - loss: 5.8327 - acc: 0.637 - ETA: 0s - loss: 5.8328 - acc: 0.6379Epoch 00085: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 5.8112 - acc: 0.6392 - val_loss: 7.1465 - val_acc: 0.5078\n",
      "Epoch 87/100\n",
      "6620/6680 [============================>.] - ETA: 2s - loss: 4.8354 - acc: 0.700 - ETA: 3s - loss: 5.7757 - acc: 0.641 - ETA: 3s - loss: 6.1249 - acc: 0.620 - ETA: 3s - loss: 6.1594 - acc: 0.617 - ETA: 3s - loss: 6.0231 - acc: 0.626 - ETA: 3s - loss: 5.8764 - acc: 0.635 - ETA: 3s - loss: 5.8429 - acc: 0.637 - ETA: 3s - loss: 5.7879 - acc: 0.640 - ETA: 3s - loss: 5.9171 - acc: 0.632 - ETA: 3s - loss: 5.9225 - acc: 0.632 - ETA: 3s - loss: 5.9436 - acc: 0.631 - ETA: 3s - loss: 5.8055 - acc: 0.639 - ETA: 3s - loss: 5.8599 - acc: 0.636 - ETA: 2s - loss: 5.8806 - acc: 0.635 - ETA: 2s - loss: 5.8516 - acc: 0.637 - ETA: 2s - loss: 5.8778 - acc: 0.635 - ETA: 2s - loss: 5.7723 - acc: 0.641 - ETA: 2s - loss: 5.6888 - acc: 0.647 - ETA: 2s - loss: 5.7299 - acc: 0.644 - ETA: 2s - loss: 5.8093 - acc: 0.639 - ETA: 2s - loss: 5.7930 - acc: 0.640 - ETA: 2s - loss: 5.7488 - acc: 0.643 - ETA: 2s - loss: 5.7720 - acc: 0.641 - ETA: 2s - loss: 5.7525 - acc: 0.643 - ETA: 2s - loss: 5.7757 - acc: 0.641 - ETA: 2s - loss: 5.7961 - acc: 0.640 - ETA: 2s - loss: 5.8397 - acc: 0.637 - ETA: 2s - loss: 5.8682 - acc: 0.635 - ETA: 2s - loss: 5.8849 - acc: 0.634 - ETA: 2s - loss: 5.8555 - acc: 0.636 - ETA: 2s - loss: 5.8552 - acc: 0.636 - ETA: 2s - loss: 5.8122 - acc: 0.639 - ETA: 2s - loss: 5.8325 - acc: 0.638 - ETA: 1s - loss: 5.8316 - acc: 0.638 - ETA: 1s - loss: 5.8890 - acc: 0.634 - ETA: 1s - loss: 5.8959 - acc: 0.634 - ETA: 1s - loss: 5.9161 - acc: 0.633 - ETA: 1s - loss: 5.8952 - acc: 0.634 - ETA: 1s - loss: 5.8710 - acc: 0.635 - ETA: 1s - loss: 5.8776 - acc: 0.635 - ETA: 1s - loss: 5.8593 - acc: 0.636 - ETA: 1s - loss: 5.8539 - acc: 0.636 - ETA: 1s - loss: 5.8088 - acc: 0.639 - ETA: 1s - loss: 5.8276 - acc: 0.638 - ETA: 1s - loss: 5.8308 - acc: 0.638 - ETA: 1s - loss: 5.8265 - acc: 0.638 - ETA: 1s - loss: 5.8252 - acc: 0.638 - ETA: 1s - loss: 5.8324 - acc: 0.638 - ETA: 1s - loss: 5.8242 - acc: 0.638 - ETA: 0s - loss: 5.8230 - acc: 0.638 - ETA: 0s - loss: 5.8258 - acc: 0.638 - ETA: 0s - loss: 5.8222 - acc: 0.638 - ETA: 0s - loss: 5.8211 - acc: 0.638 - ETA: 0s - loss: 5.7965 - acc: 0.640 - ETA: 0s - loss: 5.8085 - acc: 0.639 - ETA: 0s - loss: 5.7932 - acc: 0.640 - ETA: 0s - loss: 5.7962 - acc: 0.640 - ETA: 0s - loss: 5.8048 - acc: 0.639 - ETA: 0s - loss: 5.7964 - acc: 0.640 - ETA: 0s - loss: 5.7802 - acc: 0.641 - ETA: 0s - loss: 5.7913 - acc: 0.640 - ETA: 0s - loss: 5.7862 - acc: 0.641 - ETA: 0s - loss: 5.7787 - acc: 0.641 - ETA: 0s - loss: 5.7918 - acc: 0.640 - ETA: 0s - loss: 5.7819 - acc: 0.641 - ETA: 0s - loss: 5.7897 - acc: 0.640 - ETA: 0s - loss: 5.7971 - acc: 0.6403Epoch 00086: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 5.8102 - acc: 0.6395 - val_loss: 7.1448 - val_acc: 0.5102\n",
      "Epoch 88/100\n",
      "6660/6680 [============================>.] - ETA: 3s - loss: 4.8354 - acc: 0.700 - ETA: 3s - loss: 5.7757 - acc: 0.641 - ETA: 3s - loss: 6.0443 - acc: 0.625 - ETA: 3s - loss: 6.4472 - acc: 0.600 - ETA: 3s - loss: 6.3740 - acc: 0.604 - ETA: 3s - loss: 6.1189 - acc: 0.620 - ETA: 3s - loss: 6.0947 - acc: 0.621 - ETA: 3s - loss: 6.0116 - acc: 0.627 - ETA: 3s - loss: 6.0251 - acc: 0.626 - ETA: 3s - loss: 6.1214 - acc: 0.620 - ETA: 2s - loss: 5.9607 - acc: 0.630 - ETA: 2s - loss: 5.8676 - acc: 0.636 - ETA: 2s - loss: 5.8623 - acc: 0.636 - ETA: 2s - loss: 5.8939 - acc: 0.634 - ETA: 2s - loss: 5.9394 - acc: 0.631 - ETA: 2s - loss: 5.9344 - acc: 0.631 - ETA: 2s - loss: 6.0148 - acc: 0.626 - ETA: 2s - loss: 6.0026 - acc: 0.627 - ETA: 2s - loss: 5.9966 - acc: 0.628 - ETA: 2s - loss: 5.9785 - acc: 0.629 - ETA: 2s - loss: 6.0091 - acc: 0.627 - ETA: 2s - loss: 5.9995 - acc: 0.627 - ETA: 2s - loss: 5.9409 - acc: 0.631 - ETA: 2s - loss: 5.9623 - acc: 0.630 - ETA: 2s - loss: 6.0148 - acc: 0.626 - ETA: 2s - loss: 5.9889 - acc: 0.628 - ETA: 2s - loss: 5.9473 - acc: 0.630 - ETA: 2s - loss: 5.9420 - acc: 0.630 - ETA: 1s - loss: 5.9519 - acc: 0.630 - ETA: 1s - loss: 5.9631 - acc: 0.629 - ETA: 1s - loss: 5.9474 - acc: 0.630 - ETA: 1s - loss: 6.0014 - acc: 0.627 - ETA: 1s - loss: 5.9759 - acc: 0.628 - ETA: 1s - loss: 5.9376 - acc: 0.631 - ETA: 1s - loss: 5.9337 - acc: 0.631 - ETA: 1s - loss: 5.8808 - acc: 0.634 - ETA: 1s - loss: 5.8787 - acc: 0.634 - ETA: 1s - loss: 5.8809 - acc: 0.634 - ETA: 1s - loss: 5.8983 - acc: 0.633 - ETA: 1s - loss: 5.8827 - acc: 0.634 - ETA: 1s - loss: 5.8807 - acc: 0.634 - ETA: 1s - loss: 5.8371 - acc: 0.637 - ETA: 1s - loss: 5.8334 - acc: 0.637 - ETA: 1s - loss: 5.8437 - acc: 0.637 - ETA: 1s - loss: 5.8249 - acc: 0.638 - ETA: 1s - loss: 5.8237 - acc: 0.638 - ETA: 1s - loss: 5.8266 - acc: 0.638 - ETA: 0s - loss: 5.8195 - acc: 0.638 - ETA: 0s - loss: 5.8383 - acc: 0.637 - ETA: 0s - loss: 5.8408 - acc: 0.637 - ETA: 0s - loss: 5.8307 - acc: 0.637 - ETA: 0s - loss: 5.7989 - acc: 0.639 - ETA: 0s - loss: 5.8110 - acc: 0.639 - ETA: 0s - loss: 5.8343 - acc: 0.637 - ETA: 0s - loss: 5.8417 - acc: 0.637 - ETA: 0s - loss: 5.8431 - acc: 0.637 - ETA: 0s - loss: 5.8341 - acc: 0.637 - ETA: 0s - loss: 5.8470 - acc: 0.636 - ETA: 0s - loss: 5.8270 - acc: 0.638 - ETA: 0s - loss: 5.8344 - acc: 0.637 - ETA: 0s - loss: 5.8397 - acc: 0.637 - ETA: 0s - loss: 5.8410 - acc: 0.637 - ETA: 0s - loss: 5.8180 - acc: 0.638 - ETA: 0s - loss: 5.8055 - acc: 0.639 - ETA: 0s - loss: 5.8103 - acc: 0.6390Epoch 00087: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 5.8097 - acc: 0.6391 - val_loss: 7.1093 - val_acc: 0.5102\n",
      "Epoch 89/100\n",
      "6580/6680 [============================>.] - ETA: 3s - loss: 6.4472 - acc: 0.600 - ETA: 3s - loss: 6.5624 - acc: 0.592 - ETA: 3s - loss: 5.9100 - acc: 0.633 - ETA: 3s - loss: 5.8652 - acc: 0.636 - ETA: 3s - loss: 5.8764 - acc: 0.635 - ETA: 3s - loss: 5.7803 - acc: 0.641 - ETA: 3s - loss: 5.5702 - acc: 0.654 - ETA: 2s - loss: 5.6413 - acc: 0.650 - ETA: 2s - loss: 5.6047 - acc: 0.652 - ETA: 2s - loss: 5.6907 - acc: 0.646 - ETA: 2s - loss: 5.7022 - acc: 0.646 - ETA: 2s - loss: 5.7803 - acc: 0.641 - ETA: 2s - loss: 5.7693 - acc: 0.642 - ETA: 2s - loss: 5.6764 - acc: 0.647 - ETA: 2s - loss: 5.7394 - acc: 0.643 - ETA: 2s - loss: 5.6719 - acc: 0.648 - ETA: 2s - loss: 5.7469 - acc: 0.643 - ETA: 2s - loss: 5.7772 - acc: 0.641 - ETA: 2s - loss: 5.7357 - acc: 0.644 - ETA: 2s - loss: 5.7945 - acc: 0.640 - ETA: 2s - loss: 5.7266 - acc: 0.644 - ETA: 2s - loss: 5.7806 - acc: 0.641 - ETA: 2s - loss: 5.7873 - acc: 0.640 - ETA: 2s - loss: 5.7746 - acc: 0.641 - ETA: 2s - loss: 5.7629 - acc: 0.642 - ETA: 2s - loss: 5.7398 - acc: 0.643 - ETA: 2s - loss: 5.7184 - acc: 0.645 - ETA: 2s - loss: 5.7328 - acc: 0.644 - ETA: 1s - loss: 5.7683 - acc: 0.642 - ETA: 1s - loss: 5.7961 - acc: 0.640 - ETA: 1s - loss: 5.7453 - acc: 0.643 - ETA: 1s - loss: 5.7522 - acc: 0.643 - ETA: 1s - loss: 5.7683 - acc: 0.642 - ETA: 1s - loss: 5.7883 - acc: 0.640 - ETA: 1s - loss: 5.8154 - acc: 0.639 - ETA: 1s - loss: 5.7919 - acc: 0.640 - ETA: 1s - loss: 5.7620 - acc: 0.642 - ETA: 1s - loss: 5.7841 - acc: 0.641 - ETA: 1s - loss: 5.8050 - acc: 0.639 - ETA: 1s - loss: 5.7930 - acc: 0.640 - ETA: 1s - loss: 5.7731 - acc: 0.641 - ETA: 1s - loss: 5.7845 - acc: 0.641 - ETA: 1s - loss: 5.7812 - acc: 0.641 - ETA: 1s - loss: 5.7817 - acc: 0.641 - ETA: 1s - loss: 5.7675 - acc: 0.642 - ETA: 1s - loss: 5.7648 - acc: 0.642 - ETA: 0s - loss: 5.7925 - acc: 0.640 - ETA: 0s - loss: 5.8190 - acc: 0.639 - ETA: 0s - loss: 5.8251 - acc: 0.638 - ETA: 0s - loss: 5.8082 - acc: 0.639 - ETA: 0s - loss: 5.8235 - acc: 0.638 - ETA: 0s - loss: 5.7837 - acc: 0.641 - ETA: 0s - loss: 5.7954 - acc: 0.640 - ETA: 0s - loss: 5.7747 - acc: 0.641 - ETA: 0s - loss: 5.7780 - acc: 0.641 - ETA: 0s - loss: 5.7701 - acc: 0.642 - ETA: 0s - loss: 5.7679 - acc: 0.642 - ETA: 0s - loss: 5.7847 - acc: 0.641 - ETA: 0s - loss: 5.7951 - acc: 0.640 - ETA: 0s - loss: 5.8096 - acc: 0.639 - ETA: 0s - loss: 5.8049 - acc: 0.639 - ETA: 0s - loss: 5.8024 - acc: 0.640 - ETA: 0s - loss: 5.7949 - acc: 0.640 - ETA: 0s - loss: 5.8097 - acc: 0.6395Epoch 00088: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 5.8096 - acc: 0.6395 - val_loss: 7.1636 - val_acc: 0.5030\n",
      "Epoch 90/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6600/6680 [============================>.] - ETA: 3s - loss: 8.0590 - acc: 0.500 - ETA: 4s - loss: 7.0920 - acc: 0.560 - ETA: 3s - loss: 6.6084 - acc: 0.590 - ETA: 3s - loss: 6.1786 - acc: 0.616 - ETA: 3s - loss: 6.1927 - acc: 0.615 - ETA: 3s - loss: 6.1450 - acc: 0.618 - ETA: 3s - loss: 6.1306 - acc: 0.619 - ETA: 3s - loss: 6.0811 - acc: 0.622 - ETA: 3s - loss: 6.2141 - acc: 0.614 - ETA: 3s - loss: 6.0210 - acc: 0.625 - ETA: 3s - loss: 6.0654 - acc: 0.622 - ETA: 3s - loss: 6.0406 - acc: 0.624 - ETA: 3s - loss: 6.0340 - acc: 0.625 - ETA: 3s - loss: 5.9133 - acc: 0.632 - ETA: 3s - loss: 5.9091 - acc: 0.632 - ETA: 3s - loss: 6.0136 - acc: 0.626 - ETA: 3s - loss: 6.0470 - acc: 0.624 - ETA: 3s - loss: 6.0570 - acc: 0.623 - ETA: 2s - loss: 6.0180 - acc: 0.626 - ETA: 2s - loss: 6.1107 - acc: 0.620 - ETA: 2s - loss: 6.0903 - acc: 0.621 - ETA: 2s - loss: 6.0423 - acc: 0.624 - ETA: 2s - loss: 6.0544 - acc: 0.624 - ETA: 2s - loss: 5.9920 - acc: 0.627 - ETA: 2s - loss: 5.9865 - acc: 0.628 - ETA: 2s - loss: 5.9784 - acc: 0.628 - ETA: 2s - loss: 5.8958 - acc: 0.633 - ETA: 2s - loss: 5.9073 - acc: 0.633 - ETA: 2s - loss: 5.9095 - acc: 0.633 - ETA: 2s - loss: 5.8688 - acc: 0.635 - ETA: 2s - loss: 5.8562 - acc: 0.636 - ETA: 2s - loss: 5.8558 - acc: 0.636 - ETA: 2s - loss: 5.8722 - acc: 0.635 - ETA: 2s - loss: 5.8785 - acc: 0.635 - ETA: 2s - loss: 5.8707 - acc: 0.635 - ETA: 2s - loss: 5.8483 - acc: 0.636 - ETA: 2s - loss: 5.7971 - acc: 0.639 - ETA: 2s - loss: 5.7973 - acc: 0.639 - ETA: 1s - loss: 5.7927 - acc: 0.640 - ETA: 1s - loss: 5.8021 - acc: 0.639 - ETA: 1s - loss: 5.8376 - acc: 0.637 - ETA: 1s - loss: 5.8420 - acc: 0.637 - ETA: 1s - loss: 5.8516 - acc: 0.636 - ETA: 1s - loss: 5.8514 - acc: 0.636 - ETA: 1s - loss: 5.8583 - acc: 0.636 - ETA: 1s - loss: 5.8927 - acc: 0.634 - ETA: 1s - loss: 5.8983 - acc: 0.633 - ETA: 1s - loss: 5.8744 - acc: 0.635 - ETA: 1s - loss: 5.8925 - acc: 0.634 - ETA: 1s - loss: 5.8722 - acc: 0.635 - ETA: 1s - loss: 5.8789 - acc: 0.634 - ETA: 1s - loss: 5.9192 - acc: 0.632 - ETA: 1s - loss: 5.9489 - acc: 0.630 - ETA: 1s - loss: 5.9235 - acc: 0.632 - ETA: 1s - loss: 5.9021 - acc: 0.633 - ETA: 1s - loss: 5.8913 - acc: 0.634 - ETA: 1s - loss: 5.8970 - acc: 0.633 - ETA: 0s - loss: 5.9015 - acc: 0.633 - ETA: 0s - loss: 5.9089 - acc: 0.633 - ETA: 0s - loss: 5.9008 - acc: 0.633 - ETA: 0s - loss: 5.9089 - acc: 0.633 - ETA: 0s - loss: 5.8991 - acc: 0.633 - ETA: 0s - loss: 5.9070 - acc: 0.633 - ETA: 0s - loss: 5.9013 - acc: 0.633 - ETA: 0s - loss: 5.8834 - acc: 0.634 - ETA: 0s - loss: 5.8765 - acc: 0.635 - ETA: 0s - loss: 5.8670 - acc: 0.635 - ETA: 0s - loss: 5.8794 - acc: 0.634 - ETA: 0s - loss: 5.8682 - acc: 0.635 - ETA: 0s - loss: 5.8855 - acc: 0.634 - ETA: 0s - loss: 5.8540 - acc: 0.636 - ETA: 0s - loss: 5.8506 - acc: 0.636 - ETA: 0s - loss: 5.8480 - acc: 0.636 - ETA: 0s - loss: 5.8207 - acc: 0.638 - ETA: 0s - loss: 5.8131 - acc: 0.6391Epoch 00089: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 5.8086 - acc: 0.6394 - val_loss: 7.2117 - val_acc: 0.5054\n",
      "Epoch 91/100\n",
      "6620/6680 [============================>.] - ETA: 4s - loss: 4.8354 - acc: 0.700 - ETA: 3s - loss: 5.3727 - acc: 0.666 - ETA: 3s - loss: 5.7146 - acc: 0.645 - ETA: 3s - loss: 5.7924 - acc: 0.640 - ETA: 3s - loss: 5.8025 - acc: 0.640 - ETA: 3s - loss: 5.9100 - acc: 0.633 - ETA: 3s - loss: 5.8716 - acc: 0.635 - ETA: 3s - loss: 5.7973 - acc: 0.640 - ETA: 3s - loss: 5.7720 - acc: 0.641 - ETA: 3s - loss: 5.7565 - acc: 0.642 - ETA: 3s - loss: 5.7614 - acc: 0.642 - ETA: 3s - loss: 5.7174 - acc: 0.645 - ETA: 3s - loss: 5.7664 - acc: 0.642 - ETA: 3s - loss: 5.7565 - acc: 0.642 - ETA: 3s - loss: 5.7243 - acc: 0.644 - ETA: 2s - loss: 5.7628 - acc: 0.642 - ETA: 2s - loss: 5.8273 - acc: 0.638 - ETA: 2s - loss: 5.8161 - acc: 0.639 - ETA: 2s - loss: 5.8520 - acc: 0.636 - ETA: 2s - loss: 5.8146 - acc: 0.639 - ETA: 2s - loss: 5.8324 - acc: 0.638 - ETA: 2s - loss: 5.8073 - acc: 0.639 - ETA: 2s - loss: 5.8146 - acc: 0.639 - ETA: 2s - loss: 5.8284 - acc: 0.638 - ETA: 2s - loss: 5.8411 - acc: 0.637 - ETA: 2s - loss: 5.8065 - acc: 0.639 - ETA: 2s - loss: 5.7936 - acc: 0.640 - ETA: 2s - loss: 5.7582 - acc: 0.642 - ETA: 2s - loss: 5.7302 - acc: 0.644 - ETA: 2s - loss: 5.7548 - acc: 0.643 - ETA: 2s - loss: 5.7345 - acc: 0.644 - ETA: 2s - loss: 5.7527 - acc: 0.643 - ETA: 1s - loss: 5.7586 - acc: 0.642 - ETA: 1s - loss: 5.7452 - acc: 0.643 - ETA: 1s - loss: 5.7606 - acc: 0.642 - ETA: 1s - loss: 5.7340 - acc: 0.644 - ETA: 1s - loss: 5.7219 - acc: 0.645 - ETA: 1s - loss: 5.7372 - acc: 0.644 - ETA: 1s - loss: 5.7510 - acc: 0.643 - ETA: 1s - loss: 5.7400 - acc: 0.643 - ETA: 1s - loss: 5.7456 - acc: 0.643 - ETA: 1s - loss: 5.7309 - acc: 0.644 - ETA: 1s - loss: 5.7554 - acc: 0.642 - ETA: 1s - loss: 5.7495 - acc: 0.643 - ETA: 1s - loss: 5.7617 - acc: 0.642 - ETA: 1s - loss: 5.7554 - acc: 0.642 - ETA: 1s - loss: 5.7669 - acc: 0.642 - ETA: 1s - loss: 5.7779 - acc: 0.641 - ETA: 1s - loss: 5.7517 - acc: 0.643 - ETA: 0s - loss: 5.7265 - acc: 0.644 - ETA: 0s - loss: 5.7280 - acc: 0.644 - ETA: 0s - loss: 5.7043 - acc: 0.646 - ETA: 0s - loss: 5.7216 - acc: 0.645 - ETA: 0s - loss: 5.7353 - acc: 0.644 - ETA: 0s - loss: 5.7424 - acc: 0.643 - ETA: 0s - loss: 5.7606 - acc: 0.642 - ETA: 0s - loss: 5.7471 - acc: 0.643 - ETA: 0s - loss: 5.7537 - acc: 0.643 - ETA: 0s - loss: 5.7490 - acc: 0.643 - ETA: 0s - loss: 5.7662 - acc: 0.642 - ETA: 0s - loss: 5.7881 - acc: 0.640 - ETA: 0s - loss: 5.7910 - acc: 0.640 - ETA: 0s - loss: 5.7989 - acc: 0.640 - ETA: 0s - loss: 5.7964 - acc: 0.640 - ETA: 0s - loss: 5.7990 - acc: 0.640 - ETA: 0s - loss: 5.7991 - acc: 0.640 - ETA: 0s - loss: 5.7996 - acc: 0.6402Epoch 00090: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 5.8102 - acc: 0.6395 - val_loss: 7.1460 - val_acc: 0.5078\n",
      "Epoch 92/100\n",
      "6640/6680 [============================>.] - ETA: 5s - loss: 6.4472 - acc: 0.600 - ETA: 3s - loss: 5.9100 - acc: 0.633 - ETA: 3s - loss: 5.7146 - acc: 0.645 - ETA: 3s - loss: 5.6887 - acc: 0.647 - ETA: 3s - loss: 5.6047 - acc: 0.652 - ETA: 3s - loss: 5.5550 - acc: 0.655 - ETA: 3s - loss: 5.5925 - acc: 0.653 - ETA: 3s - loss: 5.6838 - acc: 0.647 - ETA: 3s - loss: 5.6963 - acc: 0.646 - ETA: 3s - loss: 5.7565 - acc: 0.642 - ETA: 2s - loss: 5.9100 - acc: 0.633 - ETA: 2s - loss: 5.9368 - acc: 0.631 - ETA: 2s - loss: 5.9513 - acc: 0.630 - ETA: 2s - loss: 6.0046 - acc: 0.627 - ETA: 2s - loss: 5.9807 - acc: 0.628 - ETA: 2s - loss: 5.9597 - acc: 0.630 - ETA: 2s - loss: 5.9318 - acc: 0.632 - ETA: 2s - loss: 5.8716 - acc: 0.635 - ETA: 2s - loss: 5.8428 - acc: 0.637 - ETA: 2s - loss: 5.8967 - acc: 0.634 - ETA: 2s - loss: 5.9226 - acc: 0.632 - ETA: 2s - loss: 5.9148 - acc: 0.633 - ETA: 2s - loss: 5.9555 - acc: 0.630 - ETA: 2s - loss: 5.9493 - acc: 0.630 - ETA: 2s - loss: 5.9435 - acc: 0.631 - ETA: 2s - loss: 5.9261 - acc: 0.632 - ETA: 2s - loss: 5.9041 - acc: 0.633 - ETA: 2s - loss: 5.8797 - acc: 0.635 - ETA: 1s - loss: 5.8551 - acc: 0.636 - ETA: 1s - loss: 5.8534 - acc: 0.636 - ETA: 1s - loss: 5.8415 - acc: 0.637 - ETA: 1s - loss: 5.8503 - acc: 0.637 - ETA: 1s - loss: 5.8295 - acc: 0.638 - ETA: 1s - loss: 5.8110 - acc: 0.639 - ETA: 1s - loss: 5.7970 - acc: 0.640 - ETA: 1s - loss: 5.7963 - acc: 0.640 - ETA: 1s - loss: 5.7914 - acc: 0.640 - ETA: 1s - loss: 5.7624 - acc: 0.642 - ETA: 1s - loss: 5.7594 - acc: 0.642 - ETA: 1s - loss: 5.7604 - acc: 0.642 - ETA: 1s - loss: 5.7847 - acc: 0.641 - ETA: 1s - loss: 5.7813 - acc: 0.641 - ETA: 1s - loss: 5.7812 - acc: 0.641 - ETA: 1s - loss: 5.7816 - acc: 0.641 - ETA: 1s - loss: 5.7856 - acc: 0.641 - ETA: 1s - loss: 5.7785 - acc: 0.641 - ETA: 0s - loss: 5.7488 - acc: 0.643 - ETA: 0s - loss: 5.7499 - acc: 0.643 - ETA: 0s - loss: 5.7477 - acc: 0.643 - ETA: 0s - loss: 5.7326 - acc: 0.644 - ETA: 0s - loss: 5.7338 - acc: 0.644 - ETA: 0s - loss: 5.7348 - acc: 0.644 - ETA: 0s - loss: 5.7394 - acc: 0.643 - ETA: 0s - loss: 5.7259 - acc: 0.644 - ETA: 0s - loss: 5.7355 - acc: 0.644 - ETA: 0s - loss: 5.7451 - acc: 0.643 - ETA: 0s - loss: 5.7382 - acc: 0.644 - ETA: 0s - loss: 5.7172 - acc: 0.645 - ETA: 0s - loss: 5.7079 - acc: 0.645 - ETA: 0s - loss: 5.7279 - acc: 0.644 - ETA: 0s - loss: 5.7497 - acc: 0.643 - ETA: 0s - loss: 5.7806 - acc: 0.641 - ETA: 0s - loss: 5.7785 - acc: 0.641 - ETA: 0s - loss: 5.7985 - acc: 0.640 - ETA: 0s - loss: 5.8087 - acc: 0.6396Epoch 00091: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 5.8101 - acc: 0.6395 - val_loss: 7.1972 - val_acc: 0.5054\n",
      "Epoch 93/100\n",
      "6580/6680 [============================>.] - ETA: 4s - loss: 6.4472 - acc: 0.600 - ETA: 3s - loss: 6.3129 - acc: 0.608 - ETA: 3s - loss: 6.1786 - acc: 0.616 - ETA: 3s - loss: 6.4472 - acc: 0.600 - ETA: 3s - loss: 6.5624 - acc: 0.592 - ETA: 3s - loss: 6.5092 - acc: 0.596 - ETA: 3s - loss: 6.4472 - acc: 0.600 - ETA: 3s - loss: 6.3129 - acc: 0.608 - ETA: 3s - loss: 6.2900 - acc: 0.609 - ETA: 3s - loss: 6.0357 - acc: 0.625 - ETA: 3s - loss: 5.9358 - acc: 0.631 - ETA: 2s - loss: 5.8999 - acc: 0.633 - ETA: 2s - loss: 6.0416 - acc: 0.624 - ETA: 2s - loss: 6.0056 - acc: 0.626 - ETA: 2s - loss: 5.9356 - acc: 0.631 - ETA: 2s - loss: 5.9060 - acc: 0.633 - ETA: 2s - loss: 5.9258 - acc: 0.631 - ETA: 2s - loss: 5.8478 - acc: 0.636 - ETA: 2s - loss: 5.8629 - acc: 0.635 - ETA: 2s - loss: 5.8082 - acc: 0.639 - ETA: 2s - loss: 5.8236 - acc: 0.638 - ETA: 2s - loss: 5.7649 - acc: 0.641 - ETA: 2s - loss: 5.8023 - acc: 0.639 - ETA: 2s - loss: 5.7954 - acc: 0.639 - ETA: 2s - loss: 5.8219 - acc: 0.638 - ETA: 2s - loss: 5.8010 - acc: 0.639 - ETA: 2s - loss: 5.8372 - acc: 0.637 - ETA: 2s - loss: 5.8373 - acc: 0.637 - ETA: 2s - loss: 5.8079 - acc: 0.639 - ETA: 1s - loss: 5.7696 - acc: 0.641 - ETA: 1s - loss: 5.7391 - acc: 0.643 - ETA: 1s - loss: 5.7469 - acc: 0.643 - ETA: 1s - loss: 5.7685 - acc: 0.641 - ETA: 1s - loss: 5.7599 - acc: 0.642 - ETA: 1s - loss: 5.7791 - acc: 0.641 - ETA: 1s - loss: 5.8431 - acc: 0.637 - ETA: 1s - loss: 5.8684 - acc: 0.635 - ETA: 1s - loss: 5.8367 - acc: 0.637 - ETA: 1s - loss: 5.8555 - acc: 0.636 - ETA: 1s - loss: 5.8813 - acc: 0.634 - ETA: 1s - loss: 5.8431 - acc: 0.637 - ETA: 1s - loss: 5.8268 - acc: 0.638 - ETA: 1s - loss: 5.8449 - acc: 0.637 - ETA: 1s - loss: 5.8440 - acc: 0.637 - ETA: 1s - loss: 5.8573 - acc: 0.636 - ETA: 1s - loss: 5.8422 - acc: 0.637 - ETA: 1s - loss: 5.8618 - acc: 0.636 - ETA: 0s - loss: 5.8730 - acc: 0.635 - ETA: 0s - loss: 5.8683 - acc: 0.635 - ETA: 0s - loss: 5.8597 - acc: 0.636 - ETA: 0s - loss: 5.8689 - acc: 0.635 - ETA: 0s - loss: 5.8461 - acc: 0.637 - ETA: 0s - loss: 5.8422 - acc: 0.637 - ETA: 0s - loss: 5.8415 - acc: 0.637 - ETA: 0s - loss: 5.8430 - acc: 0.637 - ETA: 0s - loss: 5.8245 - acc: 0.638 - ETA: 0s - loss: 5.8214 - acc: 0.638 - ETA: 0s - loss: 5.8320 - acc: 0.637 - ETA: 0s - loss: 5.8315 - acc: 0.638 - ETA: 0s - loss: 5.8178 - acc: 0.638 - ETA: 0s - loss: 5.8260 - acc: 0.638 - ETA: 0s - loss: 5.8333 - acc: 0.637 - ETA: 0s - loss: 5.8278 - acc: 0.638 - ETA: 0s - loss: 5.8193 - acc: 0.638 - ETA: 0s - loss: 5.8068 - acc: 0.6395Epoch 00092: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 5.8092 - acc: 0.6394 - val_loss: 7.1412 - val_acc: 0.5102\n",
      "Epoch 94/100\n",
      "6620/6680 [============================>.] - ETA: 5s - loss: 4.8354 - acc: 0.700 - ETA: 3s - loss: 6.2170 - acc: 0.614 - ETA: 3s - loss: 5.5173 - acc: 0.657 - ETA: 3s - loss: 5.3727 - acc: 0.666 - ETA: 3s - loss: 5.1508 - acc: 0.680 - ETA: 3s - loss: 5.3535 - acc: 0.667 - ETA: 3s - loss: 5.4704 - acc: 0.660 - ETA: 3s - loss: 5.4080 - acc: 0.664 - ETA: 3s - loss: 5.4539 - acc: 0.661 - ETA: 2s - loss: 5.5427 - acc: 0.656 - ETA: 2s - loss: 5.5518 - acc: 0.655 - ETA: 2s - loss: 5.6413 - acc: 0.650 - ETA: 2s - loss: 5.6289 - acc: 0.650 - ETA: 2s - loss: 5.6183 - acc: 0.651 - ETA: 2s - loss: 5.6521 - acc: 0.649 - ETA: 2s - loss: 5.6514 - acc: 0.649 - ETA: 2s - loss: 5.6413 - acc: 0.650 - ETA: 2s - loss: 5.6871 - acc: 0.647 - ETA: 2s - loss: 5.6933 - acc: 0.646 - ETA: 2s - loss: 5.6496 - acc: 0.649 - ETA: 2s - loss: 5.6179 - acc: 0.651 - ETA: 2s - loss: 5.6413 - acc: 0.650 - ETA: 2s - loss: 5.5848 - acc: 0.653 - ETA: 2s - loss: 5.6140 - acc: 0.651 - ETA: 2s - loss: 5.5693 - acc: 0.654 - ETA: 2s - loss: 5.5910 - acc: 0.653 - ETA: 2s - loss: 5.5511 - acc: 0.655 - ETA: 2s - loss: 5.5654 - acc: 0.654 - ETA: 2s - loss: 5.6078 - acc: 0.652 - ETA: 1s - loss: 5.6143 - acc: 0.651 - ETA: 1s - loss: 5.6309 - acc: 0.650 - ETA: 1s - loss: 5.5856 - acc: 0.653 - ETA: 1s - loss: 5.6168 - acc: 0.651 - ETA: 1s - loss: 5.6318 - acc: 0.650 - ETA: 1s - loss: 5.6306 - acc: 0.650 - ETA: 1s - loss: 5.6535 - acc: 0.649 - ETA: 1s - loss: 5.6532 - acc: 0.649 - ETA: 1s - loss: 5.6914 - acc: 0.646 - ETA: 1s - loss: 5.7023 - acc: 0.646 - ETA: 1s - loss: 5.7011 - acc: 0.646 - ETA: 1s - loss: 5.7195 - acc: 0.645 - ETA: 1s - loss: 5.7095 - acc: 0.645 - ETA: 1s - loss: 5.7230 - acc: 0.644 - ETA: 1s - loss: 5.7322 - acc: 0.644 - ETA: 1s - loss: 5.7769 - acc: 0.641 - ETA: 1s - loss: 5.8232 - acc: 0.638 - ETA: 1s - loss: 5.8435 - acc: 0.637 - ETA: 1s - loss: 5.8561 - acc: 0.636 - ETA: 0s - loss: 5.8508 - acc: 0.636 - ETA: 0s - loss: 5.8595 - acc: 0.636 - ETA: 0s - loss: 5.8521 - acc: 0.636 - ETA: 0s - loss: 5.8644 - acc: 0.636 - ETA: 0s - loss: 5.8266 - acc: 0.638 - ETA: 0s - loss: 5.8268 - acc: 0.638 - ETA: 0s - loss: 5.8139 - acc: 0.639 - ETA: 0s - loss: 5.8080 - acc: 0.639 - ETA: 0s - loss: 5.8022 - acc: 0.640 - ETA: 0s - loss: 5.8044 - acc: 0.639 - ETA: 0s - loss: 5.8256 - acc: 0.638 - ETA: 0s - loss: 5.8172 - acc: 0.639 - ETA: 0s - loss: 5.8170 - acc: 0.639 - ETA: 0s - loss: 5.8219 - acc: 0.638 - ETA: 0s - loss: 5.8293 - acc: 0.638 - ETA: 0s - loss: 5.8263 - acc: 0.638 - ETA: 0s - loss: 5.8260 - acc: 0.638 - ETA: 0s - loss: 5.8086 - acc: 0.6396Epoch 00093: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 5.8095 - acc: 0.6395 - val_loss: 7.1226 - val_acc: 0.5066\n",
      "Epoch 95/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6620/6680 [============================>.] - ETA: 5s - loss: 8.0590 - acc: 0.500 - ETA: 5s - loss: 6.4472 - acc: 0.600 - ETA: 4s - loss: 6.8950 - acc: 0.572 - ETA: 4s - loss: 6.3897 - acc: 0.603 - ETA: 3s - loss: 5.8958 - acc: 0.634 - ETA: 3s - loss: 5.9435 - acc: 0.631 - ETA: 3s - loss: 5.9470 - acc: 0.631 - ETA: 3s - loss: 5.9495 - acc: 0.630 - ETA: 3s - loss: 5.8630 - acc: 0.636 - ETA: 3s - loss: 5.8341 - acc: 0.638 - ETA: 3s - loss: 5.7964 - acc: 0.640 - ETA: 3s - loss: 5.8818 - acc: 0.635 - ETA: 2s - loss: 5.9274 - acc: 0.632 - ETA: 2s - loss: 5.9662 - acc: 0.629 - ETA: 2s - loss: 5.9101 - acc: 0.633 - ETA: 2s - loss: 5.8894 - acc: 0.634 - ETA: 2s - loss: 6.0007 - acc: 0.627 - ETA: 2s - loss: 5.9711 - acc: 0.629 - ETA: 2s - loss: 5.8927 - acc: 0.634 - ETA: 2s - loss: 5.9571 - acc: 0.630 - ETA: 2s - loss: 6.0286 - acc: 0.626 - ETA: 2s - loss: 5.9623 - acc: 0.630 - ETA: 2s - loss: 5.9481 - acc: 0.631 - ETA: 2s - loss: 5.9965 - acc: 0.628 - ETA: 2s - loss: 5.9534 - acc: 0.630 - ETA: 2s - loss: 5.9638 - acc: 0.630 - ETA: 2s - loss: 5.9160 - acc: 0.633 - ETA: 2s - loss: 5.9157 - acc: 0.633 - ETA: 1s - loss: 5.8732 - acc: 0.635 - ETA: 1s - loss: 5.8563 - acc: 0.636 - ETA: 1s - loss: 5.8754 - acc: 0.635 - ETA: 1s - loss: 5.8429 - acc: 0.637 - ETA: 1s - loss: 5.8172 - acc: 0.639 - ETA: 1s - loss: 5.8357 - acc: 0.637 - ETA: 1s - loss: 5.8304 - acc: 0.638 - ETA: 1s - loss: 5.8296 - acc: 0.638 - ETA: 1s - loss: 5.8256 - acc: 0.638 - ETA: 1s - loss: 5.8207 - acc: 0.638 - ETA: 1s - loss: 5.8285 - acc: 0.638 - ETA: 1s - loss: 5.8229 - acc: 0.638 - ETA: 1s - loss: 5.8185 - acc: 0.638 - ETA: 1s - loss: 5.8066 - acc: 0.639 - ETA: 1s - loss: 5.8440 - acc: 0.637 - ETA: 1s - loss: 5.8430 - acc: 0.637 - ETA: 1s - loss: 5.8421 - acc: 0.637 - ETA: 1s - loss: 5.8264 - acc: 0.638 - ETA: 1s - loss: 5.8157 - acc: 0.639 - ETA: 0s - loss: 5.8188 - acc: 0.638 - ETA: 0s - loss: 5.8217 - acc: 0.638 - ETA: 0s - loss: 5.7950 - acc: 0.640 - ETA: 0s - loss: 5.7889 - acc: 0.640 - ETA: 0s - loss: 5.7947 - acc: 0.640 - ETA: 0s - loss: 5.7853 - acc: 0.640 - ETA: 0s - loss: 5.7856 - acc: 0.640 - ETA: 0s - loss: 5.7628 - acc: 0.642 - ETA: 0s - loss: 5.7607 - acc: 0.642 - ETA: 0s - loss: 5.7870 - acc: 0.640 - ETA: 0s - loss: 5.7763 - acc: 0.641 - ETA: 0s - loss: 5.7794 - acc: 0.641 - ETA: 0s - loss: 5.7938 - acc: 0.640 - ETA: 0s - loss: 5.7830 - acc: 0.640 - ETA: 0s - loss: 5.7599 - acc: 0.642 - ETA: 0s - loss: 5.7837 - acc: 0.640 - ETA: 0s - loss: 5.7909 - acc: 0.640 - ETA: 0s - loss: 5.8053 - acc: 0.6396Epoch 00094: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 5.8086 - acc: 0.6394 - val_loss: 7.1394 - val_acc: 0.5042\n",
      "Epoch 96/100\n",
      "6660/6680 [============================>.] - ETA: 5s - loss: 4.8354 - acc: 0.700 - ETA: 3s - loss: 5.9100 - acc: 0.633 - ETA: 3s - loss: 5.3483 - acc: 0.668 - ETA: 3s - loss: 5.1341 - acc: 0.680 - ETA: 3s - loss: 5.7074 - acc: 0.644 - ETA: 3s - loss: 5.8616 - acc: 0.635 - ETA: 3s - loss: 5.9348 - acc: 0.631 - ETA: 3s - loss: 5.8916 - acc: 0.633 - ETA: 3s - loss: 5.8802 - acc: 0.634 - ETA: 3s - loss: 5.7615 - acc: 0.642 - ETA: 3s - loss: 5.7179 - acc: 0.644 - ETA: 3s - loss: 5.6346 - acc: 0.650 - ETA: 3s - loss: 5.7047 - acc: 0.645 - ETA: 3s - loss: 5.7365 - acc: 0.643 - ETA: 2s - loss: 5.7063 - acc: 0.645 - ETA: 2s - loss: 5.7999 - acc: 0.639 - ETA: 2s - loss: 5.7779 - acc: 0.641 - ETA: 2s - loss: 5.7794 - acc: 0.641 - ETA: 2s - loss: 5.7526 - acc: 0.642 - ETA: 2s - loss: 5.8204 - acc: 0.638 - ETA: 2s - loss: 5.8275 - acc: 0.638 - ETA: 2s - loss: 5.7886 - acc: 0.640 - ETA: 2s - loss: 5.7965 - acc: 0.640 - ETA: 2s - loss: 5.7485 - acc: 0.643 - ETA: 2s - loss: 5.7573 - acc: 0.642 - ETA: 2s - loss: 5.7204 - acc: 0.644 - ETA: 2s - loss: 5.7229 - acc: 0.644 - ETA: 2s - loss: 5.7199 - acc: 0.645 - ETA: 2s - loss: 5.7060 - acc: 0.645 - ETA: 1s - loss: 5.7147 - acc: 0.645 - ETA: 1s - loss: 5.7123 - acc: 0.645 - ETA: 1s - loss: 5.7298 - acc: 0.644 - ETA: 1s - loss: 5.7557 - acc: 0.642 - ETA: 1s - loss: 5.7530 - acc: 0.642 - ETA: 1s - loss: 5.7729 - acc: 0.641 - ETA: 1s - loss: 5.7818 - acc: 0.641 - ETA: 1s - loss: 5.7651 - acc: 0.642 - ETA: 1s - loss: 5.7787 - acc: 0.641 - ETA: 1s - loss: 5.7711 - acc: 0.641 - ETA: 1s - loss: 5.7645 - acc: 0.642 - ETA: 1s - loss: 5.7895 - acc: 0.640 - ETA: 1s - loss: 5.8005 - acc: 0.639 - ETA: 1s - loss: 5.8274 - acc: 0.638 - ETA: 1s - loss: 5.8405 - acc: 0.637 - ETA: 1s - loss: 5.8397 - acc: 0.637 - ETA: 1s - loss: 5.8484 - acc: 0.636 - ETA: 1s - loss: 5.8331 - acc: 0.637 - ETA: 0s - loss: 5.8325 - acc: 0.637 - ETA: 0s - loss: 5.8545 - acc: 0.636 - ETA: 0s - loss: 5.8440 - acc: 0.637 - ETA: 0s - loss: 5.8276 - acc: 0.638 - ETA: 0s - loss: 5.7991 - acc: 0.640 - ETA: 0s - loss: 5.8051 - acc: 0.639 - ETA: 0s - loss: 5.8139 - acc: 0.639 - ETA: 0s - loss: 5.8188 - acc: 0.638 - ETA: 0s - loss: 5.8100 - acc: 0.639 - ETA: 0s - loss: 5.8238 - acc: 0.638 - ETA: 0s - loss: 5.7929 - acc: 0.640 - ETA: 0s - loss: 5.8064 - acc: 0.639 - ETA: 0s - loss: 5.7985 - acc: 0.640 - ETA: 0s - loss: 5.7934 - acc: 0.640 - ETA: 0s - loss: 5.7986 - acc: 0.640 - ETA: 0s - loss: 5.8187 - acc: 0.638 - ETA: 0s - loss: 5.8135 - acc: 0.639 - ETA: 0s - loss: 5.8104 - acc: 0.6393Epoch 00095: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 5.8099 - acc: 0.6394 - val_loss: 7.1630 - val_acc: 0.5018\n",
      "Epoch 97/100\n",
      "6640/6680 [============================>.] - ETA: 2s - loss: 5.6413 - acc: 0.650 - ETA: 3s - loss: 6.4472 - acc: 0.600 - ETA: 3s - loss: 5.4215 - acc: 0.663 - ETA: 3s - loss: 5.2147 - acc: 0.676 - ETA: 3s - loss: 5.3849 - acc: 0.665 - ETA: 3s - loss: 5.2832 - acc: 0.672 - ETA: 3s - loss: 5.6169 - acc: 0.651 - ETA: 3s - loss: 5.4717 - acc: 0.660 - ETA: 3s - loss: 5.5851 - acc: 0.653 - ETA: 3s - loss: 5.6078 - acc: 0.652 - ETA: 3s - loss: 5.6565 - acc: 0.649 - ETA: 2s - loss: 5.6691 - acc: 0.648 - ETA: 2s - loss: 5.7421 - acc: 0.643 - ETA: 2s - loss: 5.8486 - acc: 0.637 - ETA: 2s - loss: 5.9354 - acc: 0.631 - ETA: 2s - loss: 5.9678 - acc: 0.629 - ETA: 2s - loss: 5.9388 - acc: 0.631 - ETA: 2s - loss: 5.9802 - acc: 0.629 - ETA: 2s - loss: 5.9620 - acc: 0.630 - ETA: 2s - loss: 5.9703 - acc: 0.629 - ETA: 2s - loss: 6.0013 - acc: 0.627 - ETA: 2s - loss: 5.9771 - acc: 0.629 - ETA: 2s - loss: 5.9623 - acc: 0.630 - ETA: 2s - loss: 5.9896 - acc: 0.628 - ETA: 2s - loss: 5.9886 - acc: 0.628 - ETA: 2s - loss: 6.0254 - acc: 0.626 - ETA: 2s - loss: 5.9807 - acc: 0.628 - ETA: 2s - loss: 5.9333 - acc: 0.631 - ETA: 2s - loss: 5.9344 - acc: 0.631 - ETA: 1s - loss: 5.9299 - acc: 0.632 - ETA: 1s - loss: 5.9521 - acc: 0.630 - ETA: 1s - loss: 5.9525 - acc: 0.630 - ETA: 1s - loss: 5.9281 - acc: 0.632 - ETA: 1s - loss: 5.8812 - acc: 0.635 - ETA: 1s - loss: 5.8220 - acc: 0.638 - ETA: 1s - loss: 5.7899 - acc: 0.640 - ETA: 1s - loss: 5.8384 - acc: 0.637 - ETA: 1s - loss: 5.7906 - acc: 0.640 - ETA: 1s - loss: 5.8532 - acc: 0.636 - ETA: 1s - loss: 5.8357 - acc: 0.637 - ETA: 1s - loss: 5.8231 - acc: 0.638 - ETA: 1s - loss: 5.8217 - acc: 0.638 - ETA: 1s - loss: 5.8288 - acc: 0.638 - ETA: 1s - loss: 5.8501 - acc: 0.637 - ETA: 1s - loss: 5.8531 - acc: 0.636 - ETA: 1s - loss: 5.8406 - acc: 0.637 - ETA: 1s - loss: 5.8304 - acc: 0.638 - ETA: 0s - loss: 5.8256 - acc: 0.638 - ETA: 0s - loss: 5.8284 - acc: 0.638 - ETA: 0s - loss: 5.8529 - acc: 0.636 - ETA: 0s - loss: 5.8519 - acc: 0.636 - ETA: 0s - loss: 5.8501 - acc: 0.636 - ETA: 0s - loss: 5.8364 - acc: 0.637 - ETA: 0s - loss: 5.8240 - acc: 0.638 - ETA: 0s - loss: 5.8201 - acc: 0.638 - ETA: 0s - loss: 5.8425 - acc: 0.637 - ETA: 0s - loss: 5.8425 - acc: 0.637 - ETA: 0s - loss: 5.8384 - acc: 0.637 - ETA: 0s - loss: 5.8505 - acc: 0.636 - ETA: 0s - loss: 5.8412 - acc: 0.637 - ETA: 0s - loss: 5.8380 - acc: 0.637 - ETA: 0s - loss: 5.8374 - acc: 0.637 - ETA: 0s - loss: 5.8312 - acc: 0.638 - ETA: 0s - loss: 5.8283 - acc: 0.638 - ETA: 0s - loss: 5.8061 - acc: 0.6396Epoch 00096: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 5.8099 - acc: 0.6394 - val_loss: 7.1485 - val_acc: 0.5114\n",
      "Epoch 98/100\n",
      "6580/6680 [============================>.] - ETA: 3s - loss: 8.0590 - acc: 0.500 - ETA: 3s - loss: 6.3129 - acc: 0.608 - ETA: 3s - loss: 6.7403 - acc: 0.581 - ETA: 3s - loss: 6.0680 - acc: 0.623 - ETA: 3s - loss: 5.9710 - acc: 0.629 - ETA: 3s - loss: 5.9398 - acc: 0.631 - ETA: 3s - loss: 6.0695 - acc: 0.623 - ETA: 3s - loss: 6.1205 - acc: 0.620 - ETA: 2s - loss: 6.1661 - acc: 0.617 - ETA: 2s - loss: 6.0779 - acc: 0.622 - ETA: 2s - loss: 5.9911 - acc: 0.628 - ETA: 2s - loss: 5.9887 - acc: 0.628 - ETA: 2s - loss: 5.9228 - acc: 0.632 - ETA: 2s - loss: 5.8784 - acc: 0.635 - ETA: 2s - loss: 5.9615 - acc: 0.630 - ETA: 2s - loss: 5.9306 - acc: 0.632 - ETA: 2s - loss: 5.8744 - acc: 0.635 - ETA: 2s - loss: 5.8768 - acc: 0.635 - ETA: 2s - loss: 5.8905 - acc: 0.634 - ETA: 2s - loss: 5.8616 - acc: 0.635 - ETA: 2s - loss: 5.8665 - acc: 0.635 - ETA: 2s - loss: 5.8909 - acc: 0.634 - ETA: 2s - loss: 5.9291 - acc: 0.631 - ETA: 2s - loss: 5.9171 - acc: 0.632 - ETA: 2s - loss: 5.8803 - acc: 0.634 - ETA: 2s - loss: 5.8897 - acc: 0.634 - ETA: 2s - loss: 5.8787 - acc: 0.634 - ETA: 1s - loss: 5.8176 - acc: 0.638 - ETA: 1s - loss: 5.7623 - acc: 0.642 - ETA: 1s - loss: 5.7530 - acc: 0.642 - ETA: 1s - loss: 5.7700 - acc: 0.641 - ETA: 1s - loss: 5.7909 - acc: 0.640 - ETA: 1s - loss: 5.7727 - acc: 0.641 - ETA: 1s - loss: 5.7547 - acc: 0.642 - ETA: 1s - loss: 5.7509 - acc: 0.642 - ETA: 1s - loss: 5.7781 - acc: 0.641 - ETA: 1s - loss: 5.7788 - acc: 0.641 - ETA: 1s - loss: 5.7752 - acc: 0.641 - ETA: 1s - loss: 5.7671 - acc: 0.642 - ETA: 1s - loss: 5.7798 - acc: 0.641 - ETA: 1s - loss: 5.8035 - acc: 0.639 - ETA: 1s - loss: 5.8140 - acc: 0.639 - ETA: 1s - loss: 5.8064 - acc: 0.639 - ETA: 1s - loss: 5.8278 - acc: 0.638 - ETA: 1s - loss: 5.8230 - acc: 0.638 - ETA: 1s - loss: 5.8396 - acc: 0.637 - ETA: 0s - loss: 5.8321 - acc: 0.638 - ETA: 0s - loss: 5.8610 - acc: 0.636 - ETA: 0s - loss: 5.8878 - acc: 0.634 - ETA: 0s - loss: 5.8861 - acc: 0.634 - ETA: 0s - loss: 5.8630 - acc: 0.636 - ETA: 0s - loss: 5.8445 - acc: 0.637 - ETA: 0s - loss: 5.8646 - acc: 0.636 - ETA: 0s - loss: 5.8452 - acc: 0.637 - ETA: 0s - loss: 5.8295 - acc: 0.638 - ETA: 0s - loss: 5.8402 - acc: 0.637 - ETA: 0s - loss: 5.8340 - acc: 0.637 - ETA: 0s - loss: 5.8383 - acc: 0.637 - ETA: 0s - loss: 5.8377 - acc: 0.637 - ETA: 0s - loss: 5.8310 - acc: 0.638 - ETA: 0s - loss: 5.8203 - acc: 0.638 - ETA: 0s - loss: 5.8124 - acc: 0.639 - ETA: 0s - loss: 5.8048 - acc: 0.639 - ETA: 0s - loss: 5.8072 - acc: 0.6395Epoch 00097: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 5.8095 - acc: 0.6394 - val_loss: 7.1786 - val_acc: 0.5018\n",
      "Epoch 99/100\n",
      "6580/6680 [============================>.] - ETA: 5s - loss: 4.8354 - acc: 0.700 - ETA: 3s - loss: 4.3749 - acc: 0.728 - ETA: 3s - loss: 4.9594 - acc: 0.692 - ETA: 3s - loss: 5.1323 - acc: 0.681 - ETA: 3s - loss: 5.3727 - acc: 0.666 - ETA: 2s - loss: 5.5070 - acc: 0.658 - ETA: 2s - loss: 5.6644 - acc: 0.648 - ETA: 2s - loss: 5.8630 - acc: 0.636 - ETA: 2s - loss: 5.9100 - acc: 0.633 - ETA: 2s - loss: 6.0282 - acc: 0.626 - ETA: 2s - loss: 5.9344 - acc: 0.631 - ETA: 2s - loss: 5.8966 - acc: 0.634 - ETA: 2s - loss: 5.9222 - acc: 0.632 - ETA: 2s - loss: 5.8797 - acc: 0.635 - ETA: 2s - loss: 5.8853 - acc: 0.634 - ETA: 2s - loss: 5.8602 - acc: 0.636 - ETA: 2s - loss: 5.9378 - acc: 0.631 - ETA: 2s - loss: 5.8866 - acc: 0.634 - ETA: 2s - loss: 5.8158 - acc: 0.639 - ETA: 2s - loss: 5.8683 - acc: 0.635 - ETA: 2s - loss: 5.9171 - acc: 0.632 - ETA: 2s - loss: 5.9120 - acc: 0.633 - ETA: 2s - loss: 5.9322 - acc: 0.631 - ETA: 2s - loss: 5.9595 - acc: 0.630 - ETA: 2s - loss: 5.9597 - acc: 0.630 - ETA: 2s - loss: 5.9086 - acc: 0.633 - ETA: 1s - loss: 5.8952 - acc: 0.634 - ETA: 1s - loss: 5.8719 - acc: 0.635 - ETA: 1s - loss: 5.8215 - acc: 0.638 - ETA: 1s - loss: 5.7868 - acc: 0.640 - ETA: 1s - loss: 5.8144 - acc: 0.638 - ETA: 1s - loss: 5.8140 - acc: 0.638 - ETA: 1s - loss: 5.8219 - acc: 0.638 - ETA: 1s - loss: 5.8395 - acc: 0.637 - ETA: 1s - loss: 5.8276 - acc: 0.637 - ETA: 1s - loss: 5.8155 - acc: 0.638 - ETA: 1s - loss: 5.8069 - acc: 0.638 - ETA: 1s - loss: 5.8035 - acc: 0.639 - ETA: 1s - loss: 5.8303 - acc: 0.637 - ETA: 1s - loss: 5.8239 - acc: 0.637 - ETA: 1s - loss: 5.8310 - acc: 0.637 - ETA: 1s - loss: 5.8393 - acc: 0.636 - ETA: 1s - loss: 5.8444 - acc: 0.636 - ETA: 1s - loss: 5.8470 - acc: 0.636 - ETA: 1s - loss: 5.8530 - acc: 0.635 - ETA: 0s - loss: 5.8339 - acc: 0.637 - ETA: 0s - loss: 5.8434 - acc: 0.636 - ETA: 0s - loss: 5.8414 - acc: 0.636 - ETA: 0s - loss: 5.8283 - acc: 0.637 - ETA: 0s - loss: 5.7866 - acc: 0.639 - ETA: 0s - loss: 5.7865 - acc: 0.639 - ETA: 0s - loss: 5.7839 - acc: 0.639 - ETA: 0s - loss: 5.7866 - acc: 0.639 - ETA: 0s - loss: 5.8042 - acc: 0.638 - ETA: 0s - loss: 5.7957 - acc: 0.639 - ETA: 0s - loss: 5.7876 - acc: 0.639 - ETA: 0s - loss: 5.7826 - acc: 0.639 - ETA: 0s - loss: 5.7925 - acc: 0.639 - ETA: 0s - loss: 5.7843 - acc: 0.639 - ETA: 0s - loss: 5.7824 - acc: 0.639 - ETA: 0s - loss: 5.7874 - acc: 0.639 - ETA: 0s - loss: 5.7705 - acc: 0.640 - ETA: 0s - loss: 5.7674 - acc: 0.6404Epoch 00098: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 5.7631 - acc: 0.6407 - val_loss: 7.1488 - val_acc: 0.5042\n",
      "Epoch 100/100\n",
      "6620/6680 [============================>.] - ETA: 3s - loss: 4.8396 - acc: 0.700 - ETA: 3s - loss: 5.9874 - acc: 0.628 - ETA: 3s - loss: 5.7760 - acc: 0.641 - ETA: 3s - loss: 5.5942 - acc: 0.652 - ETA: 3s - loss: 5.7531 - acc: 0.643 - ETA: 3s - loss: 5.5028 - acc: 0.655 - ETA: 3s - loss: 5.6000 - acc: 0.650 - ETA: 3s - loss: 5.6057 - acc: 0.650 - ETA: 2s - loss: 5.6669 - acc: 0.646 - ETA: 2s - loss: 5.6642 - acc: 0.646 - ETA: 2s - loss: 5.6241 - acc: 0.647 - ETA: 2s - loss: 5.6532 - acc: 0.645 - ETA: 2s - loss: 5.5901 - acc: 0.650 - ETA: 2s - loss: 5.4441 - acc: 0.659 - ETA: 2s - loss: 5.4470 - acc: 0.659 - ETA: 2s - loss: 5.5340 - acc: 0.653 - ETA: 2s - loss: 5.5119 - acc: 0.655 - ETA: 2s - loss: 5.5117 - acc: 0.655 - ETA: 2s - loss: 5.5610 - acc: 0.652 - ETA: 2s - loss: 5.5730 - acc: 0.651 - ETA: 2s - loss: 5.5513 - acc: 0.651 - ETA: 2s - loss: 5.5513 - acc: 0.651 - ETA: 2s - loss: 5.6050 - acc: 0.648 - ETA: 2s - loss: 5.5528 - acc: 0.652 - ETA: 2s - loss: 5.5789 - acc: 0.650 - ETA: 2s - loss: 5.5266 - acc: 0.653 - ETA: 2s - loss: 5.5427 - acc: 0.652 - ETA: 2s - loss: 5.5596 - acc: 0.651 - ETA: 1s - loss: 5.5348 - acc: 0.653 - ETA: 1s - loss: 5.5762 - acc: 0.650 - ETA: 1s - loss: 5.5372 - acc: 0.653 - ETA: 1s - loss: 5.5807 - acc: 0.650 - ETA: 1s - loss: 5.5972 - acc: 0.649 - ETA: 1s - loss: 5.6170 - acc: 0.648 - ETA: 1s - loss: 5.5907 - acc: 0.650 - ETA: 1s - loss: 5.6010 - acc: 0.649 - ETA: 1s - loss: 5.5895 - acc: 0.650 - ETA: 1s - loss: 5.6073 - acc: 0.649 - ETA: 1s - loss: 5.6203 - acc: 0.648 - ETA: 1s - loss: 5.6562 - acc: 0.646 - ETA: 1s - loss: 5.6406 - acc: 0.647 - ETA: 1s - loss: 5.6351 - acc: 0.647 - ETA: 1s - loss: 5.6572 - acc: 0.646 - ETA: 1s - loss: 5.6604 - acc: 0.646 - ETA: 1s - loss: 5.6835 - acc: 0.644 - ETA: 1s - loss: 5.6793 - acc: 0.644 - ETA: 0s - loss: 5.6750 - acc: 0.645 - ETA: 0s - loss: 5.6808 - acc: 0.644 - ETA: 0s - loss: 5.6971 - acc: 0.643 - ETA: 0s - loss: 5.7243 - acc: 0.641 - ETA: 0s - loss: 5.7231 - acc: 0.641 - ETA: 0s - loss: 5.7007 - acc: 0.643 - ETA: 0s - loss: 5.6967 - acc: 0.643 - ETA: 0s - loss: 5.6727 - acc: 0.645 - ETA: 0s - loss: 5.6496 - acc: 0.646 - ETA: 0s - loss: 5.6633 - acc: 0.645 - ETA: 0s - loss: 5.6712 - acc: 0.645 - ETA: 0s - loss: 5.6759 - acc: 0.645 - ETA: 0s - loss: 5.6859 - acc: 0.644 - ETA: 0s - loss: 5.6748 - acc: 0.645 - ETA: 0s - loss: 5.6921 - acc: 0.644 - ETA: 0s - loss: 5.7114 - acc: 0.643 - ETA: 0s - loss: 5.6906 - acc: 0.644 - ETA: 0s - loss: 5.7017 - acc: 0.6437Epoch 00099: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 5.7060 - acc: 0.6434 - val_loss: 7.1376 - val_acc: 0.5030\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22eaa8391d0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint  \n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.VGG16.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "VGG16_model.fit(train_VGG16, train_targets, \n",
    "          validation_data=(valid_VGG16, valid_targets),\n",
    "          epochs=100, batch_size=20, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Model with the Best Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG16_model.load_weights('saved_models/weights.best.VGG16.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model\n",
    "\n",
    "Now, we can use the CNN to test how well it identifies breed within our test dataset of dog images.  We print the test accuracy below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 49.6411%\n"
     ]
    }
   ],
   "source": [
    "# get index of predicted dog breed for each image in test set\n",
    "VGG16_predictions = [np.argmax(VGG16_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG16]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(VGG16_predictions)==np.argmax(test_targets, axis=1))/len(VGG16_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Dog Breed with the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extract_bottleneck_features import *\n",
    "\n",
    "def VGG16_predict_breed(img_path):\n",
    "    # extract bottleneck features\n",
    "    bottleneck_feature = extract_VGG16(path_to_tensor(img_path))\n",
    "    # obtain predicted vector\n",
    "    predicted_vector = VGG16_model.predict(bottleneck_feature)\n",
    "    # return dog breed that is predicted by the model\n",
    "    return dog_names[np.argmax(predicted_vector)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step5'></a>\n",
    "## Step 5: Create a CNN to Classify Dog Breeds (using Transfer Learning)\n",
    "\n",
    "You will now use transfer learning to create a CNN that can identify dog breed from images.  Your CNN must attain at least 60% accuracy on the test set.\n",
    "\n",
    "In Step 4, we used transfer learning to create a CNN using VGG-16 bottleneck features.  In this section, you must use the bottleneck features from a different pre-trained model.  To make things easier for you, we have pre-computed the features for all of the networks that are currently available in Keras:\n",
    "- [VGG-19](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogVGG19Data.npz) bottleneck features\n",
    "- [ResNet-50](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogResnet50Data.npz) bottleneck features\n",
    "- [Inception](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogInceptionV3Data.npz) bottleneck features\n",
    "- [Xception](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogXceptionData.npz) bottleneck features\n",
    "\n",
    "The files are encoded as such:\n",
    "\n",
    "    Dog{network}Data.npz\n",
    "    \n",
    "where `{network}`, in the above filename, can be one of `VGG19`, `Resnet50`, `InceptionV3`, or `Xception`.  Pick one of the above architectures, download the corresponding bottleneck features, and store the downloaded file in the `bottleneck_features/` folder in the repository.\n",
    "\n",
    "### (IMPLEMENTATION) Obtain Bottleneck Features\n",
    "\n",
    "In the code block below, extract the bottleneck features corresponding to the train, test, and validation sets by running the following:\n",
    "\n",
    "    bottleneck_features = np.load('bottleneck_features/Dog{network}Data.npz')\n",
    "    train_{network} = bottleneck_features['train']\n",
    "    valid_{network} = bottleneck_features['valid']\n",
    "    test_{network} = bottleneck_features['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'bottleneck_features/DogXceptionData.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-a9d9953ca5ea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#test_DogResnet50 = bottleneck_features['test']\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mbottleneck_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'bottleneck_features/DogXceptionData.npz'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mtrain_DogXceptionData\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbottleneck_features\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mvalid_DogXceptionData\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbottleneck_features\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'valid'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\conda\\envs\\dlnd\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    368\u001b[0m     \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    369\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 370\u001b[1;33m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    371\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    372\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mis_pathlib_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'bottleneck_features/DogXceptionData.npz'"
     ]
    }
   ],
   "source": [
    "### TODO: Obtain bottleneck features from another pre-trained CNN.\n",
    "#bottleneck_features = np.load('bottleneck_features/DogResnet50Data.npz')\n",
    "#train_DogResnet50 = bottleneck_features['train']\n",
    "#valid_DogResnet50 = bottleneck_features['valid']\n",
    "#test_DogResnet50 = bottleneck_features['test']\n",
    "\n",
    "bottleneck_features = np.load('bottleneck_features/DogXceptionData.npz')\n",
    "train_DogXceptionData = bottleneck_features['train']\n",
    "valid_DogXceptionData = bottleneck_features['valid']\n",
    "test_DogXceptionData = bottleneck_features['test']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Model Architecture\n",
    "\n",
    "Create a CNN to classify dog breed.  At the end of your code cell block, summarize the layers of your model by executing the line:\n",
    "    \n",
    "        <your model's name>.summary()\n",
    "   \n",
    "__Question 5:__ Outline the steps you took to get to your final CNN architecture and your reasoning at each step.  Describe why you think the architecture is suitable for the current problem.\n",
    "\n",
    "__Answer:__ \n",
    "\n",
    " - We do not need to add convolutional layers again because this model is pre-trained. all we need to do now is transfer the learning and adjust to the new data set and output list.\n",
    " - We directly added a GlobalAveragePooling layer for dimentionality reduction purposes giving it the shape of the images inside the set.\n",
    " \n",
    " - This architecture is suitable because it is a transfer-learning model where we already have a pre-trained model.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "global_average_pooling2d_3 ( (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 133)               272517    \n",
      "=================================================================\n",
      "Total params: 272,517.0\n",
      "Trainable params: 272,517.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "### TODO: Define your architecture.\n",
    "#Resnet50_model = Sequential()\n",
    "#Resnet50_model.add(GlobalAveragePooling2D(input_shape=train_DogResnet50.shape[1:]))\n",
    "#Resnet50_model.add(Dense(133, activation='softmax'))\n",
    "\n",
    "#Resnet50_model.summary()\n",
    "\n",
    "DogXceptionData_model = Sequential()\n",
    "DogXceptionData_model.add(GlobalAveragePooling2D(input_shape=train_DogXceptionData.shape[1:]))\n",
    "DogXceptionData_model.add(Dense(133, activation='softmax'))\n",
    "\n",
    "DogXceptionData_model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Compile the model.\n",
    "#Resnet50_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "DogXceptionData_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Train the Model\n",
    "\n",
    "Train your model in the code cell below.  Use model checkpointing to save the model that attains the best validation loss.  \n",
    "\n",
    "You are welcome to [augment the training data](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html), but this is not a requirement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/20\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.0584 - acc: 0.7388Epoch 00000: val_loss improved from inf to 0.52184, saving model to saved_models/weights.best.DogXceptionData.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 1.0508 - acc: 0.7403 - val_loss: 0.5218 - val_acc: 0.8347\n",
      "Epoch 2/20\n",
      "6620/6680 [============================>.] - ETA: 0s - loss: 0.3973 - acc: 0.8745Epoch 00001: val_loss improved from 0.52184 to 0.48543, saving model to saved_models/weights.best.DogXceptionData.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 0.3965 - acc: 0.8747 - val_loss: 0.4854 - val_acc: 0.8551\n",
      "Epoch 3/20\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.3197 - acc: 0.8967Epoch 00002: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 0.3221 - acc: 0.8961 - val_loss: 0.5027 - val_acc: 0.8419\n",
      "Epoch 4/20\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 0.2794 - acc: 0.9132Epoch 00003: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 0.2790 - acc: 0.9132 - val_loss: 0.4924 - val_acc: 0.8575\n",
      "Epoch 5/20\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.2450 - acc: 0.9250Epoch 00004: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 0.2457 - acc: 0.9247 - val_loss: 0.5175 - val_acc: 0.8479\n",
      "Epoch 6/20\n",
      "6620/6680 [============================>.] - ETA: 0s - loss: 0.2179 - acc: 0.9326Epoch 00005: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 0.2171 - acc: 0.9328 - val_loss: 0.5234 - val_acc: 0.8491\n",
      "Epoch 7/20\n",
      "6620/6680 [============================>.] - ETA: 0s - loss: 0.1963 - acc: 0.9397Epoch 00006: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 0.1955 - acc: 0.9398 - val_loss: 0.5252 - val_acc: 0.8551\n",
      "Epoch 8/20\n",
      "6640/6680 [============================>.] - ETA: 0s - loss: 0.1796 - acc: 0.9446Epoch 00007: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 0.1795 - acc: 0.9446 - val_loss: 0.5712 - val_acc: 0.8515\n",
      "Epoch 9/20\n",
      "6560/6680 [============================>.] - ETA: 0s - loss: 0.1595 - acc: 0.9494Epoch 00008: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 0.1614 - acc: 0.9494 - val_loss: 0.5791 - val_acc: 0.8515\n",
      "Epoch 10/20\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 0.1504 - acc: 0.9539Epoch 00009: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 0.1503 - acc: 0.9539 - val_loss: 0.5491 - val_acc: 0.8647\n",
      "Epoch 11/20\n",
      "6640/6680 [============================>.] - ETA: 0s - loss: 0.1305 - acc: 0.9616Epoch 00010: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 0.1332 - acc: 0.9612 - val_loss: 0.5917 - val_acc: 0.8515\n",
      "Epoch 12/20\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 0.1233 - acc: 0.9635Epoch 00011: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 0.1240 - acc: 0.9632 - val_loss: 0.6389 - val_acc: 0.8539\n",
      "Epoch 13/20\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.1178 - acc: 0.9642Epoch 00012: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 0.1176 - acc: 0.9642 - val_loss: 0.6155 - val_acc: 0.8503\n",
      "Epoch 14/20\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.1087 - acc: 0.9673Epoch 00013: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 0.1092 - acc: 0.9669 - val_loss: 0.6179 - val_acc: 0.8551\n",
      "Epoch 15/20\n",
      "6640/6680 [============================>.] - ETA: 0s - loss: 0.0996 - acc: 0.9718Epoch 00014: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 0.1005 - acc: 0.9716 - val_loss: 0.6147 - val_acc: 0.8659\n",
      "Epoch 16/20\n",
      "6620/6680 [============================>.] - ETA: 0s - loss: 0.0907 - acc: 0.9751Epoch 00015: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 0.0908 - acc: 0.9750 - val_loss: 0.6449 - val_acc: 0.8563\n",
      "Epoch 17/20\n",
      "6640/6680 [============================>.] - ETA: 0s - loss: 0.0850 - acc: 0.9752Epoch 00016: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 0.0869 - acc: 0.9749 - val_loss: 0.6607 - val_acc: 0.8659\n",
      "Epoch 18/20\n",
      "6640/6680 [============================>.] - ETA: 0s - loss: 0.0814 - acc: 0.9770Epoch 00017: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 0.0812 - acc: 0.9768 - val_loss: 0.6441 - val_acc: 0.8575\n",
      "Epoch 19/20\n",
      "6580/6680 [============================>.] - ETA: 0s - loss: 0.0766 - acc: 0.9796Epoch 00018: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 0.0773 - acc: 0.9796 - val_loss: 0.6942 - val_acc: 0.8587\n",
      "Epoch 20/20\n",
      "6620/6680 [============================>.] - ETA: 0s - loss: 0.0732 - acc: 0.9804Epoch 00019: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 0.0730 - acc: 0.9804 - val_loss: 0.6987 - val_acc: 0.8527\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f867c050978>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TODO: Train the model.\n",
    "#checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.Resnet50.hdf5', \n",
    "#                               verbose=1, save_best_only=True)\n",
    "\n",
    "#Resnet50_model.fit(train_DogResnet50, train_targets, \n",
    "#          validation_data=(valid_DogResnet50, valid_targets),\n",
    "#          epochs=20, batch_size=20, callbacks=[checkpointer], verbose=1)\n",
    "\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.DogXceptionData.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "DogXceptionData_model.fit(train_DogXceptionData, train_targets, \n",
    "          validation_data=(valid_DogXceptionData, valid_targets),\n",
    "          epochs=20, batch_size=20, callbacks=[checkpointer], verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Load the Model with the Best Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Load the model weights with the best validation loss.\n",
    "#Resnet50_model.load_weights('saved_models/weights.best.Resnet50.hdf5')\n",
    "\n",
    "DogXceptionData_model.load_weights('saved_models/weights.best.DogXceptionData.hdf5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Test the Model\n",
    "\n",
    "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 82.8947%\n"
     ]
    }
   ],
   "source": [
    "### TODO: Calculate classification accuracy on the test dataset.\n",
    "# get index of predicted dog breed for each image in test set\n",
    "#Resnet50_predictions = [np.argmax(Resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_DogResnet50]\n",
    "\n",
    "DogXceptionData_predictions = [np.argmax(DogXceptionData_model.predict(np.expand_dims(feature, axis=0))) for feature in test_DogXceptionData]\n",
    "\n",
    "\n",
    "# report test accuracy\n",
    "#test_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\n",
    "test_accuracy = 100*np.sum(np.array(DogXceptionData_predictions)==np.argmax(test_targets, axis=1))/len(DogXceptionData_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Predict Dog Breed with the Model\n",
    "\n",
    "Write a function that takes an image path as input and returns the dog breed (`Affenpinscher`, `Afghan_hound`, etc) that is predicted by your model.  \n",
    "\n",
    "Similar to the analogous function in Step 5, your function should have three steps:\n",
    "1. Extract the bottleneck features corresponding to the chosen CNN model.\n",
    "2. Supply the bottleneck features as input to the model to return the predicted vector.  Note that the argmax of this prediction vector gives the index of the predicted dog breed.\n",
    "3. Use the `dog_names` array defined in Step 0 of this notebook to return the corresponding breed.\n",
    "\n",
    "The functions to extract the bottleneck features can be found in `extract_bottleneck_features.py`, and they have been imported in an earlier code cell.  To obtain the bottleneck features corresponding to your chosen CNN architecture, you need to use the function\n",
    "\n",
    "    extract_{network}\n",
    "    \n",
    "where `{network}`, in the above filename, should be one of `VGG19`, `Resnet50`, `InceptionV3`, or `Xception`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Write a function that takes a path to an image as input\n",
    "### and returns the dog breed that is predicted by the model.\n",
    "from extract_bottleneck_features import *\n",
    "def predict_breed_Rasnet50(image):\n",
    "    # extract bottleneck features\n",
    "    bottleneck_feature = extract_Resnet50(path_to_tensor(img_path))\n",
    "    # obtain predicted vector\n",
    "    predicted_vector = Resnet50_model.predict(bottleneck_feature)\n",
    "    # return dog breed that is predicted by the model\n",
    "    return dog_names[np.argmax(predicted_vector)]\n",
    "\n",
    "\n",
    "def predict_breed_Xception(image):\n",
    "    # extract bottleneck features\n",
    "    bottleneck_feature = extract_Xception(path_to_tensor(image))\n",
    "    # obtain predicted vector\n",
    "    predicted_vector = DogXceptionData_model.predict(bottleneck_feature)\n",
    "    # return dog breed that is predicted by the model\n",
    "    return dog_names[np.argmax(predicted_vector)]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step6'></a>\n",
    "## Step 6: Write your Algorithm\n",
    "\n",
    "Write an algorithm that accepts a file path to an image and first determines whether the image contains a human, dog, or neither.  Then,\n",
    "- if a __dog__ is detected in the image, return the predicted breed.\n",
    "- if a __human__ is detected in the image, return the resembling dog breed.\n",
    "- if __neither__ is detected in the image, provide output that indicates an error.\n",
    "\n",
    "You are welcome to write your own functions for detecting humans and dogs in images, but feel free to use the `face_detector` and `dog_detector` functions developed above.  You are __required__ to use your CNN from Step 5 to predict dog breed.  \n",
    "\n",
    "Some sample output for our algorithm is provided below, but feel free to design your own user experience!\n",
    "\n",
    "![Sample Human Output](images/sample_human_output.png)\n",
    "\n",
    "\n",
    "### (IMPLEMENTATION) Write your Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Write your algorithm.\n",
    "### Feel free to use as many code cells as needed.\n",
    "def how_dog_is_this(img):\n",
    "    is_dog = False\n",
    "    is_human = False\n",
    "    breed = \"\"\n",
    "    if dog_detector(img_path=img):\n",
    "        is_dog = True\n",
    "        print(\"dog detected..!\")\n",
    "        breed = predict_breed_Xception(img)\n",
    "        print(\"breed is most likely to be: \" + breed)\n",
    "        \n",
    "    if face_detector(img_path=img):\n",
    "        is_human = True\n",
    "        print(\"human detected..!\")\n",
    "        breed = predict_breed_Xception(img)\n",
    "        print(\"this human looks more like: \" + breed)\n",
    "        \n",
    "    if not is_dog and not is_human:\n",
    "        print(\"We couldn't identify the image as dog or human\")\n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step7'></a>\n",
    "## Step 7: Test Your Algorithm\n",
    "\n",
    "In this section, you will take your new algorithm for a spin!  What kind of dog does the algorithm think that __you__ look like?  If you have a dog, does it predict your dog's breed accurately?  If you have a cat, does it mistakenly think that your cat is a dog?\n",
    "\n",
    "### (IMPLEMENTATION) Test Your Algorithm on Sample Images!\n",
    "\n",
    "Test your algorithm at least six images on your computer.  Feel free to use any images you like.  Use at least two human and two dog images.  \n",
    "\n",
    "__Question 6:__ Is the output better than you expected :) ?  Or worse :( ?  Provide at least three possible points of improvement for your algorithm.\n",
    "\n",
    "__Answer:__ \n",
    "Yes, The output is fairly accurate. approximately 85% accuracy.\n",
    "\n",
    "Improvement points:\n",
    " - It shouldn't print the result directly for modularity purposes\n",
    " - It would be better if it can identify an image with dog and human at the same time.(multi objects same image)\n",
    " - human face detector should detect faces from various sides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ramy.jpg\n",
      "human detected..!\n",
      "this human looks more like: Canaan_dog\n",
      "profilepic.jpg\n",
      "human detected..!\n",
      "this human looks more like: Chinese_crested\n",
      "dog1\n",
      "dog detected..!\n",
      "breed is most likely to be: American_eskimo_dog\n",
      "dog2\n",
      "dog detected..!\n",
      "breed is most likely to be: Greater_swiss_mountain_dog\n",
      "dog3\n",
      "dog detected..!\n",
      "breed is most likely to be: Bulldog\n",
      "dog4\n",
      "dog detected..!\n",
      "breed is most likely to be: Doberman_pinscher\n",
      "dog5\n",
      "dog detected..!\n",
      "breed is most likely to be: Pekingese\n",
      "dog6\n",
      "dog detected..!\n",
      "breed is most likely to be: Dachshund\n",
      "bogyalone\n",
      "human detected..!\n",
      "this human looks more like: French_bulldog\n",
      "yasmine1\n",
      "human detected..!\n",
      "this human looks more like: Petit_basset_griffon_vendeen\n"
     ]
    }
   ],
   "source": [
    "## TODO: Execute your algorithm from Step 6 on\n",
    "## at least 6 images on your computer.\n",
    "## Feel free to use as many code cells as needed.\n",
    "print(\"ramy.jpg\")\n",
    "how_dog_is_this('testdata/ramy.png')\n",
    "\n",
    "print(\"profilepic.jpg\")\n",
    "how_dog_is_this('testdata/profilepic.jpg')\n",
    "\n",
    "print(\"dog1\")\n",
    "how_dog_is_this('testdata/dog1.jpg')\n",
    "\n",
    "print(\"dog2\")\n",
    "how_dog_is_this('testdata/dog2.jpg')\n",
    "\n",
    "print(\"dog3\")\n",
    "how_dog_is_this('testdata/dog3.jpg')\n",
    "\n",
    "print(\"dog4\")\n",
    "how_dog_is_this('testdata/dog4.jpg')\n",
    "\n",
    "print(\"dog5\")\n",
    "how_dog_is_this('testdata/dog5.png')\n",
    "\n",
    "print(\"dog6\")\n",
    "how_dog_is_this('testdata/dog6.jpg')\n",
    "\n",
    "print(\"bogyalone\")\n",
    "how_dog_is_this('testdata/bodyalone.jpg')\n",
    "\n",
    "print(\"yasmine1\")\n",
    "how_dog_is_this('testdata/yasmine1.jpg')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
